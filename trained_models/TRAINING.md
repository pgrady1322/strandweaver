# ðŸ§  AI Model Training

**Custom Model Training & Retraining Guide for StrandWeaver**

[![Models](https://img.shields.io/badge/models-7%20AI%20modules-green.svg)](#model-architecture)
[![Training Data](https://img.shields.io/badge/training%20data-200%20genomes-blue.svg)](#training-data-generation)
[![XGBoost](https://img.shields.io/badge/XGBoost-9%20models-orange.svg)](#xgboost-models)
[![GNN](https://img.shields.io/badge/GNN-GATv2Conv-purple.svg)](#pathgnn-graph-neural-network)

StrandWeaver ships with pre-trained models for all 7 AI modules. This guide covers how the models were trained, how to generate custom training data for your organism or sequencing technology, and how to retrain from scratch.

> **Note**: Pre-trained models are included with the v0.2+ release. You only need this guide if you want to train custom models for organism-specific optimization.

---

## ðŸ“‹ Table of Contents

- [Model Architecture](#-model-architecture)
- [Shipped Models](#-shipped-models)
- [Training Data Generation](#-training-data-generation)
- [Custom Training](#-custom-training)
- [Colab GPU Training](#-colab-gpu-training)
- [Model Performance](#-model-performance)
- [Advanced: Class Imbalance](#-advanced-class-imbalance)

---

## ðŸ—ï¸ Model Architecture

StrandWeaver uses a **two-tier ML architecture**:

| Tier | Framework | Models | Purpose |
|------|-----------|--------|---------|
| **Tabular** | XGBoost | 9 models | Classification & regression on extracted graph/read features |
| **Graph** | PyTorch Geometric | 1 GNN | Graph-aware edge classification using GATv2Conv attention |

The XGBoost models handle structured feature analysis (edge scoring, haplotype phasing, SV detection, ultra-long routing), while the GNN propagates information across assembly graph neighborhoods to capture structural patterns that tabular features miss.

### Ensemble Design

The PathGNN receives both raw graph features **and** EdgeWarden's XGBoost `predict_proba()` outputs as edge features. This lets the GNN learn when to trust vs. override the tabular model's predictions using graph context.

```
Assembly Graph CSVs
        â”‚
        â”œâ”€â”€â–º XGBoost Models (5Ã—) â”€â”€â–º Edge scores, SV calls, phasing, routing
        â”‚         â”‚
        â”‚         â–¼ predict_proba()
        â”‚         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â–º PathGNN (GATv2Conv) â”€â”€â–º Final edge classification
                  â”‚
           Raw features + XGBoost probabilities
```

---

## ðŸ“¦ Shipped Models

All pre-trained models are stored in `trained_models/` and managed via Git LFS:

| Module | Model | File(s) | Task |
|--------|-------|---------|------|
| ðŸ›¡ï¸ **EdgeWarden** | XGBoost (Ã—5) | `edgewarden/edgewarden_{tech}.pkl` | Edge quality scoring (per-technology) |
| ï¿½ **K-Weaver** | XGBoost (Ã—4) | `kweaver/{dbg,ul_overlap,extension,polish}_model.pkl` | Optimal k-mer size prediction |
| ï¿½ðŸ§¬ **PathGNN** | GATv2Conv GNN | `pathgnn/pathgnn_model.pt` | Graph-aware edge classification |
| ðŸ”€ **DiploidAI** | XGBoost | `diploid/diploid_model.pkl` | Haplotype phasing (26 features) |
| ðŸ” **SVScribe** | XGBoost | `sv_detector/sv_detector_model.pkl` | Structural variant detection |
| ðŸ§µ **ThreadCompass** | XGBoost | `ul_routing/ul_routing_model.pkl` | Ultra-long read routing |

EdgeWarden includes 5 technology-specific models with paired feature scalers:
- `edgewarden_hifi.pkl` / `scaler_hifi.pkl`
- `edgewarden_ont_r9.pkl` / `scaler_ont_r9.pkl`
- `edgewarden_ont_r10.pkl` / `scaler_ont_r10.pkl`
- `edgewarden_illumina.pkl` / `scaler_illumina.pkl`
- `edgewarden_adna.pkl` / `scaler_adna.pkl`

> **Fallback behavior**: If any model file is missing, StrandWeaver automatically falls back to optimized heuristic scoring. The pipeline never fails due to a missing model.

K-Weaver includes 4 regression models predicting optimal k-mer sizes from 19 `ReadFeatures`:
- `dbg_model.pkl` â€” De Bruijn graph construction k
- `ul_overlap_model.pkl` â€” Ultra-long read overlap k
- `extension_model.pkl` â€” Contig extension k
- `polish_model.pkl` â€” Polishing iteration k

K-Weaver models also enforce **UL applicability**: when read N50 < 50 Kb, UL predictions are marked non-applicable with capped confidence, and the pipeline warns at assembly time.

---

## ðŸ§ª Training Data Generation

Training data is generated from **synthetic diploid genomes** that produce realistic assembly graphs with known ground truth.

### Quick Start

```bash
# Generate training data (graph-only mode, ~7 min/genome)
strandweaver train generate-training-data \
    --num-genomes 25 \
    --genome-size 1000000 \
    --graph-only \
    --output-dir training_data/batch_01

# Generate with custom parameters
strandweaver train generate-training-data \
    --num-genomes 25 \
    --genome-size 1000000 \
    --graph-only \
    --repeat-fraction 0.50 \
    --gc-content 0.38 \
    --sv-density 2e-5 \
    --hifi-coverage 30 \
    --ont-coverage 20 \
    --hic-coverage 15 \
    --output-dir training_data/batch_02
```

### Graph-Only Mode

The `--graph-only` flag simulates reads **in-memory** for realistic coverage, overlap, and sequence features without writing FASTQ/FASTA to disk. This provides:

| Metric | Full Mode | Graph-Only |
|--------|-----------|------------|
| Time per genome | ~23 min | ~7 min |
| Disk per genome | ~230 MB | ~8.5 MB |
| **Speedup** | â€” | **3.3Ã—** |
| **Disk savings** | â€” | **27Ã—** |

### Recommended Batch Diversity

For robust model training, generate batches across a range of genome parameters:

| Parameter | Range | Purpose |
|-----------|-------|---------|
| Repeat fraction | 0.20â€“0.65 | Captures simple â†’ highly repetitive genomes |
| GC content | 0.32â€“0.55 | AT-rich to GC-rich variation |
| SV density | 1e-5â€“5e-5 | Low to high structural variation |
| Read types | Mix of all | Multi-technology generalization |

The pre-trained models were trained on **200 genomes** across 8 diverse batches (850 CSVs, ~20.2M rows, 1.4 GB).

---

## ðŸš€ Custom Training

### Train All Models

```bash
# Train using local data (hybrid resampling applied automatically)
strandweaver train train-models \
    --data-dir training_data/ \
    --output-dir trained_models/ \
    --cv-folds 5 \
    --val-split 0.15 \
    --seed 42
```

### Train Individual Models

```bash
# Retrain only EdgeWarden
strandweaver train train-models \
    --data-dir training_data/ \
    --output-dir trained_models/ \
    --model edge_ai

# Retrain only DiploidAI
strandweaver train train-models \
    --data-dir training_data/ \
    --output-dir trained_models/ \
    --model diploid_ai
```

### Validate Models

```bash
# Run 5-fold cross-validation on trained models
strandweaver train evaluate-models \
    --data-dir training_data/ \
    --model-dir trained_models/ \
    --cv-folds 5
```

---

## â˜ï¸ Colab GPU Training

For large-scale hyperparameter sweeps, we recommend Google Colab with a T4 GPU.

### Phase 0a: K-Weaver Training

1. **Open** `strandweaver/training/notebooks/KWeaver_Training_Colab.ipynb`

2. **Self-contained**: Generates 300 synthetic assembly benchmarks internally â€” no external data upload needed

3. **Run all cells** â€” Optuna sweeps 20 trials per model with CUDA/hist

4. **Download** tarball â†’ extract to `trained_models/kweaver/`

### Phase 0b: ErrorSmith Training

1. **Open** `strandweaver/training/notebooks/ErrorSmith_Training_Colab.ipynb`

2. **Upload** CHM13 BAMs or use automatic SRA download

3. **Run all cells** â€” minimap2 alignment + error profiling + model training

4. **Download** trained model to `trained_models/errorsmith/`

### Phase 1: XGBoost Sweep

1. **Package training data**:
   ```bash
   # Creates graph_csvs.tar.gz (~401 MB for 200 genomes)
   ./scripts/package_training_data.sh
   ```

2. **Upload** `graph_csvs.tar.gz` to Google Colab

3. **Open** `notebooks/XGBoost_Retraining_Colab.ipynb` and run all cells

4. **Download** retrained `.pkl` files to `trained_models/`

### Phase 2: GNN Training

1. **Upload** graph CSVs + trained `edgewarden/` models from Phase 1

2. **Open** `notebooks/PathGNN_Training_Colab.ipynb` and run all cells

3. **Download** `pathgnn_model.pt` to `trained_models/pathgnn/`

### Hyperparameter Search Space

<details>
<summary>XGBoost sweep parameters</summary>

| Parameter | Range |
|-----------|-------|
| `n_estimators` | 100â€“1000 |
| `max_depth` | 3â€“14 |
| `learning_rate` | 0.01â€“0.12 |
| `subsample` | 0.7â€“1.0 |
| `colsample_bytree` | 0.7â€“1.0 |
| `min_child_weight` | 1â€“10 |
| `gamma` | 0â€“0.35 |
| `tree_method` | `gpu_hist` |

</details>

<details>
<summary>GNN sweep parameters</summary>

| Parameter | Values |
|-----------|--------|
| Hidden channels | 64, 128 |
| Layers | 3, 4 |
| Dropout | 0.1, 0.3 |
| Learning rate | 1e-3, 5e-4 |
| Attention heads | 4 |
| Convolution | EdgeConvLayer + GATv2Conv |

</details>

---

## ðŸ“Š Model Performance

### Current Model Metrics (v2)

| Model | Accuracy | F1 (weighted) | F1 (macro) | CV (5-fold) |
|-------|----------|----------------|------------|-------------|
| ðŸ›¡ï¸ EdgeWarden | 0.881 | 0.880 | 0.896 | 0.878 Â± 0.002 |
| ðŸ§¬ PathGNN | 0.897 | 0.897 | 0.897 | 0.897 Â± 0.001 |
| ðŸ”€ DiploidAI | 0.862 | 0.862 | 0.862 | 0.858 Â± 0.001 |
| ðŸ” SVScribe | 0.823 | 0.828 | 0.557 | 0.817 Â± 0.005 |
| ðŸ§µ ThreadCompass | RÂ²=0.997 | RMSE=0.0004 | â€” | RÂ²=0.997 Â± 0.0003 |

### K-Weaver Metrics (v1)

Trained on 300 synthetic assembly benchmarks using Optuna (20 trials) + 5-fold CV, CUDA/hist tree method.

| Model | Target | RÂ² | MAE | CV (5-fold) |
|-------|--------|-----|-----|-------------|
| ðŸ§  DBG | Best DBG k | 0.863 | 0.63 | 0.863 Â± 0.064 |
| ðŸ§  UL Overlap | Best UL overlap k | 0.982 | 9.36 | 0.982 Â± 0.020 |
| ðŸ§  Extension | Best extension k | 0.849 | 1.20 | 0.849 Â± 0.074 |
| ðŸ§  Polish | Best polish k | 0.881 | 1.51 | 0.881 Â± 0.067 |

### EdgeWarden Per-Class Performance

| Edge Class | F1 Score | Description |
|------------|----------|-------------|
| TRUE | 0.968 | Genuine overlaps |
| ALLELIC | 1.000 | Haplotype-specific edges |
| CHIMERIC | 0.816 | Chimeric / artifact edges |
| SV_BREAK | 0.803 | Structural variant boundaries |

### SVScribe Per-Class Performance

| SV Type | F1 Score |
|---------|----------|
| Deletion | 0.716 |
| Duplication | 0.410 |
| Inversion | 0.338 |
| Insertion | 0.319 |
| None | 1.000 |

> **Note**: SV minority classes (insertion, inversion) have lower F1 due to inherently sparse training examples (~49 SVs per 1 Mb genome). Custom training with higher `--sv-density` can improve these for SV-rich organisms.

---

## ðŸ”¬ Advanced: Class Imbalance

Assembly graph edge labels are naturally imbalanced (CHIMERIC edges outnumber ALLELIC edges by 541Ã—). StrandWeaver handles this automatically with **hybrid resampling**:

1. **Undersample** majority classes to a 100,000-sample cap
2. **Oversample** minority classes to the new median

This strategy activates automatically when the class imbalance ratio exceeds 5.0Ã—.

| Metric | Without Resampling | With Hybrid Resampling |
|--------|-------------------|----------------------|
| F1-macro | 0.623 | **0.829** (+33%) |
| ALLELIC F1 | 0.391 | **1.000** (+156%) |
| Training samples | 1,182,624 | ~285,000 |

For custom training, hybrid resampling is the default behavior. To disable it:

```bash
strandweaver train train-models \
    --data-dir training_data/ \
    --output-dir trained_models/ \
    --no-rebalance
```

---

## ðŸ“š Additional Resources

- [Main README](../README.md) â€” Full pipeline documentation
- [Training Report](training_report.json) â€” Detailed metrics for all models
- [K-Weaver Training Results](kweaver/training_results.json) â€” K-Weaver model metrics
- [K-Weaver Training Notebook](../strandweaver/training/notebooks/KWeaver_Training_Colab.ipynb)
- [ErrorSmith Training Notebook](../strandweaver/training/notebooks/ErrorSmith_Training_Colab.ipynb)
- [XGBoost Colab Notebook](../Non_Main_Commit_Files/notebooks/XGBoost_Retraining_Colab.ipynb)
- [PathGNN Colab Notebook](../Non_Main_Commit_Files/notebooks/PathGNN_Training_Colab.ipynb)

---

## ðŸ“„ License

StrandWeaver is dual-licensed under [Academic](../docs/LICENSE_ACADEMIC.md) and [Commercial](../docs/LICENSE_COMMERCIAL.md) licenses. See the main repository for details.

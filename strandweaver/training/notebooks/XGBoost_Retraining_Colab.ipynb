{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d79ad74",
   "metadata": {},
   "source": [
    "# ðŸ§¬ StrandWeaver XGBoost Retraining on Colab (GPU)\n",
    "\n",
    "Retrain all **7 XGBoost models** with **hybrid resampling**, **focal loss**,\n",
    "and **GPU-accelerated hyperparameter tuning** using XGBoost 2.x\n",
    "(`device='cuda'`, `tree_method='hist'`).\n",
    "\n",
    "**v2.1 Improvements (S1-S4):**\n",
    "- **S1**: SV-dense training data (10-50Ã— higher SV density for balanced classes)\n",
    "- **S2**: Two-stage SV detection â€” binary detector (SV vs none) + 4-class subtype\n",
    "- **S3**: Focal loss objective for SV models (down-weights easy examples)\n",
    "- **S4**: 8 new SV-specific features (19 â†’ 27 total)\n",
    "\n",
    "**What this does:**\n",
    "1. Mounts Google Drive and extracts pre-generated graph CSVs (200 genomes, 1.4 GB)\n",
    "2. Applies hybrid resampling for class-imbalanced tasks (undersample majority\n",
    "   to 100k + oversample minorities to median)\n",
    "3. Runs **Optuna Bayesian hyperparameter sweep** across all 7 models on GPU\n",
    "4. Retrains with sweep-winning configs + 5-fold cross-validation\n",
    "5. Saves model weights + metrics back to Google Drive for download\n",
    "\n",
    "**Hybrid strategy** was benchmarked against 6 alternatives on 1.2M edges from\n",
    "200 synthetic genomes: **+33% F1-macro** over the previous class-weighting baseline.\n",
    "\n",
    "**Runtime:** Set to **GPU** via `Runtime â†’ Change runtime type â†’ T4 GPU`\n",
    "\n",
    "**Prep (local, one-time):**\n",
    "```bash\n",
    "cd strandweaver      # dev branch\n",
    "./scripts/package_training_data.sh   # â†’ graph_csvs.tar.gz (~400 MB)\n",
    "# Upload graph_csvs.tar.gz to Google Drive: My Drive/Colab Notebooks/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d97de8",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8622f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ“ GPU detected: {result.stdout.strip()}\")\n",
    "    GPU_AVAILABLE = True\n",
    "else:\n",
    "    print(\"âš  No GPU detected â€” will use CPU (slower)\")\n",
    "    GPU_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf763b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q xgboost scikit-learn numpy pandas optuna\n",
    "print(\"\\nâœ“ Dependencies installed (including Optuna for Bayesian HP search)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7f982",
   "metadata": {},
   "source": [
    "## 2. Load Training Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Mount Google Drive & extract training data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from google.colab import drive\n",
    "import os, tarfile, glob, shutil\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GDRIVE_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "TARBALL = os.path.join(GDRIVE_DIR, 'graph_csvs.tar.gz')\n",
    "OUTPUT_DIR = '/content/trained_models_v2'\n",
    "GDRIVE_OUTPUT = os.path.join(GDRIVE_DIR, 'trained_models_v2.tar.gz')\n",
    "\n",
    "# Model save subdirectories\n",
    "SAVE_MAP = {\n",
    "    'edge_ai':       'edgewarden',\n",
    "    'path_gnn':      'pathgnn',\n",
    "    'diploid_ai':    'diploid',\n",
    "    'ul_routing':    'ul_routing',\n",
    "    'sv_ai':         'sv_detector',\n",
    "    'sv_ai_binary':  'sv_detector',\n",
    "    'sv_ai_subtype': 'sv_detector',\n",
    "}\n",
    "TECH_LIST = ['hifi', 'ont_r9', 'ont_r10', 'illumina', 'adna']\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Extract tarball to local SSD (much faster I/O than Drive)\n",
    "assert os.path.exists(TARBALL), f\"Tarball not found at {TARBALL}\"\n",
    "print(f\"Extracting {TARBALL} ...\")\n",
    "with tarfile.open(TARBALL, 'r:gz') as tar:\n",
    "    tar.extractall('/content/')\n",
    "\n",
    "DATA_DIR = '/content/training_data_10x' if os.path.isdir('/content/training_data_10x') else '/content/training_data'\n",
    "\n",
    "all_csvs = glob.glob(f'{DATA_DIR}/**/*.csv', recursive=True)\n",
    "print(f\"âœ“ Extracted {len(all_csvs)} CSVs to {DATA_DIR}\")\n",
    "print(f\"âœ“ Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"âœ“ Results will be saved back to: {GDRIVE_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abdf07",
   "metadata": {},
   "source": [
    "## 3. Model Definitions & Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "\n",
    "# â”€â”€ Schema v2.0 â€” Feature definitions (must match graph_training_data.py) â”€â”€\n",
    "\n",
    "# Metadata columns prepended to every CSV row (skipped during training)\n",
    "METADATA_COLUMNS = [\n",
    "    'genome_id', 'genome_size', 'chromosome_id', 'read_technology',\n",
    "    'coverage_depth', 'error_rate', 'ploidy', 'gc_content_global',\n",
    "    'repeat_density_global', 'heterozygosity_rate', 'random_seed',\n",
    "    'generator_version', 'schema_version',\n",
    "]\n",
    "\n",
    "EDGE_AI_FEATURES = [\n",
    "    'overlap_length', 'overlap_identity', 'read1_length', 'read2_length',\n",
    "    'coverage_r1', 'coverage_r2', 'gc_content_r1', 'gc_content_r2',\n",
    "    'repeat_fraction_r1', 'repeat_fraction_r2',\n",
    "    'kmer_diversity_r1', 'kmer_diversity_r2',\n",
    "    'branching_factor_r1', 'branching_factor_r2',\n",
    "    'hic_support', 'mapping_quality_r1', 'mapping_quality_r2',\n",
    "    # v2.0: graph topology\n",
    "    'clustering_coeff_r1', 'clustering_coeff_r2', 'component_size',\n",
    "    # v2.0: sequence complexity\n",
    "    'entropy_r1', 'entropy_r2', 'homopolymer_max_r1', 'homopolymer_max_r2',\n",
    "]\n",
    "\n",
    "EDGE_AI_PROVENANCE = [\n",
    "    'node_id_r1', 'node_id_r2',\n",
    "    'read1_haplotype', 'read2_haplotype',\n",
    "    'genomic_distance', 'is_repeat_region',\n",
    "]\n",
    "\n",
    "PATH_GNN_FEATURES = [\n",
    "    'overlap_length', 'overlap_identity', 'coverage_consistency',\n",
    "    'gc_similarity', 'repeat_match', 'branching_score',\n",
    "    'path_support', 'hic_contact', 'mapping_quality',\n",
    "    'kmer_match', 'sequence_complexity', 'orientation_score',\n",
    "    'distance_score', 'topology_score', 'ul_support', 'sv_evidence',\n",
    "]\n",
    "\n",
    "PATH_GNN_PROVENANCE = [\n",
    "    'node_id_r1', 'node_id_r2',\n",
    "    'read1_haplotype', 'read2_haplotype',\n",
    "    'genomic_distance', 'is_repeat_region',\n",
    "]\n",
    "\n",
    "NODE_SIGNAL_FEATURES = [\n",
    "    'coverage', 'gc_content', 'repeat_fraction', 'kmer_diversity',\n",
    "    'branching_factor', 'hic_contact_density', 'allele_frequency',\n",
    "    'heterozygosity', 'phase_consistency', 'mappability',\n",
    "    'hic_intra_contacts', 'hic_inter_contacts',\n",
    "    'hic_contact_ratio', 'hic_phase_signal',\n",
    "    # v2.0: graph topology\n",
    "    'clustering_coeff', 'component_size',\n",
    "    # v2.0: sequence complexity\n",
    "    'shannon_entropy', 'dinucleotide_bias',\n",
    "    'homopolymer_max_run', 'homopolymer_density', 'low_complexity_fraction',\n",
    "    # v2.0: coverage distribution\n",
    "    'coverage_skewness', 'coverage_kurtosis', 'coverage_cv',\n",
    "    'coverage_p10', 'coverage_p90',\n",
    "]\n",
    "\n",
    "NODE_PROVENANCE = [\n",
    "    'node_id', 'read_haplotype', 'read_start_pos', 'read_end_pos',\n",
    "    'read_length', 'is_in_repeat', 'read_technology',\n",
    "]\n",
    "\n",
    "UL_ROUTE_FEATURES = [\n",
    "    'path_length', 'num_branches', 'coverage_mean', 'coverage_std',\n",
    "    'sequence_identity', 'mapping_quality', 'num_gaps', 'gap_size_mean',\n",
    "    'kmer_consistency', 'orientation_consistency', 'ul_span', 'route_complexity',\n",
    "]\n",
    "\n",
    "SV_DETECT_FEATURES = [\n",
    "    'coverage_mean', 'coverage_std', 'coverage_median',\n",
    "    'gc_content', 'repeat_fraction', 'kmer_diversity',\n",
    "    'branching_complexity', 'hic_disruption_score',\n",
    "    'ul_support', 'mapping_quality',\n",
    "    'region_length', 'breakpoint_precision',\n",
    "    'allele_balance', 'phase_switch_rate',\n",
    "    # v2.0: coverage distribution\n",
    "    'coverage_cv', 'coverage_skewness', 'coverage_kurtosis',\n",
    "    'coverage_p10', 'coverage_p90',\n",
    "    # v2.1: SV-specific features (S4 improvement)\n",
    "    'depth_ratio_flank', 'split_read_count', 'clip_fraction',\n",
    "    'bubble_size', 'path_divergence', 'ul_spanning',\n",
    "    'coverage_drop_magnitude', 'orientation_switch_rate',\n",
    "]\n",
    "\n",
    "# Columns to skip when loading CSVs (metadata + provenance)\n",
    "_NON_FEATURE_COLUMNS = set(METADATA_COLUMNS) | set(EDGE_AI_PROVENANCE) | set(\n",
    "    PATH_GNN_PROVENANCE) | set(NODE_PROVENANCE)\n",
    "\n",
    "# â”€â”€ Model specifications â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# GPU-tuned XGBoost parameters (XGBoost 2.x API):\n",
    "#   tree_method='hist'  â€” unified histogram method (gpu_hist is deprecated)\n",
    "#   device='cuda'       â€” run on GPU (falls back to 'cpu' if no GPU)\n",
    "#   max_bin=1024        â€” finer splits, uses more GPU memory (default 256)\n",
    "DEVICE = 'cuda' if GPU_AVAILABLE else 'cpu'\n",
    "GPU_PARAMS = {\n",
    "    'tree_method': 'hist',\n",
    "    'device': DEVICE,\n",
    "    'max_bin': 1024,\n",
    "}\n",
    "print(f\"Device: {DEVICE}, max_bin: {GPU_PARAMS['max_bin']}\")\n",
    "\n",
    "MODEL_SPECS = {\n",
    "    'edge_ai': {\n",
    "        'csv_glob': '**/edge_ai_training_g*.csv',\n",
    "        'features': EDGE_AI_FEATURES,\n",
    "        'label_col': 'label',\n",
    "        'task': 'multiclass',\n",
    "        'xgb_params': {\n",
    "            'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 100,\n",
    "            **GPU_PARAMS,\n",
    "        },\n",
    "        'desc': 'Edge scoring (TRUE/ALLELIC/CHIMERIC/SV_BREAK/REPEAT)',\n",
    "    },\n",
    "    'path_gnn': {\n",
    "        'csv_glob': '**/path_gnn_training_g*.csv',\n",
    "        'features': PATH_GNN_FEATURES,\n",
    "        'label_col': 'in_correct_path',\n",
    "        'task': 'binary',\n",
    "        'xgb_params': {\n",
    "            'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 100,\n",
    "            **GPU_PARAMS,\n",
    "        },\n",
    "        'desc': 'Path edge scoring (binary)',\n",
    "    },\n",
    "    'diploid_ai': {\n",
    "        'csv_glob': '**/diploid_ai_training_g*.csv',\n",
    "        'features': NODE_SIGNAL_FEATURES,\n",
    "        'label_col': 'haplotype_label',\n",
    "        'task': 'multiclass',\n",
    "        'label_transform': lambda lbl: lbl.replace('HAP_', ''),\n",
    "        'xgb_params': {\n",
    "            'max_depth': 10, 'learning_rate': 0.03, 'n_estimators': 500,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "            'min_child_weight': 5, 'gamma': 0.1,\n",
    "            'reg_alpha': 0.1, 'reg_lambda': 1.0,\n",
    "            **GPU_PARAMS,\n",
    "        },\n",
    "        'desc': 'Haplotype phasing (A/B) â€” v2.0 topology+complexity features',\n",
    "    },\n",
    "    'ul_routing': {\n",
    "        'csv_glob': '**/ul_route_training_g*.csv',\n",
    "        'features': UL_ROUTE_FEATURES,\n",
    "        'label_col': 'route_score',\n",
    "        'task': 'regression',\n",
    "        'xgb_params': {\n",
    "            'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 100,\n",
    "            **GPU_PARAMS,\n",
    "        },\n",
    "        'desc': 'Ultra-long routing score (regression, continuous 0-1)',\n",
    "    },\n",
    "    'sv_ai': {\n",
    "        'csv_glob': '**/sv_detect_training_g*.csv',\n",
    "        'features': SV_DETECT_FEATURES,\n",
    "        'label_col': 'sv_type',\n",
    "        'task': 'multiclass',\n",
    "        'focal_loss': True,\n",
    "        'focal_alpha': 0.25,\n",
    "        'focal_gamma': 2.0,\n",
    "        'xgb_params': {\n",
    "            'max_depth': 8, 'learning_rate': 0.05, 'n_estimators': 300,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "            'min_child_weight': 3,\n",
    "            **GPU_PARAMS,\n",
    "        },\n",
    "        'desc': 'SV detection (5-class, focal loss) â€” v2.1 27 features',\n",
    "    },\n",
    "    'sv_ai_binary': {\n",
    "        'csv_glob': '**/sv_detect_training_g*.csv',\n",
    "        'features': SV_DETECT_FEATURES,\n",
    "        'label_col': 'sv_type',\n",
    "        'task': 'binary',\n",
    "        'label_transform': lambda lbl: 0 if lbl == 'none' else 1,\n",
    "        'xgb_params': {\n",
    "            'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 200,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "            **GPU_PARAMS,\n",
    "        },\n",
    "        'desc': 'SV binary detector (SV vs no-SV, stage 1 of two-stage)',\n",
    "    },\n",
    "    'sv_ai_subtype': {\n",
    "        'csv_glob': '**/sv_detect_training_g*.csv',\n",
    "        'features': SV_DETECT_FEATURES,\n",
    "        'label_col': 'sv_type',\n",
    "        'task': 'multiclass',\n",
    "        'label_filter': lambda lbl: lbl != 'none',\n",
    "        'focal_loss': True,\n",
    "        'focal_alpha': 0.25,\n",
    "        'focal_gamma': 2.0,\n",
    "        'xgb_params': {\n",
    "            'max_depth': 8, 'learning_rate': 0.05, 'n_estimators': 300,\n",
    "            'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "            'min_child_weight': 3,\n",
    "            **GPU_PARAMS,\n",
    "        },\n",
    "        'desc': 'SV subtype classifier (4-class, focal loss, stage 2 of two-stage)',\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"\\nSchema v2.1 â€” Defined {len(MODEL_SPECS)} models\")\n",
    "for name, spec in MODEL_SPECS.items():\n",
    "    fl = ' [focal]' if spec.get('focal_loss') else ''\n",
    "    lf = ' [filtered]' if spec.get('label_filter') else ''\n",
    "    print(f\"  {name}: {spec['desc']} ({len(spec['features'])} features{fl}{lf})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Resampling & Training utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def undersample(X, y, max_per_class=100_000, rng=None):\n",
    "    \"\"\"Downsample classes with more than max_per_class samples.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    keep = []\n",
    "    for cls, cnt in zip(classes, counts):\n",
    "        idx = np.where(y == cls)[0]\n",
    "        if cnt > max_per_class:\n",
    "            idx = rng.choice(idx, max_per_class, replace=False)\n",
    "        keep.append(idx)\n",
    "    keep = np.concatenate(keep)\n",
    "    rng.shuffle(keep)\n",
    "    return X[keep], y[keep]\n",
    "\n",
    "\n",
    "def oversample(X, y, target_count=None, rng=None):\n",
    "    \"\"\"Random-oversample minority classes up to target_count (default: median class size).\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    if target_count is None:\n",
    "        target_count = int(np.median(counts))\n",
    "    parts_X, parts_y = [X], [y]\n",
    "    for cls, cnt in zip(classes, counts):\n",
    "        if cnt < target_count:\n",
    "            idx = np.where(y == cls)[0]\n",
    "            extra = target_count - cnt\n",
    "            sampled = rng.choice(idx, extra, replace=True)\n",
    "            parts_X.append(X[sampled])\n",
    "            parts_y.append(y[sampled])\n",
    "    return np.concatenate(parts_X), np.concatenate(parts_y)\n",
    "\n",
    "\n",
    "def hybrid_resample(X, y, max_majority=100_000, rng=None):\n",
    "    \"\"\"Hybrid: undersample majority to max_majority, then oversample minorities to new median.\n",
    "\n",
    "    Benchmarked against 6 alternatives on 1.2 M edges from 200 synthetic\n",
    "    genomes: +33% F1-macro over class-weighting baseline.\n",
    "    \"\"\"\n",
    "    X_u, y_u = undersample(X, y, max_per_class=max_majority, rng=rng)\n",
    "    X_h, y_h = oversample(X_u, y_u, rng=rng)\n",
    "    return X_h, y_h\n",
    "\n",
    "\n",
    "# â”€â”€ Focal Loss for class-imbalanced classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _focal_loss_objective(alpha=0.25, gamma=2.0, n_classes=5):\n",
    "    \"\"\"Return (objective_fn, eval_metric_fn) for XGBoost custom focal loss.\n",
    "\n",
    "    Focal loss down-weights easy/well-classified examples so the model\n",
    "    focuses on hard minority cases like inversions and duplications.\n",
    "\n",
    "    References:\n",
    "        Lin et al., \"Focal Loss for Dense Object Detection\", ICCV 2017\n",
    "    \"\"\"\n",
    "    def focal_obj(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "        labels = dtrain.get_label().astype(int)\n",
    "        n = len(labels)\n",
    "        # predt shape: (n * n_classes,) â€” reshape to (n, n_classes)\n",
    "        preds = predt.reshape(n, n_classes)\n",
    "        # Softmax\n",
    "        preds = preds - preds.max(axis=1, keepdims=True)\n",
    "        exp_p = np.exp(preds)\n",
    "        softmax = exp_p / exp_p.sum(axis=1, keepdims=True)\n",
    "        softmax = np.clip(softmax, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        one_hot = np.zeros_like(softmax)\n",
    "        one_hot[np.arange(n), labels] = 1.0\n",
    "\n",
    "        # p_t = probability of true class\n",
    "        p_t = (softmax * one_hot).sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Focal weight: alpha * (1 - p_t)^gamma\n",
    "        focal_weight = alpha * (1.0 - p_t) ** gamma\n",
    "\n",
    "        # Gradient and hessian of focal cross-entropy\n",
    "        grad = focal_weight * (softmax - one_hot)\n",
    "        hess = focal_weight * softmax * (1.0 - softmax)\n",
    "        # Ensure positive hessian\n",
    "        hess = np.maximum(hess, 1e-7)\n",
    "\n",
    "        return grad.reshape(-1), hess.reshape(-1)\n",
    "\n",
    "    def focal_eval(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "        labels = dtrain.get_label().astype(int)\n",
    "        n = len(labels)\n",
    "        preds = predt.reshape(n, n_classes)\n",
    "        preds = preds - preds.max(axis=1, keepdims=True)\n",
    "        exp_p = np.exp(preds)\n",
    "        softmax = exp_p / exp_p.sum(axis=1, keepdims=True)\n",
    "        softmax = np.clip(softmax, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        p_t = softmax[np.arange(n), labels]\n",
    "        loss = -alpha * ((1.0 - p_t) ** gamma) * np.log(p_t)\n",
    "        return 'focal_loss', float(np.mean(loss))\n",
    "\n",
    "    return focal_obj, focal_eval\n",
    "\n",
    "\n",
    "def load_csvs(data_dir, csv_glob, features, label_col,\n",
    "              label_transform=None, label_filter=None):\n",
    "    \"\"\"Load and concatenate all matching CSVs into X, y arrays.\n",
    "\n",
    "    v2.1 schema: Supports label_filter (callable) to exclude rows\n",
    "    (e.g., remove 'none' class for subtype-only training).\n",
    "    \"\"\"\n",
    "    csv_files = sorted(glob.glob(f'{data_dir}/{csv_glob}', recursive=True))\n",
    "    if not csv_files:\n",
    "        return None, None, None\n",
    "\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            if all(c in df.columns for c in features + [label_col]):\n",
    "                # Apply label_filter before appending\n",
    "                if label_filter is not None:\n",
    "                    mask = df[label_col].apply(lambda v: label_filter(str(v)))\n",
    "                    df = df[mask]\n",
    "                    if len(df) == 0:\n",
    "                        continue\n",
    "                dfs.append(df)\n",
    "            else:\n",
    "                missing = [c for c in features + [label_col] if c not in df.columns]\n",
    "                print(f\"  \\u26a0 Skipping {Path(f).name}: missing columns {missing[:5]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \\u26a0 Error reading {Path(f).name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not dfs:\n",
    "        return None, None, None\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    if 'schema_version' in combined.columns:\n",
    "        versions = combined['schema_version'].unique()\n",
    "        print(f\"  Schema version(s): {list(versions)}\")\n",
    "\n",
    "    X = combined[features].values.astype(np.float32)\n",
    "    labels = combined[label_col].values\n",
    "    if label_transform:\n",
    "        labels = np.array([label_transform(str(l)) for l in labels])\n",
    "    label_dist = Counter(labels)\n",
    "    return X, labels, label_dist\n",
    "\n",
    "\n",
    "def train_model(X, y, xgb_params, task, val_split=0.15, seed=42,\n",
    "                focal_loss_config=None):\n",
    "    \"\"\"Train XGBoost with val split, return (model, metrics) with per-class F1.\n",
    "\n",
    "    focal_loss_config: dict with keys 'alpha', 'gamma', 'n_classes' to enable\n",
    "    focal loss objective instead of default multi:softprob.\n",
    "    \"\"\"\n",
    "    is_binary = task == 'binary' or len(set(y.tolist())) == 2\n",
    "\n",
    "    if task == 'regression':\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "            X, y, test_size=val_split, random_state=seed)\n",
    "        params = dict(xgb_params, random_state=seed, verbosity=0,\n",
    "                      early_stopping_rounds=10, objective='reg:squarederror',\n",
    "                      eval_metric='rmse')\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "        y_pred = model.predict(X_va)\n",
    "        rmse = float(np.sqrt(mean_squared_error(y_va, y_pred)))\n",
    "        r2 = float(r2_score(y_va, y_pred))\n",
    "        return model, {'val_rmse': round(rmse, 4), 'val_r2': round(r2, 4),\n",
    "                       'train_size': len(X_tr), 'val_size': len(X_va)}\n",
    "\n",
    "    # Classification (no sample_weight â€” we use resampling instead)\n",
    "    stratify = y if len(set(y.tolist())) > 1 else None\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "        X, y, test_size=val_split, random_state=seed, stratify=stratify)\n",
    "\n",
    "    params = dict(xgb_params, random_state=seed, use_label_encoder=False,\n",
    "                  verbosity=0, early_stopping_rounds=10)\n",
    "\n",
    "    # â”€â”€ Focal loss path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if focal_loss_config and not is_binary:\n",
    "        n_classes = focal_loss_config.get('n_classes', int(len(set(y.tolist()))))\n",
    "        alpha = focal_loss_config.get('alpha', 0.25)\n",
    "        gamma = focal_loss_config.get('gamma', 2.0)\n",
    "        focal_obj, focal_eval = _focal_loss_objective(alpha, gamma, n_classes)\n",
    "\n",
    "        params['disable_default_eval_metric'] = True\n",
    "        params['num_class'] = n_classes\n",
    "        # Remove conflicting keys\n",
    "        params.pop('objective', None)\n",
    "        params.pop('eval_metric', None)\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval = xgb.DMatrix(X_va, label=y_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params, dtrain,\n",
    "            num_boost_round=params.pop('n_estimators', 300),\n",
    "            obj=focal_obj,\n",
    "            custom_metric=focal_eval,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=params.pop('early_stopping_rounds', 10),\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        # Predict\n",
    "        raw_preds = bst.predict(dval)\n",
    "        y_pred = np.argmax(raw_preds.reshape(-1, n_classes), axis=1)\n",
    "        model = bst  # Return Booster (not XGBClassifier) for focal path\n",
    "\n",
    "    # â”€â”€ Standard classification path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    else:\n",
    "        if is_binary:\n",
    "            params.update({'objective': 'binary:logistic', 'eval_metric': 'logloss'})\n",
    "        else:\n",
    "            params.update({'objective': 'multi:softprob', 'eval_metric': 'mlogloss',\n",
    "                           'num_class': int(len(set(y.tolist())))})\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "        y_pred = model.predict(X_va)\n",
    "\n",
    "    acc = float(accuracy_score(y_va, y_pred))\n",
    "    f1_w = float(f1_score(y_va, y_pred, average='weighted', zero_division=0))\n",
    "    f1_m = float(f1_score(y_va, y_pred, average='macro', zero_division=0))\n",
    "\n",
    "    # Per-class F1\n",
    "    unique_classes = sorted(set(y_va.tolist()))\n",
    "    f1_per = f1_score(y_va, y_pred, labels=unique_classes, average=None, zero_division=0)\n",
    "    per_class_f1 = {str(c): round(float(f), 4) for c, f in zip(unique_classes, f1_per)}\n",
    "\n",
    "    metrics = {\n",
    "        'val_accuracy': round(acc, 4),\n",
    "        'val_f1_weighted': round(f1_w, 4),\n",
    "        'val_f1_macro': round(f1_m, 4),\n",
    "        'per_class_f1': per_class_f1,\n",
    "        'train_size': len(X_tr), 'val_size': len(X_va),\n",
    "    }\n",
    "    if focal_loss_config:\n",
    "        metrics['focal_loss'] = True\n",
    "        metrics['focal_alpha'] = focal_loss_config.get('alpha', 0.25)\n",
    "        metrics['focal_gamma'] = focal_loss_config.get('gamma', 2.0)\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "def cv_model(X, y, xgb_params, task, n_folds=5, seed=42):\n",
    "    \"\"\"Manual k-fold CV (no sample_weight â€” resampling already applied).\"\"\"\n",
    "    is_binary = task == 'binary' or len(set(y.tolist())) == 2\n",
    "\n",
    "    if task == 'regression':\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        r2_scores = []\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            params = dict(xgb_params, random_state=seed, verbosity=0,\n",
    "                          early_stopping_rounds=10, objective='reg:squarederror',\n",
    "                          eval_metric='rmse')\n",
    "            m = xgb.XGBRegressor(**params)\n",
    "            m.fit(X[train_idx], y[train_idx],\n",
    "                  eval_set=[(X[val_idx], y[val_idx])], verbose=False)\n",
    "            r2_scores.append(r2_score(y[val_idx], m.predict(X[val_idx])))\n",
    "        return {'cv_r2_mean': round(np.mean(r2_scores), 4),\n",
    "                'cv_r2_std': round(np.std(r2_scores), 4),\n",
    "                'cv_fold_scores': [round(s, 4) for s in r2_scores]}\n",
    "\n",
    "    # Classification CV\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    fold_accs = []\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        params = dict(xgb_params, random_state=seed, use_label_encoder=False,\n",
    "                      verbosity=0, early_stopping_rounds=10)\n",
    "        if is_binary:\n",
    "            params.update({'objective': 'binary:logistic', 'eval_metric': 'logloss'})\n",
    "        else:\n",
    "            params.update({'objective': 'multi:softprob', 'eval_metric': 'mlogloss',\n",
    "                           'num_class': int(len(set(y.tolist())))})\n",
    "\n",
    "        m = xgb.XGBClassifier(**params)\n",
    "        m.fit(X[train_idx], y[train_idx],\n",
    "              eval_set=[(X[val_idx], y[val_idx])], verbose=False)\n",
    "        fold_accs.append(accuracy_score(y[val_idx], m.predict(X[val_idx])))\n",
    "\n",
    "    return {'cv_accuracy_mean': round(np.mean(fold_accs), 4),\n",
    "            'cv_accuracy_std': round(np.std(fold_accs), 4),\n",
    "            'cv_fold_scores': [round(s, 4) for s in fold_accs]}\n",
    "\n",
    "\n",
    "print(\"Training utilities defined \\u2713 (hybrid resampling + focal loss + per-class F1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6a24f",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Sweep (Optuna Bayesian Optimization, GPU-Accelerated)\n",
    "\n",
    "Uses **Optuna TPE** (Tree-structured Parzen Estimator) instead of brute-force\n",
    "grid search. This explores the hyperparameter space ~10Ã— more efficiently by\n",
    "learning which regions are promising and pruning bad trials early.\n",
    "\n",
    "| Model | Trials | Focal Loss | Notes |\n",
    "|-------|--------|------------|-------|\n",
    "| edge_ai | 80 | â€” | 5-class edge scoring |\n",
    "| path_gnn | 60 | â€” | Binary path edge scoring |\n",
    "| diploid_ai | 150 | â€” | Haplotype phasing (biggest search) |\n",
    "| ul_routing | 60 | â€” | Regression (routing score) |\n",
    "| sv_ai | 100 | âœ“ | 5-class SV detection, 27 features |\n",
    "| sv_ai_binary | 80 | â€” | Binary SV detector (stage 1) |\n",
    "| sv_ai_subtype | 100 | âœ“ | 4-class SV subtype (stage 2) |\n",
    "\n",
    "**GPU utilization improvements**:\n",
    "- `device='cuda'` + `tree_method='hist'` â€” XGBoost 2.x unified GPU API\n",
    "- `max_bin=1024` (4Ã— default) â€” builds finer histograms in GPU memory\n",
    "- Optuna early pruning kills bad trials after 20 trees instead of waiting for 500\n",
    "\n",
    "**Resume support**: Optuna studies are backed by a SQLite DB at\n",
    "`sweep_optuna.db` in the output dir. Re-running this cell automatically\n",
    "resumes all studies from where they left off â€” no manual checkpointing needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7034993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from functools import partial\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# â”€â”€ Optuna search spaces per model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Each function defines the hyperparameter ranges for Optuna to explore.\n",
    "# Ranges are wider than the old grid â€” Optuna's TPE sampler focuses on\n",
    "# the promising regions automatically.\n",
    "\n",
    "def edge_ai_objective(trial, X, y, base_params, task, focal_loss_config=None):\n",
    "    hp = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "    }\n",
    "    params = dict(base_params, **hp)\n",
    "    _, metrics = train_model(X, y, params, task, focal_loss_config=focal_loss_config)\n",
    "    return metrics.get('val_f1_macro', 0)\n",
    "\n",
    "\n",
    "def path_gnn_objective(trial, X, y, base_params, task, focal_loss_config=None):\n",
    "    hp = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "    }\n",
    "    params = dict(base_params, **hp)\n",
    "    _, metrics = train_model(X, y, params, task, focal_loss_config=focal_loss_config)\n",
    "    return metrics.get('val_f1_macro', 0)\n",
    "\n",
    "\n",
    "def diploid_ai_objective(trial, X, y, base_params, task, focal_loss_config=None):\n",
    "    hp = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 14),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1200, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 5.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "    }\n",
    "    params = dict(base_params, **hp)\n",
    "    _, metrics = train_model(X, y, params, task, focal_loss_config=focal_loss_config)\n",
    "    return metrics.get('val_f1_macro', 0)\n",
    "\n",
    "\n",
    "def ul_routing_objective(trial, X, y, base_params, task, focal_loss_config=None):\n",
    "    hp = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "    }\n",
    "    params = dict(base_params, **hp)\n",
    "    _, metrics = train_model(X, y, params, task, focal_loss_config=focal_loss_config)\n",
    "    return metrics.get('val_r2', 0)\n",
    "\n",
    "\n",
    "def sv_ai_objective(trial, X, y, base_params, task, focal_loss_config=None):\n",
    "    hp = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 14),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "    }\n",
    "    params = dict(base_params, **hp)\n",
    "    _, metrics = train_model(X, y, params, task, focal_loss_config=focal_loss_config)\n",
    "    return metrics.get('val_f1_macro', 0)\n",
    "\n",
    "\n",
    "def sv_ai_binary_objective(trial, X, y, base_params, task, focal_loss_config=None):\n",
    "    hp = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 20.0),\n",
    "    }\n",
    "    params = dict(base_params, **hp)\n",
    "    _, metrics = train_model(X, y, params, task, focal_loss_config=focal_loss_config)\n",
    "    return metrics.get('val_f1_macro', 0)\n",
    "\n",
    "\n",
    "def sv_ai_subtype_objective(trial, X, y, base_params, task, focal_loss_config=None):\n",
    "    hp = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 14),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "    }\n",
    "    params = dict(base_params, **hp)\n",
    "    _, metrics = train_model(X, y, params, task, focal_loss_config=focal_loss_config)\n",
    "    return metrics.get('val_f1_macro', 0)\n",
    "\n",
    "\n",
    "OPTUNA_CONFIG = {\n",
    "    'edge_ai':       {'objective_fn': edge_ai_objective,       'n_trials': 80},\n",
    "    'path_gnn':      {'objective_fn': path_gnn_objective,      'n_trials': 60},\n",
    "    'diploid_ai':    {'objective_fn': diploid_ai_objective,     'n_trials': 150},\n",
    "    'ul_routing':    {'objective_fn': ul_routing_objective,     'n_trials': 60},\n",
    "    'sv_ai':         {'objective_fn': sv_ai_objective,          'n_trials': 100},\n",
    "    'sv_ai_binary':  {'objective_fn': sv_ai_binary_objective,   'n_trials': 80},\n",
    "    'sv_ai_subtype': {'objective_fn': sv_ai_subtype_objective,  'n_trials': 100},\n",
    "}\n",
    "\n",
    "# â”€â”€ Optuna storage (SQLite) for automatic resume â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DB_PATH = os.path.join(OUTPUT_DIR, 'sweep_optuna.db')\n",
    "storage = f'sqlite:///{DB_PATH}'\n",
    "\n",
    "sweep_results = {}\n",
    "t_sweep_start = time.time()\n",
    "\n",
    "for model_name, spec in MODEL_SPECS.items():\n",
    "    cfg = OPTUNA_CONFIG.get(model_name)\n",
    "    if not cfg:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Optuna sweep: {model_name} ({cfg['n_trials']} trials)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Load data (with label_filter support for sv_ai_subtype)\n",
    "    X, labels, label_dist = load_csvs(\n",
    "        DATA_DIR, spec['csv_glob'], spec['features'],\n",
    "        spec['label_col'], spec.get('label_transform'),\n",
    "        label_filter=spec.get('label_filter'))\n",
    "\n",
    "    if X is None:\n",
    "        print(f\"  No data, skipping\")\n",
    "        continue\n",
    "\n",
    "    if spec['task'] == 'regression':\n",
    "        y = labels.astype(np.float32)\n",
    "    elif spec['task'] == 'binary':\n",
    "        y = np.array([int(v) for v in labels], dtype=np.int32)\n",
    "    else:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(labels)\n",
    "\n",
    "    # Hybrid resampling if imbalanced\n",
    "    resampled = False\n",
    "    if spec['task'] in ('multiclass', 'binary'):\n",
    "        counts = list(label_dist.values())\n",
    "        imbalance = max(counts) / max(min(counts), 1)\n",
    "        if imbalance > 5.0:\n",
    "            rng = np.random.default_rng(42)\n",
    "            X, y = hybrid_resample(X, y, rng=rng)\n",
    "            resampled = True\n",
    "            print(f\"  Hybrid-resampled -> {len(y):,} samples\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Build focal loss config if spec has it\n",
    "    focal_loss_cfg = None\n",
    "    if spec.get('focal_loss') and spec['task'] == 'multiclass':\n",
    "        focal_loss_cfg = {\n",
    "            'alpha': spec.get('focal_alpha', 0.25),\n",
    "            'gamma': spec.get('focal_gamma', 2.0),\n",
    "            'n_classes': int(len(set(y.tolist()))),\n",
    "        }\n",
    "        print(f\"  Focal loss enabled (alpha={focal_loss_cfg['alpha']}, gamma={focal_loss_cfg['gamma']})\")\n",
    "\n",
    "    # Create or load existing Optuna study (resume-safe)\n",
    "    direction = 'maximize'\n",
    "    study = optuna.create_study(\n",
    "        study_name=model_name,\n",
    "        storage=storage,\n",
    "        load_if_exists=True,\n",
    "        direction=direction,\n",
    "        sampler=TPESampler(seed=42),\n",
    "        pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=20),\n",
    "    )\n",
    "\n",
    "    completed = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "    remaining = max(0, cfg['n_trials'] - completed)\n",
    "\n",
    "    if remaining == 0:\n",
    "        print(f\"  âœ“ Already done ({completed} trials, best={study.best_value:.4f})\")\n",
    "    else:\n",
    "        if completed > 0:\n",
    "            print(f\"  Resuming: {completed} done, {remaining} remaining (best so far: {study.best_value:.4f})\")\n",
    "        # Bind data into objective via functools.partial (avoids late-binding)\n",
    "        objective = partial(cfg['objective_fn'], X=X_scaled, y=y,\n",
    "                            base_params=spec['xgb_params'], task=spec['task'],\n",
    "                            focal_loss_config=focal_loss_cfg)\n",
    "        study.optimize(objective, n_trials=remaining, show_progress_bar=True)\n",
    "\n",
    "    # Extract best results\n",
    "    best_trial = study.best_trial\n",
    "    best_params = best_trial.params\n",
    "    best_score = best_trial.value\n",
    "\n",
    "    # Re-run best params to get full metrics (including per-class F1)\n",
    "    final_params = dict(spec['xgb_params'], **best_params)\n",
    "    _, best_metrics = train_model(X_scaled, y, final_params, spec['task'],\n",
    "                                   focal_loss_config=focal_loss_cfg)\n",
    "\n",
    "    sweep_results[model_name] = {\n",
    "        'status': 'complete',\n",
    "        'best_params': best_params,\n",
    "        'best_score': round(best_score, 4),\n",
    "        'best_metrics': best_metrics,\n",
    "        'n_trials': len(study.trials),\n",
    "        'resampled': resampled,\n",
    "        'focal_loss': bool(focal_loss_cfg),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n  âœ“ Best: score={best_score:.4f}  ({len(study.trials)} trials)\")\n",
    "    print(f\"    params={best_params}\")\n",
    "    if 'per_class_f1' in best_metrics:\n",
    "        for cls_name, f1_val in best_metrics['per_class_f1'].items():\n",
    "            print(f\"        class {cls_name:8s}  F1={f1_val:.4f}\")\n",
    "\n",
    "t_sweep_total = time.time() - t_sweep_start\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  Sweep complete! ({len(sweep_results)} models, {t_sweep_total:.0f}s total)\")\n",
    "print(f\"  Optuna DB: {DB_PATH}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40babd9b",
   "metadata": {},
   "source": [
    "### Sweep Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b77b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Sweep summary table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# If kernel restarted, reload from Optuna DB\n",
    "if not sweep_results:\n",
    "    _db = os.path.join(OUTPUT_DIR, 'sweep_optuna.db')\n",
    "    if os.path.exists(_db):\n",
    "        _storage = f'sqlite:///{_db}'\n",
    "        sweep_results = {}\n",
    "        for name in OPTUNA_CONFIG:\n",
    "            try:\n",
    "                study = optuna.load_study(study_name=name, storage=_storage)\n",
    "                best = study.best_trial\n",
    "                final_params = dict(MODEL_SPECS[name]['xgb_params'], **best.params)\n",
    "                sweep_results[name] = {\n",
    "                    'status': 'complete',\n",
    "                    'best_params': best.params,\n",
    "                    'best_score': round(best.value, 4),\n",
    "                    'n_trials': len(study.trials),\n",
    "                }\n",
    "                print(f\"  Loaded {name}: {len(study.trials)} trials\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No sweep results found â€” run the sweep cell first.\\n\")\n",
    "\n",
    "done = [m for m, sr in sweep_results.items() if sr.get('status') == 'complete']\n",
    "pending = [m for m in OPTUNA_CONFIG if m not in done]\n",
    "\n",
    "print(f\"{'Model':<15} {'Best Score':<14} {'Trials':<10} {'Best Params'}\")\n",
    "print('-' * 90)\n",
    "for name in OPTUNA_CONFIG:\n",
    "    sr = sweep_results.get(name)\n",
    "    if sr and sr.get('status') == 'complete':\n",
    "        print(f\"{name:<15} {sr['best_score']:<14.4f} {sr.get('n_trials','?'):<10} {sr['best_params']}\")\n",
    "\n",
    "if pending:\n",
    "    print(f\"\\nâš  {len(pending)} model(s) not yet swept: {pending}\")\n",
    "    print(\"  Re-run the sweep cell to continue.\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ All {len(done)} models swept successfully!\")\n",
    "\n",
    "# Save finalized sweep results as JSON too\n",
    "with open(os.path.join(OUTPUT_DIR, 'sweep_results.json'), 'w') as f:\n",
    "    json.dump(sweep_results, f, indent=2, default=str)\n",
    "print(f\"\\nSweep results saved -> {OUTPUT_DIR}/sweep_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6240954",
   "metadata": {},
   "source": [
    "## 8. Retrain with Best Hyperparameters\n",
    "\n",
    "Retrain all models using the sweep-winning hyperparameters, with full\n",
    "5-fold CV, and overwrite the previous models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa50e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Retrain all models with sweep-winning hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# If kernel restarted, reload sweep results from Optuna DB or JSON\n",
    "if not sweep_results:\n",
    "    _sr_path = os.path.join(OUTPUT_DIR, 'sweep_results.json')\n",
    "    _db = os.path.join(OUTPUT_DIR, 'sweep_optuna.db')\n",
    "    if os.path.exists(_sr_path):\n",
    "        with open(_sr_path) as f:\n",
    "            sweep_results = json.load(f)\n",
    "        print(f\"Loaded sweep results from {_sr_path}\")\n",
    "    elif os.path.exists(_db):\n",
    "        _storage = f'sqlite:///{_db}'\n",
    "        sweep_results = {}\n",
    "        for name in OPTUNA_CONFIG:\n",
    "            try:\n",
    "                study = optuna.load_study(study_name=name, storage=_storage)\n",
    "                sweep_results[name] = {\n",
    "                    'status': 'complete',\n",
    "                    'best_params': study.best_trial.params,\n",
    "                    'best_score': round(study.best_value, 4),\n",
    "                }\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"Loaded {len(sweep_results)} model(s) from Optuna DB\")\n",
    "\n",
    "print(\"Retraining with sweep-optimized hyperparameters...\\n\")\n",
    "\n",
    "final_models = {}\n",
    "final_results = {}\n",
    "t_start = time.time()\n",
    "\n",
    "for model_name, spec in MODEL_SPECS.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Retrain: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if model_name in sweep_results:\n",
    "        best_hp = sweep_results[model_name]['best_params']\n",
    "        xgb_params = dict(spec['xgb_params'], **best_hp)\n",
    "        print(f\"  Using sweep-optimized: {best_hp}\")\n",
    "    else:\n",
    "        xgb_params = spec['xgb_params']\n",
    "        print(f\"  Using default params\")\n",
    "\n",
    "    X, labels, label_dist = load_csvs(\n",
    "        DATA_DIR, spec['csv_glob'], spec['features'],\n",
    "        spec['label_col'], spec.get('label_transform'),\n",
    "        label_filter=spec.get('label_filter'))\n",
    "\n",
    "    if X is None:\n",
    "        print(f\"  No data, skipping\")\n",
    "        continue\n",
    "\n",
    "    if spec['task'] == 'regression':\n",
    "        y = labels.astype(np.float32)\n",
    "    elif spec['task'] == 'binary':\n",
    "        y = np.array([int(v) for v in labels], dtype=np.int32)\n",
    "    else:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(labels)\n",
    "\n",
    "    rebalance = 'none'\n",
    "    if spec['task'] in ('multiclass', 'binary'):\n",
    "        counts = list(label_dist.values())\n",
    "        imbalance = max(counts) / max(min(counts), 1)\n",
    "        if imbalance > 5.0:\n",
    "            rng = np.random.default_rng(42)\n",
    "            X, y = hybrid_resample(X, y, rng=rng)\n",
    "            rebalance = 'hybrid'\n",
    "            print(f\"  Hybrid-resampled -> {len(y):,} samples\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Build focal loss config if spec has it\n",
    "    focal_loss_cfg = None\n",
    "    if spec.get('focal_loss') and spec['task'] == 'multiclass':\n",
    "        focal_loss_cfg = {\n",
    "            'alpha': spec.get('focal_alpha', 0.25),\n",
    "            'gamma': spec.get('focal_gamma', 2.0),\n",
    "            'n_classes': int(len(set(y.tolist()))),\n",
    "        }\n",
    "        print(f\"  Focal loss enabled (alpha={focal_loss_cfg['alpha']}, gamma={focal_loss_cfg['gamma']})\")\n",
    "\n",
    "    model, metrics = train_model(X_scaled, y, xgb_params, spec['task'],\n",
    "                                  focal_loss_config=focal_loss_cfg)\n",
    "    metrics['rebalance_strategy'] = rebalance\n",
    "    if model_name in sweep_results:\n",
    "        metrics['sweep_best_params'] = sweep_results[model_name]['best_params']\n",
    "\n",
    "    if spec['task'] == 'regression':\n",
    "        print(f\"  Val: RMSE={metrics['val_rmse']:.4f}  R2={metrics['val_r2']:.4f}\")\n",
    "    else:\n",
    "        print(f\"  Val: acc={metrics['val_accuracy']:.4f}  F1w={metrics['val_f1_weighted']:.4f}  F1m={metrics['val_f1_macro']:.4f}\")\n",
    "        if 'per_class_f1' in metrics:\n",
    "            for cls, f1v in metrics['per_class_f1'].items():\n",
    "                print(f\"       class {cls:12s}  F1={f1v:.4f}\")\n",
    "\n",
    "    cv_metrics = cv_model(X_scaled, y, xgb_params, spec['task'])\n",
    "    metrics.update(cv_metrics)\n",
    "    metrics['label_distribution'] = {str(k): int(v) for k, v in label_dist.items()}\n",
    "    metrics['num_samples'] = len(y)\n",
    "    metrics['num_features'] = X.shape[1]\n",
    "\n",
    "    if 'cv_accuracy_mean' in cv_metrics:\n",
    "        print(f\"  CV:  acc={cv_metrics['cv_accuracy_mean']:.4f} +/- {cv_metrics['cv_accuracy_std']:.4f}\")\n",
    "    else:\n",
    "        print(f\"  CV:  R2={cv_metrics['cv_r2_mean']:.4f} +/- {cv_metrics['cv_r2_std']:.4f}\")\n",
    "\n",
    "    final_models[model_name] = (model, scaler, metrics)\n",
    "    final_results[model_name] = metrics\n",
    "\n",
    "# â”€â”€ Save models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_FILENAMES = {\n",
    "    'path_gnn':      'pathgnn_scorer.pkl',\n",
    "    'diploid_ai':    'diploid_model.pkl',\n",
    "    'ul_routing':    'ul_routing_model.pkl',\n",
    "    'sv_ai':         'sv_detector_model.pkl',\n",
    "    'sv_ai_binary':  'sv_binary_model.pkl',\n",
    "    'sv_ai_subtype': 'sv_subtype_model.pkl',\n",
    "}\n",
    "\n",
    "for model_name, (model, scaler, metrics) in final_models.items():\n",
    "    subdir = os.path.join(OUTPUT_DIR, SAVE_MAP[model_name])\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "    if model_name == 'edge_ai':\n",
    "        for tech in TECH_LIST:\n",
    "            with open(os.path.join(subdir, f'edgewarden_{tech}.pkl'), 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            with open(os.path.join(subdir, f'scaler_{tech}.pkl'), 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "    else:\n",
    "        model_filename = MODEL_FILENAMES[model_name]\n",
    "        with open(os.path.join(subdir, model_filename), 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    with open(os.path.join(subdir, f'training_metadata_{model_name}.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"  Done: {model_name} -> {subdir}/{MODEL_FILENAMES.get(model_name, 'edgewarden_*.pkl')}\")\n",
    "\n",
    "# â”€â”€ Final report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "t_total = time.time() - t_start\n",
    "report = {\n",
    "    'training_method': 'hybrid_resampling_gpu_sweep_optimized_v2.1',\n",
    "    'device': DEVICE,\n",
    "    'resampling_strategy': 'undersample_100k + oversample_to_median',\n",
    "    'improvements': [\n",
    "        'S1: SV-dense training data (10-50x baseline density)',\n",
    "        'S2: Two-stage SV detection (binary + subtype)',\n",
    "        'S3: Focal loss for SV models',\n",
    "        'S4: 8 new SV-specific features (27 total)',\n",
    "    ],\n",
    "    'total_time_seconds': round(t_total, 1),\n",
    "    'models': final_results,\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, 'training_report.json'), 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nFinal report -> {OUTPUT_DIR}/training_report.json\")\n",
    "print(f\"  Total retrain time: {t_total:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a6061",
   "metadata": {},
   "source": [
    "## 9. Save Models to Google Drive\n",
    "\n",
    "Saves the retrained models back to Google Drive for persistence.\n",
    "Download from Drive to your local repo:\n",
    "```bash\n",
    "# From Google Drive, download trained_models_v2.tar.gz, then:\n",
    "tar xzf trained_models_v2.tar.gz\n",
    "cp -r trained_models_v2/* trained_models_10x/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Save retrained models to Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import shutil\n",
    "\n",
    "# Create tarball on local SSD first (fast)\n",
    "local_tar = '/content/trained_models_v2.tar.gz'\n",
    "!cd /content && tar czf trained_models_v2.tar.gz trained_models_v2/\n",
    "\n",
    "# Copy to Google Drive\n",
    "shutil.copy2(local_tar, GDRIVE_OUTPUT)\n",
    "\n",
    "# Verify\n",
    "drive_size_mb = os.path.getsize(GDRIVE_OUTPUT) / (1024 * 1024)\n",
    "print(f\"âœ“ Saved to Google Drive: {GDRIVE_OUTPUT} ({drive_size_mb:.1f} MB)\")\n",
    "print(f\"  Contains {len(final_models)} models with sweep-optimized hyperparameters\")\n",
    "print(f\"  Training report: trained_models_v2/training_report.json\")\n",
    "print(f\"  Sweep results:   trained_models_v2/sweep_results.json\")\n",
    "print(f\"\\n  Download from Drive â†’ My Drive/Colab Notebooks/trained_models_v2.tar.gz\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

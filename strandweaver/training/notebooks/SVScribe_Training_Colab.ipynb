{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693274bd",
   "metadata": {},
   "source": [
    "# ğŸ§¬ StrandWeaver â€” SVScribe Training (Colab GPU)\n",
    "\n",
    "Train **1 XGBoost multiclass classifier** for graph-region structural variant type prediction.\n",
    "Training uses **SV-dense synthetic data** generated by `generate_training_data.py --graph-training`.\n",
    "\n",
    "| Class | Label | Description |\n",
    "|-------|-------|-------------|\n",
    "| 0 | `none` | No structural variant (background) |\n",
    "| 1 | `deletion` | Deletion SV |\n",
    "| 2 | `insertion` | Insertion SV |\n",
    "| 3 | `inversion` | Inversion SV |\n",
    "| 4 | `duplication` | Duplication SV |\n",
    "\n",
    "**Training data layout (5 batches Ã— 40 genomes = 200 genomes):**\n",
    "```\n",
    "training_data_sv_dense/\n",
    "â”œâ”€â”€ sv_dense_1e4/          Moderate SV density (10Ã— baseline), all 4 types\n",
    "â”œâ”€â”€ sv_dense_2e4/          High SV density (20Ã— baseline), higher repeats\n",
    "â”œâ”€â”€ sv_dense_3e4/          Very high SV density (30Ã— baseline)\n",
    "â”œâ”€â”€ sv_dense_ins_focus/    Insertion-enriched (50% insertion, 50% other)\n",
    "â”œâ”€â”€ sv_dense_inv_dup_focus/ Inversion+duplication-enriched\n",
    "â””â”€â”€ sv_dense_generation_report.json\n",
    "```\n",
    "\n",
    "Each genome directory contains:\n",
    "```\n",
    "genome_NNNN/\n",
    "â”œâ”€â”€ haplotype_A.fasta\n",
    "â”œâ”€â”€ haplotype_B.fasta\n",
    "â”œâ”€â”€ hifi.fastq\n",
    "â”œâ”€â”€ sv_truth.json\n",
    "â”œâ”€â”€ metadata.json\n",
    "â””â”€â”€ graph_training/\n",
    "    â”œâ”€â”€ sv_detect_training_gNNNN.csv   â† training rows (this notebook)\n",
    "    â”œâ”€â”€ edge_ai_training_gNNNN.csv\n",
    "    â”œâ”€â”€ diploid_ai_training_gNNNN.csv\n",
    "    â”œâ”€â”€ path_gnn_training_gNNNN.csv\n",
    "    â””â”€â”€ overlap_graph_gNNNN.gfa\n",
    "```\n",
    "\n",
    "### 40 Features â†’ 5-class SV type prediction\n",
    "\n",
    "| # | Feature | Description |\n",
    "|---|---------|-------------|\n",
    "| 1â€“13 | Metadata | genome_id, genome_size, chromosome_id, read_technology, coverage_depth, error_rate, ploidy, gc_content_global, repeat_density_global, heterozygosity_rate, random_seed, generator_version, schema_version |\n",
    "| 14â€“18 | Coverage stats | coverage_mean, coverage_std, coverage_median, gc_content, repeat_fraction |\n",
    "| 19â€“23 | Graph topology | kmer_diversity, branching_complexity, hic_disruption_score, ul_support, mapping_quality |\n",
    "| 24â€“28 | Breakpoint signals | region_length, breakpoint_precision, allele_balance, phase_switch_rate, coverage_cv |\n",
    "| 29â€“33 | Distribution stats | coverage_skewness, coverage_kurtosis, coverage_p10, coverage_p90, depth_ratio_flank |\n",
    "| 34â€“40 | SV signatures | split_read_count, clip_fraction, bubble_size, path_divergence, ul_spanning, coverage_drop_magnitude, orientation_switch_rate |\n",
    "\n",
    "### Setup\n",
    "1. `Runtime â†’ Change runtime type â†’ T4 GPU` (or A100 if available)\n",
    "2. Upload `training_data_sv_dense/` to Google Drive (or generate on Colab)\n",
    "3. Run cells top-to-bottom\n",
    "\n",
    "> **Output:** `sv_detector_model.pkl` â€” drop into `strandweaver/trained_models/sv_detector/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a04fe8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, sys, os\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ“ GPU detected: {result.stdout.strip()}\")\n",
    "    GPU_AVAILABLE = True\n",
    "else:\n",
    "    print(\"âš  No GPU detected â€” XGBoost will use CPU (still fast, just slower HP search)\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# Check RAM\n",
    "import psutil\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f\"âœ“ RAM: {ram_gb:.1f} GB {'(High-RAM âœ“)' if ram_gb > 20 else '(consider High-RAM runtime)'}\")\n",
    "print(f\"âœ“ Python: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q xgboost scikit-learn numpy pandas optuna matplotlib seaborn\n",
    "\n",
    "# Clone StrandWeaver (dev branch) and install â€” pull latest on re-runs\n",
    "!git clone -b dev https://github.com/pgrady1322/strandweaver.git /content/strandweaver 2>/dev/null || \\\n",
    "    (cd /content/strandweaver && git pull origin dev)\n",
    "!cd /content/strandweaver && pip install -e . -q\n",
    "\n",
    "print(\"\\nâœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Mount Google Drive (for data + saving results) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# â”€â”€ Directory layout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GDRIVE_DIR        = '/content/drive/MyDrive/Colab_Training'\n",
    "SVSCRIBE_OUT      = '/content/svscribe_training'\n",
    "MODELS_OUT        = '/content/trained_models'\n",
    "GDRIVE_SVSCRIBE   = os.path.join(GDRIVE_DIR, 'svscribe_models')\n",
    "\n",
    "for d in [GDRIVE_DIR, SVSCRIBE_OUT, MODELS_OUT]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Working dirs created\")\n",
    "print(f\"  SVScribe output:  {SVSCRIBE_OUT}\")\n",
    "print(f\"  Models output:    {MODELS_OUT}\")\n",
    "print(f\"  Drive export:     {GDRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger('svscribe_training')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# XGBoost device config\n",
    "XGB_DEVICE = 'cuda' if GPU_AVAILABLE else 'cpu'\n",
    "XGB_TREE = 'hist'  # gpu_hist deprecated in XGBoost 2.x â€” 'hist' auto-uses GPU\n",
    "\n",
    "print(f\"âœ“ XGBoost {xgb.__version__}\")\n",
    "print(f\"âœ“ NumPy {np.__version__}\")\n",
    "print(f\"âœ“ Pandas {pd.__version__}\")\n",
    "print(f\"âœ“ Device: {XGB_DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761740ff",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Edit the cell below to point to your SV training data.\n",
    "\n",
    "**Option A (recommended):** Upload the `training_data_sv_dense/` directory to Google Drive.\n",
    "\n",
    "**Option B:** Generate training data on Colab:\n",
    "```bash\n",
    "cd /content/strandweaver\n",
    "python strandweaver/user_training/generate_training_data.py \\\n",
    "    --genome-size 2000000 -n 40 --sv-density 0.0001 \\\n",
    "    --sv-types deletion insertion inversion duplication \\\n",
    "    --read-types hifi --coverage 30 --graph-training \\\n",
    "    -o /content/sv_training_data --seed 42\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  SVSCRIBE CONFIGURATION â€” edit these before running\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Data source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Path to the training_data_sv_dense directory on Google Drive or Colab\n",
    "SV_DATA_DIR = os.path.join(GDRIVE_DIR, 'training_data_sv_dense')\n",
    "\n",
    "# Batch directories (all should exist under SV_DATA_DIR)\n",
    "SV_BATCHES = [\n",
    "    'sv_dense_1e4',           # Moderate SV density (10Ã— baseline)\n",
    "    'sv_dense_2e4',           # High SV density (20Ã— baseline)\n",
    "    'sv_dense_3e4',           # Very high SV density (30Ã— baseline)\n",
    "    'sv_dense_ins_focus',     # Insertion-enriched\n",
    "    'sv_dense_inv_dup_focus', # Inversion+duplication-enriched\n",
    "]\n",
    "\n",
    "# â”€â”€ SV type label mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SV_TYPE_MAP = {\n",
    "    'none':        0,\n",
    "    'deletion':    1,\n",
    "    'insertion':   2,\n",
    "    'inversion':   3,\n",
    "    'duplication': 4,\n",
    "}\n",
    "SV_TYPE_NAMES = {v: k for k, v in SV_TYPE_MAP.items()}\n",
    "\n",
    "# â”€â”€ Features to use for training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Metadata columns are useful context features for the model:\n",
    "# they capture genome characteristics that affect SV signal patterns.\n",
    "METADATA_FEATURES = [\n",
    "    'genome_size', 'coverage_depth', 'error_rate',\n",
    "    'gc_content_global', 'repeat_density_global', 'heterozygosity_rate',\n",
    "]\n",
    "\n",
    "# Signal features (from graph region analysis)\n",
    "SIGNAL_FEATURES = [\n",
    "    'coverage_mean', 'coverage_std', 'coverage_median',\n",
    "    'gc_content', 'repeat_fraction',\n",
    "    'kmer_diversity', 'branching_complexity',\n",
    "    'hic_disruption_score', 'ul_support', 'mapping_quality',\n",
    "    'region_length', 'breakpoint_precision', 'allele_balance',\n",
    "    'phase_switch_rate', 'coverage_cv',\n",
    "    'coverage_skewness', 'coverage_kurtosis',\n",
    "    'coverage_p10', 'coverage_p90', 'depth_ratio_flank',\n",
    "    'split_read_count', 'clip_fraction',\n",
    "    'bubble_size', 'path_divergence', 'ul_spanning',\n",
    "    'coverage_drop_magnitude', 'orientation_switch_rate',\n",
    "]\n",
    "\n",
    "ALL_FEATURES = METADATA_FEATURES + SIGNAL_FEATURES\n",
    "SV_LABEL = 'sv_type'\n",
    "\n",
    "# â”€â”€ Training hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "N_OPTUNA_TRIALS = 30\n",
    "N_CV_FOLDS = 5\n",
    "\n",
    "# â”€â”€ Validate data directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NOTE: Uses glob instead of os.listdir() â€” Drive FUSE caching can\n",
    "# cause listdir() to return empty/partial results on first access.\n",
    "import glob as _glob\n",
    "\n",
    "print(f\"SV data directory: {SV_DATA_DIR}\")\n",
    "for batch in SV_BATCHES:\n",
    "    batch_path = os.path.join(SV_DATA_DIR, batch)\n",
    "    if os.path.isdir(batch_path):\n",
    "        # glob is more reliable than os.listdir on Google Drive FUSE\n",
    "        genome_dirs = sorted(_glob.glob(os.path.join(batch_path, 'genome_*', '')))\n",
    "        n_genomes = len(genome_dirs)\n",
    "        if n_genomes == 0:\n",
    "            # Fallback: count any subdirectory (handles renamed dirs)\n",
    "            genome_dirs = [\n",
    "                d for d in _glob.glob(os.path.join(batch_path, '*', ''))\n",
    "                if os.path.isdir(d)\n",
    "            ]\n",
    "            n_genomes = len(genome_dirs)\n",
    "        print(f\"  âœ“ {batch}: {n_genomes} genomes\")\n",
    "    else:\n",
    "        print(f\"  âœ— {batch}: NOT FOUND at {batch_path}\")\n",
    "\n",
    "# Check generation report\n",
    "report_path = os.path.join(SV_DATA_DIR, 'sv_dense_generation_report.json')\n",
    "if os.path.exists(report_path):\n",
    "    with open(report_path) as f:\n",
    "        report = json.load(f)\n",
    "    for name, batch_info in report.get('batches', {}).items():\n",
    "        status = batch_info.get('status', 'unknown')\n",
    "        elapsed = batch_info.get('elapsed_seconds', 0)\n",
    "        print(f\"  {name}: {status} ({elapsed/60:.1f} min)\")\n",
    "else:\n",
    "    print(f\"  âš  Generation report not found at {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb52471",
   "metadata": {},
   "source": [
    "## 3. Load & Merge Training Data\n",
    "Loads `sv_detect_training_*.csv` from all batches and genomes into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45939547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Concatenate all SV detect CSVs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Uses glob for CSV discovery â€” more reliable than os.listdir() on\n",
    "# Google Drive FUSE mounts (avoids Directory caching issues).\n",
    "frames = []\n",
    "batch_counts = {}\n",
    "\n",
    "for batch in SV_BATCHES:\n",
    "    batch_dir = os.path.join(SV_DATA_DIR, batch)\n",
    "    if not os.path.isdir(batch_dir):\n",
    "        print(f\"  âš  Skipping {batch} (not found)\")\n",
    "        continue\n",
    "\n",
    "    # Glob for all sv_detect_training CSVs recursively under this batch\n",
    "    csv_pattern = os.path.join(batch_dir, '*', 'graph_training', 'sv_detect_training_*.csv')\n",
    "    csv_files = sorted(_glob.glob(csv_pattern))\n",
    "\n",
    "    if not csv_files:\n",
    "        # Fallback: try recursive glob in case directory structure varies\n",
    "        csv_files = sorted(_glob.glob(os.path.join(batch_dir, '**', 'sv_detect_training_*.csv'), recursive=True))\n",
    "\n",
    "    batch_frames = []\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df['batch'] = batch\n",
    "            batch_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  âš  Failed to read {csv_path}: {e}\")\n",
    "\n",
    "    if batch_frames:\n",
    "        batch_df = pd.concat(batch_frames, ignore_index=True)\n",
    "        batch_counts[batch] = len(batch_df)\n",
    "        frames.append(batch_df)\n",
    "        print(f\"  âœ“ {batch}: {len(batch_df):,} rows from {len(batch_frames)} files\")\n",
    "    else:\n",
    "        print(f\"  âš  {batch}: no CSV files found (pattern: {csv_pattern})\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No training data found! Check SV_DATA_DIR path and Drive mount.\")\n",
    "\n",
    "df_all = pd.concat(frames, ignore_index=True)\n",
    "print(f\"\\n{'â•'*50}\")\n",
    "print(f\"Total rows: {len(df_all):,}\")\n",
    "print(f\"Columns:    {len(df_all.columns)}\")\n",
    "print(f\"Batches:    {len(batch_counts)}\")\n",
    "for b, n in batch_counts.items():\n",
    "    print(f\"  {b}: {n:,} ({n/len(df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caa095",
   "metadata": {},
   "source": [
    "## 4. Inspect & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ad5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Encode SV type labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_all['sv_type_encoded'] = df_all[SV_LABEL].map(SV_TYPE_MAP)\n",
    "\n",
    "# Check for unmapped values\n",
    "unmapped = df_all[df_all['sv_type_encoded'].isna()][SV_LABEL].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"âš  Unmapped sv_type values: {unmapped}\")\n",
    "    df_all = df_all.dropna(subset=['sv_type_encoded'])\n",
    "    df_all['sv_type_encoded'] = df_all['sv_type_encoded'].astype(int)\n",
    "\n",
    "# â”€â”€ Class distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Class distribution:\")\n",
    "for label, count in df_all['sv_type_encoded'].value_counts().sort_index().items():\n",
    "    pct = count / len(df_all) * 100\n",
    "    name = SV_TYPE_NAMES.get(label, f'unknown_{label}')\n",
    "    print(f\"  {label} ({name:15s}): {count:>8,} ({pct:5.1f}%)\")\n",
    "\n",
    "# â”€â”€ Feature summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nFeature summary ({len(ALL_FEATURES)} features):\")\n",
    "missing = [f for f in ALL_FEATURES if f not in df_all.columns]\n",
    "if missing:\n",
    "    print(f\"  âš  Missing columns: {missing}\")\n",
    "    ALL_FEATURES = [f for f in ALL_FEATURES if f in df_all.columns]\n",
    "    print(f\"  Using {len(ALL_FEATURES)} available features\")\n",
    "\n",
    "print(df_all[ALL_FEATURES].describe().round(3).to_string())\n",
    "\n",
    "# â”€â”€ Check for NaN/Inf â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "nan_counts = df_all[ALL_FEATURES].isna().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0]\n",
    "if len(nan_cols) > 0:\n",
    "    print(f\"\\nâš  Columns with NaN values:\")\n",
    "    for col, cnt in nan_cols.items():\n",
    "        print(f\"  {col}: {cnt:,} NaN ({cnt/len(df_all)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ No NaN values in features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77218bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualize class balance and key feature distributions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('SVScribe Training Data Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Class distribution\n",
    "ax = axes[0, 0]\n",
    "class_counts = df_all['sv_type_encoded'].value_counts().sort_index()\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12', '#9b59b6']\n",
    "bars = ax.bar([SV_TYPE_NAMES[i] for i in class_counts.index], class_counts.values, color=colors)\n",
    "ax.set_title('Class Distribution')\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "# 2. Coverage distribution by SV type\n",
    "ax = axes[0, 1]\n",
    "for sv_code in sorted(df_all['sv_type_encoded'].unique()):\n",
    "    subset = df_all[df_all['sv_type_encoded'] == sv_code]['coverage_mean']\n",
    "    ax.hist(subset, bins=50, alpha=0.5, label=SV_TYPE_NAMES[sv_code])\n",
    "ax.set_title('Coverage Mean by SV Type')\n",
    "ax.set_xlabel('coverage_mean')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# 3. Region length distribution\n",
    "ax = axes[0, 2]\n",
    "for sv_code in sorted(df_all['sv_type_encoded'].unique()):\n",
    "    subset = df_all[df_all['sv_type_encoded'] == sv_code]['region_length']\n",
    "    ax.hist(subset, bins=50, alpha=0.5, label=SV_TYPE_NAMES[sv_code])\n",
    "ax.set_title('Region Length by SV Type')\n",
    "ax.set_xlabel('region_length')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# 4. Breakpoint precision\n",
    "ax = axes[1, 0]\n",
    "for sv_code in sorted(df_all['sv_type_encoded'].unique()):\n",
    "    subset = df_all[df_all['sv_type_encoded'] == sv_code]['breakpoint_precision']\n",
    "    ax.hist(subset, bins=50, alpha=0.5, label=SV_TYPE_NAMES[sv_code])\n",
    "ax.set_title('Breakpoint Precision by SV Type')\n",
    "ax.set_xlabel('breakpoint_precision')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# 5. Coverage drop magnitude\n",
    "ax = axes[1, 1]\n",
    "for sv_code in sorted(df_all['sv_type_encoded'].unique()):\n",
    "    subset = df_all[df_all['sv_type_encoded'] == sv_code]['coverage_drop_magnitude']\n",
    "    ax.hist(subset, bins=50, alpha=0.5, label=SV_TYPE_NAMES[sv_code])\n",
    "ax.set_title('Coverage Drop Magnitude by SV Type')\n",
    "ax.set_xlabel('coverage_drop_magnitude')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# 6. Batch distribution\n",
    "ax = axes[1, 2]\n",
    "batch_cts = df_all['batch'].value_counts()\n",
    "ax.barh(range(len(batch_cts)), batch_cts.values, color='steelblue')\n",
    "ax.set_yticks(range(len(batch_cts)))\n",
    "ax.set_yticklabels(batch_cts.index, fontsize=9)\n",
    "ax.set_title('Rows per Batch')\n",
    "ax.set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(os.path.join(SVSCRIBE_OUT, 'sv_data_overview.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Data overview plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8c1f5",
   "metadata": {},
   "source": [
    "## 5. Train SVScribe Classifier\n",
    "XGBoost multiclass classifier with **Optuna HP search** + **hybrid resampling** (undersample majority `none` class, oversample minority SV types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ef9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "def hybrid_resample(X, y, max_majority=50_000, random_state=42):\n",
    "    \"\"\"\n",
    "    Hybrid resampling: undersample majority (none) to max_majority,\n",
    "    oversample minorities to median class size.\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    class_counts = dict(zip(classes, counts))\n",
    "\n",
    "    # Cap majority class\n",
    "    capped = {c: min(n, max_majority) for c, n in class_counts.items()}\n",
    "    median_size = int(np.median(list(capped.values())))\n",
    "\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in classes:\n",
    "        mask = y == cls\n",
    "        X_cls, y_cls = X[mask], y[mask]\n",
    "        target = max(median_size, min(len(X_cls), max_majority))\n",
    "\n",
    "        if len(X_cls) > target:\n",
    "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
    "                                  random_state=random_state, replace=False)\n",
    "        elif len(X_cls) < target:\n",
    "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
    "                                  random_state=random_state, replace=True)\n",
    "        else:\n",
    "            X_rs, y_rs = X_cls, y_cls\n",
    "\n",
    "        X_resampled.append(X_rs)\n",
    "        y_resampled.append(y_rs)\n",
    "\n",
    "    return np.vstack(X_resampled), np.concatenate(y_resampled)\n",
    "\n",
    "\n",
    "def optuna_objective(trial, X, y):\n",
    "    \"\"\"Optuna objective for SVScribe multiclass classifier.\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'tree_method': XGB_TREE,\n",
    "        'device': XGB_DEVICE,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# â”€â”€ Prepare data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X = df_all[ALL_FEATURES].fillna(0).values\n",
    "y = df_all['sv_type_encoded'].values\n",
    "\n",
    "print(f\"Raw data: {X.shape[0]:,} samples Ã— {X.shape[1]} features\")\n",
    "print(f\"Class distribution: {dict(Counter(y))}\")\n",
    "\n",
    "# â”€â”€ Hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_balanced, y_balanced = hybrid_resample(X, y, max_majority=50_000)\n",
    "print(f\"\\nAfter hybrid resampling: {X_balanced.shape[0]:,} samples\")\n",
    "print(f\"Balanced distribution:  {dict(Counter(y_balanced))}\")\n",
    "\n",
    "# â”€â”€ Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nRunning Optuna ({N_OPTUNA_TRIALS} trials)...\")\n",
    "t0 = time.time()\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: optuna_objective(trial, X_balanced, y_balanced),\n",
    "               n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params['tree_method'] = XGB_TREE\n",
    "best_params['device'] = XGB_DEVICE\n",
    "best_params['objective'] = 'multi:softprob'\n",
    "best_params['num_class'] = 5\n",
    "best_params['eval_metric'] = 'mlogloss'\n",
    "best_params['random_state'] = SEED\n",
    "\n",
    "elapsed_hp = time.time() - t0\n",
    "print(f\"\\nBest F1-macro: {study.best_value:.4f} ({elapsed_hp/60:.1f} min)\")\n",
    "print(f\"Best params: depth={best_params['max_depth']}, \"\n",
    "      f\"lr={best_params['learning_rate']:.4f}, \"\n",
    "      f\"n_est={best_params['n_estimators']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60237a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ K-fold cross-validation with best params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"Running {N_CV_FOLDS}-fold CV...\")\n",
    "skf = StratifiedKFold(n_splits=N_CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "cv_acc = []\n",
    "cv_f1 = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_balanced, y_balanced)):\n",
    "    X_train, X_val = X_balanced[train_idx], X_balanced[val_idx]\n",
    "    y_train, y_val = y_balanced[train_idx], y_balanced[val_idx]\n",
    "\n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    cv_acc.append(acc)\n",
    "    cv_f1.append(f1)\n",
    "    print(f\"  Fold {fold+1}: Accuracy={acc:.4f}, F1-macro={f1:.4f}\")\n",
    "\n",
    "print(f\"\\nCV Summary: Accuracy={np.mean(cv_acc):.4f}Â±{np.std(cv_acc):.4f}, \"\n",
    "      f\"F1-macro={np.mean(cv_f1):.4f}Â±{np.std(cv_f1):.4f}\")\n",
    "\n",
    "# â”€â”€ Train final model on all balanced data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nTraining final model on all {len(y_balanced):,} samples...\")\n",
    "final_model = xgb.XGBClassifier(**best_params)\n",
    "final_model.fit(X_balanced, y_balanced)\n",
    "\n",
    "# Save model\n",
    "sv_model_dir = os.path.join(MODELS_OUT, 'sv_detector')\n",
    "os.makedirs(sv_model_dir, exist_ok=True)\n",
    "\n",
    "pkl_path = os.path.join(sv_model_dir, 'sv_detector_model.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(f\"\\nâœ“ Saved: {pkl_path}\")\n",
    "\n",
    "# â”€â”€ Classification report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n{'â•'*60}\")\n",
    "print(\"Classification Report (last CV fold):\")\n",
    "print(f\"{'â•'*60}\")\n",
    "class_names = ['none', 'deletion', 'insertion', 'inversion', 'duplication']\n",
    "print(classification_report(y_val, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "# â”€â”€ Confusion matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('SVScribe â€” Confusion Matrix (last CV fold)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SVSCRIBE_OUT, 'sv_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save training results\n",
    "sv_results = {\n",
    "    'best_params': {k: v for k, v in best_params.items() if k != 'device'},\n",
    "    'cv_accuracy_mean': float(np.mean(cv_acc)),\n",
    "    'cv_accuracy_std': float(np.std(cv_acc)),\n",
    "    'cv_f1_macro_mean': float(np.mean(cv_f1)),\n",
    "    'cv_f1_macro_std': float(np.std(cv_f1)),\n",
    "    'n_samples_raw': int(len(y)),\n",
    "    'n_samples_balanced': int(len(y_balanced)),\n",
    "    'n_features': len(ALL_FEATURES),\n",
    "    'feature_names': ALL_FEATURES,\n",
    "    'sv_type_map': SV_TYPE_MAP,\n",
    "    'class_distribution_raw': {SV_TYPE_NAMES[k]: int(v) for k, v in Counter(y).items()},\n",
    "    'class_distribution_balanced': {SV_TYPE_NAMES[k]: int(v) for k, v in Counter(y_balanced).items()},\n",
    "    'batches_used': SV_BATCHES,\n",
    "}\n",
    "with open(os.path.join(sv_model_dir, 'training_results.json'), 'w') as f:\n",
    "    json.dump(sv_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ“ Training results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fc6be",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d03568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Feature importance plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "fig.suptitle('Feature Importance â€” SVScribe SV Detector', fontsize=16, fontweight='bold')\n",
    "\n",
    "importances = final_model.feature_importances_\n",
    "sorted_idx = np.argsort(importances)\n",
    "ax.barh(range(len(sorted_idx)), importances[sorted_idx], color='steelblue')\n",
    "ax.set_yticks(range(len(sorted_idx)))\n",
    "ax.set_yticklabels([ALL_FEATURES[i] for i in sorted_idx], fontsize=9)\n",
    "ax.set_xlabel('Importance (gain)')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(os.path.join(MODELS_OUT, 'svscribe_feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Feature importance plot saved\")\n",
    "\n",
    "# Top 10 features\n",
    "print(\"\\nTop 10 features:\")\n",
    "top10_idx = sorted_idx[-10:][::-1]\n",
    "for i, idx in enumerate(top10_idx):\n",
    "    print(f\"  {i+1}. {ALL_FEATURES[idx]:30s} {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e906f",
   "metadata": {},
   "source": [
    "## 7. Per-Type Precisionâ€“Recall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Per-SV-type metrics across all CV folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Re-run CV to collect per-type metrics\n",
    "per_type_precision = {c: [] for c in range(5)}\n",
    "per_type_recall = {c: [] for c in range(5)}\n",
    "per_type_f1 = {c: [] for c in range(5)}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_balanced, y_balanced)):\n",
    "    X_train, X_val = X_balanced[train_idx], X_balanced[val_idx]\n",
    "    y_train, y_val = y_balanced[train_idx], y_balanced[val_idx]\n",
    "\n",
    "    m = xgb.XGBClassifier(**best_params)\n",
    "    m.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    y_pred = m.predict(X_val)\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_pred, labels=list(range(5)))\n",
    "    for c in range(5):\n",
    "        per_type_precision[c].append(prec[c])\n",
    "        per_type_recall[c].append(rec[c])\n",
    "        per_type_f1[c].append(f1[c])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Per-Type Metrics (5-fold CV)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, metric, metric_name in zip(axes, [per_type_precision, per_type_recall, per_type_f1],\n",
    "                                    ['Precision', 'Recall', 'F1']):\n",
    "    means = [np.mean(metric[c]) for c in range(5)]\n",
    "    stds = [np.std(metric[c]) for c in range(5)]\n",
    "    ax.bar(class_names, means, yerr=stds, capsize=5, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_title(metric_name)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "plt.savefig(os.path.join(SVSCRIBE_OUT, 'sv_per_type_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Table\n",
    "print(f\"\\n{'SV Type':<15}  {'Precision':>12}  {'Recall':>12}  {'F1':>12}\")\n",
    "print('â”€' * 55)\n",
    "for c in range(5):\n",
    "    print(f\"{class_names[c]:<15}  \"\n",
    "          f\"{np.mean(per_type_precision[c]):.3f}Â±{np.std(per_type_precision[c]):.3f}  \"\n",
    "          f\"{np.mean(per_type_recall[c]):.3f}Â±{np.std(per_type_recall[c]):.3f}  \"\n",
    "          f\"{np.mean(per_type_f1[c]):.3f}Â±{np.std(per_type_f1[c]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c7d56",
   "metadata": {},
   "source": [
    "## 8. Export Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Package and export to Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import tarfile, shutil, glob\n",
    "\n",
    "GDRIVE_TARBALL = os.path.join(GDRIVE_DIR, 'svscribe_models.tar.gz')\n",
    "\n",
    "# List all model files\n",
    "model_files = glob.glob(f'{sv_model_dir}/**/*', recursive=True)\n",
    "print(f\"Model files to export ({len(model_files)}):\")\n",
    "for f in sorted(model_files):\n",
    "    if os.path.isfile(f):\n",
    "        size_kb = os.path.getsize(f) / 1024\n",
    "        print(f\"  {os.path.relpath(f, sv_model_dir):40s} {size_kb:>8.1f} KB\")\n",
    "\n",
    "# Create tarball\n",
    "print(f\"\\nCreating tarball...\")\n",
    "with tarfile.open(GDRIVE_TARBALL, 'w:gz') as tar:\n",
    "    tar.add(sv_model_dir, arcname='sv_detector')\n",
    "\n",
    "tarball_size = os.path.getsize(GDRIVE_TARBALL) / (1024 * 1024)\n",
    "print(f\"\\nâœ… Exported to Google Drive:\")\n",
    "print(f\"   {GDRIVE_TARBALL}\")\n",
    "print(f\"   Size: {tarball_size:.1f} MB\")\n",
    "\n",
    "# Also copy to Drive for easy access\n",
    "if os.path.exists(GDRIVE_SVSCRIBE):\n",
    "    shutil.rmtree(GDRIVE_SVSCRIBE)\n",
    "shutil.copytree(sv_model_dir, GDRIVE_SVSCRIBE)\n",
    "print(f\"   Copied â†’ {GDRIVE_SVSCRIBE}\")\n",
    "\n",
    "print(f\"\\nğŸ“¥ To download locally:\")\n",
    "print(f\"   1. Open Google Drive â†’ My Drive/Colab_Training/\")\n",
    "print(f\"   2. Download svscribe_models.tar.gz\")\n",
    "print(f\"   3. Extract to strandweaver/trained_models/sv_detector/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375aa9f2",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘         SVScribe Model Training â€” Summary                       â•‘\")\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  1 XGBoost Multiclass Classifier:                                â•‘\")\n",
    "print(\"â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â•‘\")\n",
    "print(f\"â•‘    sv_detector_model       F1={sv_results['cv_f1_macro_mean']:.3f}Â±\"\n",
    "      f\"{sv_results['cv_f1_macro_std']:.3f}  \"\n",
    "      f\"Acc={sv_results['cv_accuracy_mean']:.3f}    â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  Classes: none | deletion | insertion |                          â•‘\")\n",
    "print(\"â•‘           inversion | duplication                                â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(f\"â•‘  Training data:                                                  â•‘\")\n",
    "print(f\"â•‘    Raw samples:      {sv_results['n_samples_raw']:>8,}                            â•‘\")\n",
    "print(f\"â•‘    Balanced samples: {sv_results['n_samples_balanced']:>8,}                            â•‘\")\n",
    "print(f\"â•‘    Features:         {sv_results['n_features']:>8}                            â•‘\")\n",
    "print(f\"â•‘    Batches:          {len(SV_BATCHES):>8}                            â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  SV density tiers:                                               â•‘\")\n",
    "print(\"â•‘    sv_dense_1e4          10Ã— baseline, all 4 SV types            â•‘\")\n",
    "print(\"â•‘    sv_dense_2e4          20Ã— baseline, higher repeats            â•‘\")\n",
    "print(\"â•‘    sv_dense_3e4          30Ã— baseline                            â•‘\")\n",
    "print(\"â•‘    sv_dense_ins_focus    Insertion-enriched                       â•‘\")\n",
    "print(\"â•‘    sv_dense_inv_dup_focus Inversion+duplication-enriched          â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  Output files on Google Drive:                                   â•‘\")\n",
    "print(f\"â•‘    {GDRIVE_DIR}/\")\n",
    "print(f\"â•‘    â”œâ”€â”€ svscribe_models/     (1 .pkl + results JSON)\")\n",
    "print(f\"â•‘    â””â”€â”€ svscribe_models.tar.gz\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  Next steps:                                                     â•‘\")\n",
    "print(\"â•‘    1. Download sv_detector_model.pkl                             â•‘\")\n",
    "print(\"â•‘    2. Place in strandweaver/trained_models/sv_detector/          â•‘\")\n",
    "print(\"â•‘    3. Pipeline auto-loads: 'SVDetectorAI loaded'                 â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

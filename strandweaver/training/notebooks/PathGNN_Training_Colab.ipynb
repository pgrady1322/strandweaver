{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a2384a",
   "metadata": {},
   "source": [
    "# ðŸ§¬ StrandWeaver PathGNN Training on Google Colab\n",
    "\n",
    "Train the **PathGNN** (Graph Neural Network for assembly path prediction) on a free T4 GPU.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Installs StrandWeaver + PyTorch + PyTorch Geometric\n",
    "2. Generates synthetic training data (or loads uploaded data)\n",
    "3. Converts edge/node CSV â†’ PyG graph `Data` objects\n",
    "4. Trains PathGNNModel (EdgeConv + GATv2 attention) with early stopping\n",
    "5. Exports `pathgnn_model.pt` for download\n",
    "\n",
    "**Runtime:** Set to **GPU** via `Runtime â†’ Change runtime type â†’ T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26baed",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify GPU availability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, sys\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ“ GPU detected: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"âš  No GPU detected â€” training will be slower on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PyTorch + PyG for the GNN, plus StrandWeaver itself\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q torch-geometric\n",
    "!pip install -q xgboost scikit-learn\n",
    "\n",
    "# Clone StrandWeaver and install\n",
    "!git clone https://github.com/pgrady1322/strandweaver.git /content/strandweaver 2>/dev/null || echo \"Already cloned\"\n",
    "!cd /content/strandweaver && pip install -e . --no-deps -q\n",
    "\n",
    "print(\"\\nâœ“ Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0132f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, MessagePassing\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch {torch.__version__}  |  Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}  |  \"\n",
    "          f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a16f4f",
   "metadata": {},
   "source": [
    "## 2. Upload or Generate Training Data\n",
    "\n",
    "âš¡ **Recommended: Option A (Upload)**  \n",
    "Data generation is CPU-only and takes **hours** on Colab. Generate locally instead, then upload just the graph CSVs (~50 MB). The GPU is only used during training (Section 5).\n",
    "\n",
    "**Option A:** Upload pre-generated graph CSVs from your local machine *(~1 min)*  \n",
    "**Option B:** Generate fresh data on Colab *(~30 min per genome â€” NOT recommended)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OPTION A: Upload graph CSVs from local machine  (âš¡ RECOMMENDED)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Step 1 â€” On your LOCAL machine, package just the graph CSV files:\n",
    "#\n",
    "#   cd /path/to/strandweaver      # dev branch\n",
    "#   find training_data_10x -name \"*.csv\" -path \"*/graph_training/*\" \\\n",
    "#       | tar czf graph_csvs.tar.gz -T -\n",
    "#\n",
    "# Step 2 â€” Upload & extract here:\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload graph_csvs.tar.gz\n",
    "\n",
    "import os, tarfile\n",
    "with tarfile.open(\"graph_csvs.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"/content/\")\n",
    "\n",
    "# Auto-detect which directory was extracted\n",
    "DATA_DIR = \"/content/training_data_10x\" if os.path.isdir(\"/content/training_data_10x\") else \"/content/training_data\"\n",
    "\n",
    "# Find all genome directories (handles both flat and batch-nested layouts)\n",
    "import glob\n",
    "genome_dirs = sorted(glob.glob(f'{DATA_DIR}/**/genome_*', recursive=True))\n",
    "if not genome_dirs:\n",
    "    genome_dirs = sorted(glob.glob(f'{DATA_DIR}/genome_*'))\n",
    "\n",
    "print(f\"âœ“ Found {len(genome_dirs)} genome directories in {DATA_DIR}\")\n",
    "print(f\"  Example: {genome_dirs[0] if genome_dirs else 'NONE'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OPTION B: Generate data on Colab  (âš ï¸ SLOW â€” NOT recommended)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Data generation is CPU-only (~30 min/genome). Only use this if you\n",
    "# cannot generate locally. Reduce -n to 3-5 for a quick test.\n",
    "#\n",
    "# GENERATE_ON_COLAB = False  # Flip to True to enable\n",
    "# DATA_DIR = '/content/training_data'\n",
    "#\n",
    "# if GENERATE_ON_COLAB:\n",
    "#     !cd /content/strandweaver && python -m strandweaver.cli train generate-data \\\n",
    "#         --genome-size 1000000 -n 5 \\\n",
    "#         --read-types hifi --read-types ont --read-types hic \\\n",
    "#         --coverage 30 --coverage 20 --coverage 15 \\\n",
    "#         --repeat-density 0.35 --graph-training --seed 42 \\\n",
    "#         -o {DATA_DIR}\n",
    "#     print(f\"\\nâœ“ Training data generated in {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0a8e2",
   "metadata": {},
   "source": [
    "## 3. Load Graph Data & Build PyG Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2986e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# â”€â”€ Schema v2.0 â€” Feature columns (must match graph_training_data.py) â”€â”€\n",
    "\n",
    "# Metadata columns prepended to every CSV row (skipped by loader)\n",
    "METADATA_COLUMNS = [\n",
    "    'genome_id', 'genome_size', 'chromosome_id', 'read_technology',\n",
    "    'coverage_depth', 'error_rate', 'ploidy', 'gc_content_global',\n",
    "    'repeat_density_global', 'heterozygosity_rate', 'random_seed',\n",
    "    'generator_version', 'schema_version',\n",
    "]\n",
    "\n",
    "EDGE_AI_FEATURES = [\n",
    "    'overlap_length', 'overlap_identity', 'read1_length', 'read2_length',\n",
    "    'coverage_r1', 'coverage_r2', 'gc_content_r1', 'gc_content_r2',\n",
    "    'repeat_fraction_r1', 'repeat_fraction_r2',\n",
    "    'kmer_diversity_r1', 'kmer_diversity_r2',\n",
    "    'branching_factor_r1', 'branching_factor_r2',\n",
    "    'hic_support', 'mapping_quality_r1', 'mapping_quality_r2',\n",
    "    # v2.0: graph topology\n",
    "    'clustering_coeff_r1', 'clustering_coeff_r2', 'component_size',\n",
    "    # v2.0: sequence complexity\n",
    "    'entropy_r1', 'entropy_r2', 'homopolymer_max_r1', 'homopolymer_max_r2',\n",
    "]\n",
    "\n",
    "EDGE_AI_PROVENANCE = [\n",
    "    'node_id_r1', 'node_id_r2',\n",
    "    'read1_haplotype', 'read2_haplotype',\n",
    "    'genomic_distance', 'is_repeat_region',\n",
    "]\n",
    "\n",
    "NODE_FEATURES = [\n",
    "    'coverage', 'gc_content', 'repeat_fraction', 'kmer_diversity',\n",
    "    'branching_factor', 'hic_contact_density', 'allele_frequency',\n",
    "    'heterozygosity', 'phase_consistency', 'mappability',\n",
    "    'hic_intra_contacts', 'hic_inter_contacts',\n",
    "    'hic_contact_ratio', 'hic_phase_signal',\n",
    "    # v2.0: graph topology\n",
    "    'clustering_coeff', 'component_size',\n",
    "    # v2.0: sequence complexity\n",
    "    'shannon_entropy', 'dinucleotide_bias',\n",
    "    'homopolymer_max_run', 'homopolymer_density', 'low_complexity_fraction',\n",
    "    # v2.0: coverage distribution\n",
    "    'coverage_skewness', 'coverage_kurtosis', 'coverage_cv',\n",
    "    'coverage_p10', 'coverage_p90',\n",
    "]\n",
    "\n",
    "NODE_PROVENANCE = [\n",
    "    'node_id', 'read_haplotype', 'read_start_pos', 'read_end_pos',\n",
    "    'read_length', 'is_in_repeat', 'read_technology',\n",
    "]\n",
    "\n",
    "# Labels aligned with actual training data output\n",
    "EDGE_LABEL_MAP = {'TRUE': 0, 'ALLELIC': 1, 'REPEAT': 2, 'SV_BREAK': 3, 'CHIMERIC': 4}\n",
    "NODE_LABEL_MAP = {'HAP_A': 0, 'HAP_B': 1}\n",
    "NUM_NODE_CLASSES = 2\n",
    "NUM_EDGE_CLASSES = 5  # v2.0: added REPEAT class\n",
    "\n",
    "print(f\"Schema v2.0\")\n",
    "print(f\"Edge features: {len(EDGE_AI_FEATURES)}\")\n",
    "print(f\"Node features: {len(NODE_FEATURES)}\")\n",
    "print(f\"Edge classes: {NUM_EDGE_CLASSES}  Node classes: {NUM_NODE_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b66c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_csvs(genome_dir: str) -> Data:\n",
    "    \"\"\"Load a single genome's graph data into a PyG Data object.\n",
    "\n",
    "    Schema v2.0: Handles metadata columns (prepended), provenance columns\n",
    "    (appended before label), and the expanded feature sets. Also backward-\n",
    "    compatible with v1.0 CSVs (missing features are zero-padded).\n",
    "\n",
    "    Uses node_id_r1/node_id_r2 columns for REAL graph topology (not random).\n",
    "    Also loads node_id from diploid CSV for consistent node ordering.\n",
    "\n",
    "    Args:\n",
    "        genome_dir: Path to a genome directory (e.g. .../genome_0001).\n",
    "                    Expects a graph_training/ subdirectory inside.\n",
    "    \"\"\"\n",
    "    graph_dir = Path(genome_dir) / 'graph_training'\n",
    "    if not graph_dir.exists():\n",
    "        if Path(genome_dir).name == 'graph_training':\n",
    "            graph_dir = Path(genome_dir)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # â”€â”€ Load edge data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    edge_csv = list(graph_dir.glob('edge_ai_training_*.csv'))\n",
    "    if not edge_csv:\n",
    "        return None\n",
    "\n",
    "    edge_features = []\n",
    "    edge_labels = []\n",
    "    edge_node_ids = []  # (source_id, target_id) per edge\n",
    "    has_node_ids = False\n",
    "\n",
    "    with open(edge_csv[0]) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        # Check for node ID columns (provenance â€” v2.0)\n",
    "        has_node_ids = ('node_id_r1' in reader.fieldnames and\n",
    "                        'node_id_r2' in reader.fieldnames)\n",
    "\n",
    "        # Check which features are available (v1 may have fewer)\n",
    "        available_edge_feats = [c for c in EDGE_AI_FEATURES if c in reader.fieldnames]\n",
    "        if len(available_edge_feats) < 10:\n",
    "            print(f\"  âš  {edge_csv[0].name}: too few features ({len(available_edge_feats)})\")\n",
    "            return None\n",
    "\n",
    "        for row in reader:\n",
    "            try:\n",
    "                # Use available features, zero-pad any missing (v1â†’v2 compat)\n",
    "                feat = [float(row.get(c, 0.0)) for c in EDGE_AI_FEATURES]\n",
    "                lbl = EDGE_LABEL_MAP.get(row['label'], len(EDGE_LABEL_MAP) - 1)\n",
    "                edge_features.append(feat)\n",
    "                edge_labels.append(lbl)\n",
    "                if has_node_ids:\n",
    "                    edge_node_ids.append((row['node_id_r1'], row['node_id_r2']))\n",
    "            except (KeyError, ValueError):\n",
    "                continue\n",
    "\n",
    "    if len(edge_features) < 10:\n",
    "        return None\n",
    "\n",
    "    # â”€â”€ Load node data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    node_csv = list(graph_dir.glob('diploid_ai_training_*.csv'))\n",
    "    node_features = []\n",
    "    node_labels = []\n",
    "    node_id_list = []  # ordered node IDs\n",
    "    has_node_id_col = False\n",
    "\n",
    "    if node_csv:\n",
    "        with open(node_csv[0]) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            has_node_id_col = 'node_id' in reader.fieldnames\n",
    "\n",
    "            # Check which features are available (v1â†’v2 compat)\n",
    "            available_node_feats = [c for c in NODE_FEATURES if c in reader.fieldnames]\n",
    "            if len(available_node_feats) < 5:\n",
    "                print(f\"  âš  {node_csv[0].name}: too few matching columns ({len(available_node_feats)})\")\n",
    "\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    # Use available features, zero-pad missing ones (v1â†’v2 compat)\n",
    "                    feat = [float(row.get(c, 0.0)) for c in NODE_FEATURES]\n",
    "                    lbl = NODE_LABEL_MAP.get(row.get('haplotype_label', ''), -1)\n",
    "                    node_features.append(feat)\n",
    "                    node_labels.append(lbl)\n",
    "                    if has_node_id_col:\n",
    "                        node_id_list.append(row['node_id'])\n",
    "                except (KeyError, ValueError):\n",
    "                    continue\n",
    "\n",
    "    num_nodes = max(len(node_features), 100)\n",
    "\n",
    "    # â”€â”€ Build edge_index from real topology â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    num_edges = len(edge_features)\n",
    "\n",
    "    if has_node_ids and node_id_list:\n",
    "        # Real topology: map node string IDs â†’ integer indices\n",
    "        node_id_to_idx = {nid: idx for idx, nid in enumerate(node_id_list)}\n",
    "        src_list, dst_list = [], []\n",
    "        keep_mask = []\n",
    "        for i, (src_id, dst_id) in enumerate(edge_node_ids):\n",
    "            src_idx = node_id_to_idx.get(src_id)\n",
    "            dst_idx = node_id_to_idx.get(dst_id)\n",
    "            if src_idx is not None and dst_idx is not None and src_idx != dst_idx:\n",
    "                src_list.append(src_idx)\n",
    "                dst_list.append(dst_idx)\n",
    "                keep_mask.append(True)\n",
    "            else:\n",
    "                keep_mask.append(False)\n",
    "        src = np.array(src_list, dtype=np.int64)\n",
    "        dst = np.array(dst_list, dtype=np.int64)\n",
    "        edge_features = [edge_features[i] for i in range(len(keep_mask)) if keep_mask[i]]\n",
    "        edge_labels = [edge_labels[i] for i in range(len(keep_mask)) if keep_mask[i]]\n",
    "    else:\n",
    "        # Fallback: random topology for old CSVs without node IDs\n",
    "        src = np.random.RandomState(42).randint(0, num_nodes, size=num_edges)\n",
    "        dst = np.random.RandomState(43).randint(0, num_nodes, size=num_edges)\n",
    "        mask = src != dst\n",
    "        src, dst = src[mask], dst[mask]\n",
    "        edge_features = [edge_features[i] for i in range(len(mask)) if mask[i]]\n",
    "        edge_labels = [edge_labels[i] for i in range(len(mask)) if mask[i]]\n",
    "\n",
    "    # â”€â”€ Build node features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if len(node_features) < num_nodes:\n",
    "        while len(node_features) < num_nodes:\n",
    "            node_features.append([0.0] * len(NODE_FEATURES))\n",
    "            node_labels.append(-1)\n",
    "    elif len(node_features) > num_nodes:\n",
    "        node_features = node_features[:num_nodes]\n",
    "        node_labels = node_labels[:num_nodes]\n",
    "\n",
    "    # â”€â”€ Convert to tensors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor([src.tolist(), dst.tolist()], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    y_edge = torch.tensor(edge_labels, dtype=torch.long)\n",
    "    y_node = torch.tensor(node_labels, dtype=torch.long)\n",
    "\n",
    "    # Binary edge label: TRUE (0) vs everything else\n",
    "    y_edge_binary = (y_edge == 0).float()\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y_edge=y_edge,\n",
    "        y_edge_binary=y_edge_binary,\n",
    "        y_node=y_node,\n",
    "    )\n",
    "    return data\n",
    "\n",
    "print(\"Graph loader defined âœ“ (v2.0 schema, real topology from node IDs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef37e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load all genome graphs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# genome_dirs was set in Cell 7 (upload) or Cell 8 (generate)\n",
    "print(f\"Loading {len(genome_dirs)} genomes...\")\n",
    "\n",
    "all_graphs = []\n",
    "for i, gd in enumerate(genome_dirs):\n",
    "    data = load_graph_from_csvs(gd)\n",
    "    if data is not None:\n",
    "        all_graphs.append(data)\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Loaded {i + 1}/{len(genome_dirs)} ({len(all_graphs)} valid)...\")\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(all_graphs)} graphs from {len(genome_dirs)} directories\")\n",
    "if all_graphs:\n",
    "    g = all_graphs[0]\n",
    "    print(f\"  Example graph: {g.num_nodes} nodes, {g.num_edges} edges\")\n",
    "    print(f\"  Node features: {g.x.shape}\")\n",
    "    print(f\"  Edge features: {g.edge_attr.shape}\")\n",
    "    print(f\"  Edge label dist: {dict(zip(*np.unique(g.y_edge.numpy(), return_counts=True)))}\")\n",
    "else:\n",
    "    raise RuntimeError(\"No valid graphs loaded! Check DATA_DIR and CSV contents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train/val split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_graphs, val_graphs = train_test_split(\n",
    "    all_graphs, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training graphs: {len(train_graphs)}\")\n",
    "print(f\"Validation graphs: {len(val_graphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58443849",
   "metadata": {},
   "source": [
    "## 4. Define PathGNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91816906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    \"\"\"Configuration for GNN model.\n",
    "\n",
    "    v2.0 schema: num_node_features=26 (was 14), num_edge_features=24 (was 17).\n",
    "    Added graph topology, sequence complexity, and coverage distribution features.\n",
    "    \"\"\"\n",
    "    num_node_features: int = 26   # v2.0: 14 base + 2 topology + 5 complexity + 5 coverage\n",
    "    num_edge_features: int = 24   # v2.0: 17 base + 3 topology + 4 complexity\n",
    "    hidden_channels: int = 64\n",
    "    num_layers: int = 3\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    batch_size: int = 1          # graph-level batching\n",
    "    epochs: int = 100\n",
    "    patience: int = 15\n",
    "    use_attention: bool = True\n",
    "    use_batch_norm: bool = True\n",
    "    attention_heads: int = 4\n",
    "    output_dim: int = 1\n",
    "    num_node_classes: int = NUM_NODE_CLASSES  # HAP_A, HAP_B\n",
    "    num_ensemble_features: int = 0  # XGBoost ensemble features per edge\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class EdgeConvLayer(MessagePassing):\n",
    "    \"\"\"Custom edge convolution layer with edge features.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.lin_src = nn.Linear(in_channels, out_channels)\n",
    "        self.lin_dst = nn.Linear(in_channels, out_channels)\n",
    "        self.lin_edge = nn.Linear(edge_dim, out_channels)\n",
    "        self.lin_combined = nn.Linear(3 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr=None):\n",
    "        msg_src = self.lin_src(x_i)\n",
    "        msg_dst = self.lin_dst(x_j)\n",
    "        if edge_attr is not None:\n",
    "            msg_edge = self.lin_edge(edge_attr)\n",
    "        else:\n",
    "            msg_edge = torch.zeros_like(msg_src)\n",
    "        msg = torch.cat([msg_src, msg_dst, msg_edge], dim=-1)\n",
    "        return F.relu(self.lin_combined(msg))\n",
    "\n",
    "\n",
    "class PathGNNModel(nn.Module):\n",
    "    \"\"\"Graph Neural Network for assembly path prediction.\n",
    "\n",
    "    Architecture:\n",
    "      - N EdgeConv layers with BatchNorm\n",
    "      - Optional GATv2 attention layer\n",
    "      - Edge classification head (TRUE vs non-TRUE)\n",
    "      - Node classification head (haplotype assignment)\n",
    "\n",
    "    Supports ensemble mode: additional XGBoost prediction features\n",
    "    are concatenated to edge_attr before message passing.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GNNConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device_name = config.device\n",
    "\n",
    "        # Total edge feature dim (base + ensemble)\n",
    "        total_edge_dim = config.num_edge_features + config.num_ensemble_features\n",
    "\n",
    "        # Edge convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        in_ch = config.num_node_features\n",
    "        for _ in range(config.num_layers):\n",
    "            self.convs.append(EdgeConvLayer(in_ch, config.hidden_channels, total_edge_dim))\n",
    "            if config.use_batch_norm:\n",
    "                self.norms.append(nn.BatchNorm1d(config.hidden_channels))\n",
    "            else:\n",
    "                self.norms.append(nn.Identity())\n",
    "            in_ch = config.hidden_channels\n",
    "\n",
    "        # Attention layer\n",
    "        if config.use_attention:\n",
    "            attn_out_per_head = config.hidden_channels // config.attention_heads\n",
    "            self.attention = GATv2Conv(\n",
    "                config.hidden_channels, attn_out_per_head,\n",
    "                heads=config.attention_heads, dropout=config.dropout)\n",
    "            attn_out = attn_out_per_head * config.attention_heads\n",
    "            out_ch = config.hidden_channels + attn_out\n",
    "        else:\n",
    "            self.attention = None\n",
    "            out_ch = config.hidden_channels\n",
    "\n",
    "        # Prediction heads\n",
    "        self.edge_head = nn.Sequential(\n",
    "            nn.Linear(out_ch * 2, config.hidden_channels),\n",
    "            nn.ReLU(), nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_channels, config.output_dim),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.node_head = nn.Sequential(\n",
    "            nn.Linear(out_ch, config.hidden_channels),\n",
    "            nn.ReLU(), nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_channels, config.num_node_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n",
    "\n",
    "        # Concat ensemble features if present\n",
    "        if hasattr(data, 'ensemble_edge_feat') and data.ensemble_edge_feat is not None:\n",
    "            if edge_attr is not None:\n",
    "                edge_attr = torch.cat([edge_attr, data.ensemble_edge_feat], dim=-1)\n",
    "            else:\n",
    "                edge_attr = data.ensemble_edge_feat\n",
    "\n",
    "        # Message passing\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.config.dropout, training=self.training)\n",
    "\n",
    "        # Attention\n",
    "        if self.attention is not None:\n",
    "            x_attn = self.attention(x, edge_index)\n",
    "            x = torch.cat([x, x_attn], dim=-1)\n",
    "\n",
    "        # Edge predictions\n",
    "        src, dst = edge_index\n",
    "        edge_repr = torch.cat([x[src], x[dst]], dim=-1)\n",
    "        edge_logits = self.edge_head(edge_repr).squeeze(-1)\n",
    "\n",
    "        # Node predictions (log_softmax for CrossEntropyLoss)\n",
    "        node_logits = self.node_head(x)\n",
    "\n",
    "        return {'edge_logits': edge_logits, 'node_logits': node_logits, 'node_embeddings': x}\n",
    "\n",
    "\n",
    "# Instantiate with auto-detected feature dims from loaded data\n",
    "if all_graphs:\n",
    "    actual_node_dim = all_graphs[0].x.shape[1]\n",
    "    actual_edge_dim = all_graphs[0].edge_attr.shape[1]\n",
    "    config = GNNConfig(num_node_features=actual_node_dim, num_edge_features=actual_edge_dim)\n",
    "    print(f\"Auto-detected feature dims from data: node={actual_node_dim}, edge={actual_edge_dim}\")\n",
    "else:\n",
    "    config = GNNConfig()\n",
    "    print(f\"Using default feature dims: node={config.num_node_features}, edge={config.num_edge_features}\")\n",
    "\n",
    "model = PathGNNModel(config).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nPathGNNModel: {total_params:,} parameters\")\n",
    "print(f\"  hidden_channels={config.hidden_channels}, layers={config.num_layers}\")\n",
    "print(f\"  attention_heads={config.attention_heads}, node_classes={config.num_node_classes}\")\n",
    "print(f\"  node_features={config.num_node_features}, edge_features={config.num_edge_features}\")\n",
    "print(f\"  ensemble_features={config.num_ensemble_features}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ceb85d",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1590a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Loss functions\n",
    "edge_loss_fn = nn.BCELoss()   # Binary: TRUE edge vs not\n",
    "node_loss_fn = nn.CrossEntropyLoss()  # HAP_A vs HAP_B\n",
    "\n",
    "\n",
    "def train_epoch(model, graphs, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_edge_correct = 0\n",
    "    total_edges = 0\n",
    "\n",
    "    for data in graphs:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        # Edge loss (binary: is this a TRUE edge?)\n",
    "        e_loss = edge_loss_fn(out['edge_logits'], data.y_edge_binary.to(device))\n",
    "\n",
    "        # Node loss (haplotype classification: HAP_A vs HAP_B)\n",
    "        n_loss = torch.tensor(0.0, device=device)\n",
    "        if hasattr(data, 'y_node'):\n",
    "            valid_mask = data.y_node >= 0  # exclude unknown (-1)\n",
    "            if valid_mask.sum() > 0:\n",
    "                n_loss = node_loss_fn(\n",
    "                    out['node_logits'][valid_mask],\n",
    "                    data.y_node[valid_mask].to(device))\n",
    "\n",
    "        loss = e_loss + 0.3 * n_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = (out['edge_logits'] > 0.5).long()\n",
    "        total_edge_correct += (preds == data.y_edge_binary.long().to(device)).sum().item()\n",
    "        total_edges += len(data.y_edge_binary)\n",
    "\n",
    "    avg_loss = total_loss / len(graphs)\n",
    "    accuracy = total_edge_correct / max(1, total_edges)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, graphs):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in graphs:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "\n",
    "        e_loss = edge_loss_fn(out['edge_logits'], data.y_edge_binary.to(device))\n",
    "        total_loss += e_loss.item()\n",
    "\n",
    "        preds = (out['edge_logits'] > 0.5).long().cpu().numpy()\n",
    "        labels = data.y_edge_binary.long().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(graphs))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "\n",
    "print(\"Training functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748430b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NUM_EPOCHS = config.epochs\n",
    "PATIENCE = config.patience\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "print(f\"Training for up to {NUM_EPOCHS} epochs (patience={PATIENCE})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "t_start = time.time()\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_graphs, optimizer)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_graphs)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        marker = ' â˜…'\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        marker = ''\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1 or marker:\n",
    "        print(f\"Epoch {epoch:3d}  \"\n",
    "              f\"train_loss={train_loss:.4f}  \"\n",
    "              f\"val_loss={val_loss:.4f}  \"\n",
    "              f\"val_acc={val_acc:.4f}  \"\n",
    "              f\"val_f1={val_f1:.4f}{marker}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "elapsed = time.time() - t_start\n",
    "print(f\"\\nTraining complete in {elapsed:.1f}s\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Restore best model\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Restored best model checkpoint âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee271214",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history['val_acc'], label='Val Accuracy', linewidth=2, color='green')\n",
    "ax2.plot(history['val_f1'], label='Val F1', linewidth=2, color='orange')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Validation Metrics')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/pathgnn_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved training curves âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31b2b3",
   "metadata": {},
   "source": [
    "## 7. Export Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e213163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Save in StrandWeaver-compatible format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = '/content/trained_pathgnn'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the full model checkpoint\n",
    "save_path = os.path.join(OUTPUT_DIR, 'pathgnn_model.pt')\n",
    "torch.save({\n",
    "    'model_type': 'PathGNNModel',\n",
    "    'config': {\n",
    "        'num_node_features': config.num_node_features,\n",
    "        'num_edge_features': config.num_edge_features,\n",
    "        'hidden_channels': config.hidden_channels,\n",
    "        'num_layers': config.num_layers,\n",
    "        'dropout': config.dropout,\n",
    "        'use_attention': config.use_attention,\n",
    "        'use_batch_norm': config.use_batch_norm,\n",
    "        'output_dim': config.output_dim,\n",
    "        'num_ensemble_features': config.num_ensemble_features,\n",
    "    },\n",
    "    'model_state': model.state_dict(),\n",
    "    'training_history': history,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_val_f1': max(history['val_f1']) if history['val_f1'] else 0,\n",
    "}, save_path)\n",
    "\n",
    "# Also save training metadata\n",
    "meta = {\n",
    "    'model_type': 'PathGNNModel',\n",
    "    'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'num_training_graphs': len(train_graphs),\n",
    "    'num_validation_graphs': len(val_graphs),\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_val_f1': max(history['val_f1']) if history['val_f1'] else 0,\n",
    "    'epochs_trained': len(history['train_loss']),\n",
    "    'config': vars(config),\n",
    "    'uses_real_topology': True,\n",
    "    'uses_hic_features': True,\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, 'training_metadata.json'), 'w') as f:\n",
    "    json.dump(meta, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")\n",
    "print(f\"Size: {os.path.getsize(save_path) / 1024:.1f} KB\")\n",
    "print(f\"Parameters: {meta['num_parameters']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Download model files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from google.colab import files\n",
    "\n",
    "# Zip the output directory\n",
    "!cd /content && tar czf pathgnn_trained.tar.gz trained_pathgnn/\n",
    "\n",
    "print(\"\\nDownloading trained model...\")\n",
    "print(\"After download, extract and copy to your StrandWeaver model dir:\")\n",
    "print(\"  tar xzf pathgnn_trained.tar.gz\")\n",
    "print(\"  cp trained_pathgnn/pathgnn_model.pt trained_models/pathgnn/pathgnn_model.pt\")\n",
    "\n",
    "files.download('/content/pathgnn_trained.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3323e26",
   "metadata": {},
   "source": [
    "## 8. Quick Validation\n",
    "\n",
    "Verify the exported model loads correctly in StrandWeaver's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915cd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify the model loads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "checkpoint = torch.load(save_path, map_location='cpu', weights_only=False)\n",
    "print(\"Checkpoint keys:\", list(checkpoint.keys()))\n",
    "print(f\"Config: {checkpoint['config']}\")\n",
    "print(f\"State dict keys: {len(checkpoint['model_state'])} tensors\")\n",
    "print(f\"Best val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "print(f\"Best val F1: {checkpoint['best_val_f1']:.4f}\")\n",
    "\n",
    "# Reload into a fresh model\n",
    "cfg2 = GNNConfig(**{k: v for k, v in checkpoint['config'].items() if k != 'device'})\n",
    "model2 = PathGNNModel(cfg2)\n",
    "model2.load_state_dict(checkpoint['model_state'])\n",
    "model2.eval()\n",
    "print(f\"\\nâœ“ Model reloads and is ready for inference ({sum(p.numel() for p in model2.parameters()):,} params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e628de",
   "metadata": {},
   "source": [
    "## 9. Ensemble Mode: XGBoost â†’ GNN Features\n",
    "\n",
    "Upload pre-trained XGBoost edge models (from the XGBoost Retraining notebook)\n",
    "to inject their prediction probabilities as extra edge features for the GNN.\n",
    "This commonly gives 1-3% absolute improvement over either model alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae223d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Ensemble: inject XGBoost predictions as GNN edge features â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Upload edgewarden_hifi.pkl (or any tech's edge model) from the\n",
    "# XGBoost retraining notebook's output.\n",
    "#\n",
    "# Skip this cell if you don't have a pre-trained XGBoost model yet â€”\n",
    "# the base GNN will still work fine without ensemble features.\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ENSEMBLE_MODE = False  # Set True after uploading XGBoost model\n",
    "\n",
    "if ENSEMBLE_MODE:\n",
    "    from google.colab import files as colab_files\n",
    "    print(\"Upload edgewarden_hifi.pkl and scaler_hifi.pkl:\")\n",
    "    uploaded_xgb = colab_files.upload()\n",
    "\n",
    "    xgb_model = pickle.loads(list(uploaded_xgb.values())[0])\n",
    "\n",
    "    # For each graph, predict edge scores and attach as ensemble features\n",
    "    for data in all_graphs:\n",
    "        edge_feat_np = data.edge_attr.numpy()\n",
    "        # XGBoost predict_proba â†’ N_classes columns (e.g., 4 for edge_ai)\n",
    "        xgb_probs = xgb_model.predict_proba(edge_feat_np)\n",
    "        data.ensemble_edge_feat = torch.tensor(xgb_probs, dtype=torch.float)\n",
    "\n",
    "    NUM_ENSEMBLE = xgb_probs.shape[1]\n",
    "    print(f\"âœ“ Ensemble features added: {NUM_ENSEMBLE} XGBoost probability columns per edge\")\n",
    "    print(\"  Re-instantiate the model with num_ensemble_features set, then re-train\")\n",
    "else:\n",
    "    NUM_ENSEMBLE = 0\n",
    "    print(\"Ensemble mode OFF â€” set ENSEMBLE_MODE = True after uploading XGBoost model\")\n",
    "    print(\"(The base GNN trains fine without this â€” ensemble is an optional boost)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db33f2f",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Sweep\n",
    "\n",
    "Quick grid search over the most impactful GNN hyperparameters.\n",
    "Each combination trains for up to 60 epochs with early stopping.\n",
    "~2 min per config on T4 GPU â†’ 12 combos â‰ˆ 25 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd4401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Hyperparameter sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import itertools\n",
    "\n",
    "SWEEP_GRID = {\n",
    "    'hidden_channels': [64, 128],\n",
    "    'num_layers': [3, 4],\n",
    "    'dropout': [0.1, 0.3],\n",
    "    'learning_rate': [1e-3, 5e-4],\n",
    "}\n",
    "\n",
    "# Generate all combos\n",
    "keys = list(SWEEP_GRID.keys())\n",
    "combos = list(itertools.product(*[SWEEP_GRID[k] for k in keys]))\n",
    "print(f\"Sweeping {len(combos)} hyperparameter combinations...\")\n",
    "print(f\"Grid: {SWEEP_GRID}\\n\")\n",
    "\n",
    "sweep_results = []\n",
    "\n",
    "for i, vals in enumerate(combos):\n",
    "    params = dict(zip(keys, vals))\n",
    "    print(f\"[{i+1}/{len(combos)}] {params}\")\n",
    "\n",
    "    # Build config\n",
    "    cfg = GNNConfig(\n",
    "        num_node_features=len(NODE_FEATURES),\n",
    "        num_edge_features=len(EDGE_AI_FEATURES),\n",
    "        hidden_channels=params['hidden_channels'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout=params['dropout'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        epochs=60,       # shorter for sweep\n",
    "        patience=10,\n",
    "        num_ensemble_features=NUM_ENSEMBLE,\n",
    "    )\n",
    "\n",
    "    # Build and train model\n",
    "    sweep_model = PathGNNModel(cfg).to(device)\n",
    "    sweep_opt = torch.optim.Adam(\n",
    "        sweep_model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "    sweep_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        sweep_opt, mode='min', factor=0.5, patience=3, verbose=False)\n",
    "\n",
    "    best_vl = float('inf')\n",
    "    best_vf1 = 0.0\n",
    "    patience_ctr = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        tl, ta = train_epoch(sweep_model, train_graphs, sweep_opt)\n",
    "        vl, va, vf1 = evaluate(sweep_model, val_graphs)\n",
    "        sweep_sched.step(vl)\n",
    "\n",
    "        if vl < best_vl:\n",
    "            best_vl = vl\n",
    "            best_vf1 = vf1\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "\n",
    "        if patience_ctr >= cfg.patience:\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    n_params = sum(p.numel() for p in sweep_model.parameters())\n",
    "\n",
    "    result = {\n",
    "        **params,\n",
    "        'best_val_loss': round(best_vl, 4),\n",
    "        'best_val_f1': round(best_vf1, 4),\n",
    "        'epochs': epoch,\n",
    "        'time_s': round(elapsed, 1),\n",
    "        'params': n_params,\n",
    "    }\n",
    "    sweep_results.append(result)\n",
    "    print(f\"  â†’ val_f1={best_vf1:.4f}  val_loss={best_vl:.4f}  \"\n",
    "          f\"({epoch} epochs, {elapsed:.1f}s, {n_params:,} params)\")\n",
    "\n",
    "# Sort by val_f1\n",
    "sweep_results.sort(key=lambda r: r['best_val_f1'], reverse=True)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 5 CONFIGURATIONS:\")\n",
    "print(f\"{'='*80}\")\n",
    "for i, r in enumerate(sweep_results[:5]):\n",
    "    print(f\"  #{i+1}  F1={r['best_val_f1']:.4f}  loss={r['best_val_loss']:.4f}  \"\n",
    "          f\"h={r['hidden_channels']} L={r['num_layers']} \"\n",
    "          f\"d={r['dropout']} lr={r['learning_rate']}  \"\n",
    "          f\"({r['params']:,} params, {r['time_s']:.0f}s)\")\n",
    "\n",
    "# Save sweep results\n",
    "with open('/content/sweep_results.json', 'w') as f:\n",
    "    json.dump(sweep_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395010ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Retrain best config for full epochs and export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best = sweep_results[0]\n",
    "print(f\"Retraining best config: h={best['hidden_channels']} L={best['num_layers']} \"\n",
    "      f\"d={best['dropout']} lr={best['learning_rate']}\")\n",
    "\n",
    "best_cfg = GNNConfig(\n",
    "    num_node_features=len(NODE_FEATURES),\n",
    "    num_edge_features=len(EDGE_AI_FEATURES),\n",
    "    hidden_channels=best['hidden_channels'],\n",
    "    num_layers=best['num_layers'],\n",
    "    dropout=best['dropout'],\n",
    "    learning_rate=best['learning_rate'],\n",
    "    epochs=100,\n",
    "    patience=15,\n",
    "    num_ensemble_features=NUM_ENSEMBLE,\n",
    ")\n",
    "\n",
    "best_model = PathGNNModel(best_cfg).to(device)\n",
    "best_opt = torch.optim.Adam(\n",
    "    best_model.parameters(), lr=best_cfg.learning_rate, weight_decay=best_cfg.weight_decay)\n",
    "best_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    best_opt, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "best_vl = float('inf')\n",
    "best_state = None\n",
    "patience_ctr = 0\n",
    "best_history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(1, best_cfg.epochs + 1):\n",
    "    tl, ta = train_epoch(best_model, train_graphs, best_opt)\n",
    "    vl, va, vf1 = evaluate(best_model, val_graphs)\n",
    "    best_sched.step(vl)\n",
    "\n",
    "    best_history['train_loss'].append(tl)\n",
    "    best_history['val_loss'].append(vl)\n",
    "    best_history['val_acc'].append(va)\n",
    "    best_history['val_f1'].append(vf1)\n",
    "\n",
    "    marker = ''\n",
    "    if vl < best_vl:\n",
    "        best_vl = vl\n",
    "        best_state = best_model.state_dict().copy()\n",
    "        patience_ctr = 0\n",
    "        marker = ' â˜…'\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1 or marker:\n",
    "        print(f\"Epoch {epoch:3d}  train_loss={tl:.4f}  val_loss={vl:.4f}  \"\n",
    "              f\"val_acc={va:.4f}  val_f1={vf1:.4f}{marker}\")\n",
    "\n",
    "    if patience_ctr >= best_cfg.patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "best_model.load_state_dict(best_state)\n",
    "print(f\"\\nâœ“ Best model retrained in {elapsed:.1f}s\")\n",
    "print(f\"  Best val_f1: {max(best_history['val_f1']):.4f}\")\n",
    "print(f\"  Best val_loss: {best_vl:.4f}\")\n",
    "\n",
    "# â”€â”€ Export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SWEEP_DIR = '/content/trained_pathgnn_sweep'\n",
    "os.makedirs(SWEEP_DIR, exist_ok=True)\n",
    "sweep_save = os.path.join(SWEEP_DIR, 'pathgnn_model.pt')\n",
    "torch.save({\n",
    "    'model_type': 'PathGNNModel',\n",
    "    'config': {\n",
    "        'num_node_features': best_cfg.num_node_features,\n",
    "        'num_edge_features': best_cfg.num_edge_features,\n",
    "        'hidden_channels': best_cfg.hidden_channels,\n",
    "        'num_layers': best_cfg.num_layers,\n",
    "        'dropout': best_cfg.dropout,\n",
    "        'use_attention': best_cfg.use_attention,\n",
    "        'use_batch_norm': best_cfg.use_batch_norm,\n",
    "        'output_dim': best_cfg.output_dim,\n",
    "        'num_ensemble_features': best_cfg.num_ensemble_features,\n",
    "    },\n",
    "    'model_state': best_model.state_dict(),\n",
    "    'training_history': best_history,\n",
    "    'best_val_loss': best_vl,\n",
    "    'best_val_f1': max(best_history['val_f1']),\n",
    "    'sweep_results': sweep_results,\n",
    "}, sweep_save)\n",
    "\n",
    "meta = {\n",
    "    'model_type': 'PathGNNModel_sweep',\n",
    "    'num_parameters': sum(p.numel() for p in best_model.parameters()),\n",
    "    'best_config': {k: v for k, v in best.items() if k not in ('params', 'time_s')},\n",
    "    'sweep_combos_tested': len(sweep_results),\n",
    "    'best_val_f1': max(best_history['val_f1']),\n",
    "    'best_val_loss': best_vl,\n",
    "    'uses_real_topology': True,\n",
    "    'uses_hic_features': True,\n",
    "    'uses_ensemble': NUM_ENSEMBLE > 0,\n",
    "}\n",
    "with open(os.path.join(SWEEP_DIR, 'training_metadata.json'), 'w') as f:\n",
    "    json.dump(meta, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nSaved to {sweep_save}\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "!cd /content && tar czf pathgnn_sweep.tar.gz trained_pathgnn_sweep/\n",
    "files.download('/content/pathgnn_sweep.tar.gz')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

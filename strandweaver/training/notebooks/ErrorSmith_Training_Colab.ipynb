{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e99cd83",
   "metadata": {},
   "source": [
    "# ğŸ”¬ StrandWeaver â€” ErrorSmith Training (Colab GPU)\n",
    "\n",
    "Train **1 XGBoost multiclass classifier** for per-base sequencing error prediction.\n",
    "Training uses **chemistry-specific** (flow cell / machine / chemistry) BAMs aligned to **HG002v1.0.1** (Q100 ground truth) and/or **CHM13v2.0**.\n",
    "\n",
    "| Class | Label | Description |\n",
    "|-------|-------|-------------|\n",
    "| 0 | `correct` | Base matches reference |\n",
    "| 1 | `substitution` | Wrong base |\n",
    "| 2 | `insertion` | Extra base in read |\n",
    "| 3 | `deletion` | Missing base in read |\n",
    "| 4 | `homopolymer_error` | Ins/del in homopolymer context |\n",
    "\n",
    "**Data sources:**\n",
    "- **HG002v1.0.1** â€” Q100 T2T assembly, 10 BAMs across 10 chemistry codes (see config cell)\n",
    "- **CHM13v2.0** â€” Original training reference (legacy, 6 chemistries)\n",
    "\n",
    "| Code | Chemistry | Platform | HG002 BAMs |\n",
    "|------|-----------|----------|------------|\n",
    "| 0 | `pacbio_hifi_sequel2` | PacBio HiFi (Sequel II/IIe) | â€” |\n",
    "| 1 | `ont_lsk110_r941` | ONT LSK-110, R9.4.1 | â€” |\n",
    "| 2 | `ont_ulk001_r941` | ONT ULK-001, R9.4.1 (ultra long) | â€” |\n",
    "| 3 | `ont_lsk114_r1041` | ONT LSK-114, R10.4.1 (standard ligation) | â€” |\n",
    "| 4 | `ont_ulk114_r1041` | ONT ULK-114, R10.4.1 (standard ultra long) | âœ“ |\n",
    "| 5 | `illumina_hiseq2500` | Illumina HiSeq 2500 | âœ“ 2Ã—250 |\n",
    "| 6 | `pacbio_onso` | PacBio Onso (short-read SBB, NOT HiFi) | âœ“ NIST 2024Q1 |\n",
    "| 7 | `element_aviti` | Element Aviti (standard / long) | âœ“ Standard + Long |\n",
    "| 8 | `element_ultraq` | Element UltraQ | âœ“ NIST Jun 2024 |\n",
    "| 9 | `pacbio_hifi_revio` | PacBio HiFi (Revio) | âœ“ 3-cell |\n",
    "| 10 | `ont_r1041_duplex` | ONT R10.4.1 Duplex | âœ“ |\n",
    "| 11 | `ont_ulk114_r1041_hiacc` | ONT UL R10.4.1 High-Accuracy (exp.) | âœ“ |\n",
    "| 12 | `ont_ulk114_r1041_dorado` | ONT UL R10.4.1 Dorado | âœ“ |\n",
    "\n",
    "### Setup\n",
    "1. `Runtime â†’ Change runtime type â†’ T4 GPU` (or A100 if available)\n",
    "2. Run cells top-to-bottom\n",
    "\n",
    "### Data options\n",
    "- **Option 1 (recommended):** Transfer HG002 BAMs to Google Drive via E2 rclone (see `Non_Main_Commit_Files/hg002_transfer/`)\n",
    "- **Option 2:** Upload pre-aligned CHM13 BAMs to Google Drive `ErrorSmith_bams/`\n",
    "\n",
    "> **K-Weaver training** is in a separate notebook: `KWeaver_Training_Colab.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f019a5",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938661aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, sys, os\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ“ GPU detected: {result.stdout.strip()}\")\n",
    "    GPU_AVAILABLE = True\n",
    "else:\n",
    "    print(\"âš  No GPU detected â€” XGBoost will use CPU (still fast, just slower HP search)\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# Check RAM\n",
    "import psutil\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f\"âœ“ RAM: {ram_gb:.1f} GB {'(High-RAM âœ“)' if ram_gb > 20 else '(consider High-RAM runtime)'}\")\n",
    "print(f\"âœ“ Python: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q xgboost scikit-learn numpy pandas optuna pysam biopython\n",
    "\n",
    "# Clone StrandWeaver (dev branch) and install â€” pull latest on re-runs\n",
    "!git clone -b dev https://github.com/pgrady1322/strandweaver.git /content/strandweaver 2>/dev/null || \\\n",
    "    (cd /content/strandweaver && git pull origin dev)\n",
    "!cd /content/strandweaver && pip install -e . -q\n",
    "\n",
    "print(\"\\nâœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Mount Google Drive (for saving results) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# â”€â”€ Directory layout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GDRIVE_DIR        = '/content/drive/MyDrive/Colab_Training'\n",
    "ERRORSMITH_OUT    = '/content/errorsmith_training'\n",
    "MODELS_OUT        = '/content/trained_models'\n",
    "GDRIVE_ERRORSMITH = os.path.join(GDRIVE_DIR, 'errorsmith_models')\n",
    "\n",
    "for d in [GDRIVE_DIR, ERRORSMITH_OUT, MODELS_OUT]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Working dirs created\")\n",
    "print(f\"  ErrorSmith output:  {ERRORSMITH_OUT}\")\n",
    "print(f\"  Models output:      {MODELS_OUT}\")\n",
    "print(f\"  Drive export:       {GDRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26849712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify StrandWeaver imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sys.path.insert(0, '/content/strandweaver')\n",
    "sys.path.insert(0, '/content/strandweaver/strandweaver/training/scripts/')\n",
    "\n",
    "from strandweaver.io_utils import SeqRead\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger('colab_training')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# XGBoost device config\n",
    "XGB_DEVICE = 'cuda' if GPU_AVAILABLE else 'cpu'\n",
    "XGB_TREE = 'hist'  # gpu_hist is deprecated in XGBoost 2.x â€” 'hist' auto-uses GPU\n",
    "\n",
    "print(f\"âœ“ StrandWeaver imports OK\")\n",
    "print(f\"âœ“ XGBoost {xgb.__version__}\")\n",
    "print(f\"âœ“ NumPy {np.__version__}\")\n",
    "print(f\"âœ“ Pandas {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e2f8b",
   "metadata": {},
   "source": [
    "## 2. Configuration & Data Source\n",
    "\n",
    "**Edit the cell below to match your setup.**\n",
    "\n",
    "### Option 1: HG002v1.0.1 BAMs from Google Drive (recommended)\n",
    "\n",
    "Transfer HG002 BAMs using the E2 rclone scripts in `Non_Main_Commit_Files/hg002_transfer/`.\n",
    "After transfer, BAMs will be at:\n",
    "```\n",
    "My Drive/Colab_Training/ErrorSmith_bams/HG002/\n",
    "â”œâ”€â”€ illumina/             (HiSeq 2500, 2Ã—250)           code 5\n",
    "â”œâ”€â”€ pacbio_hifi_revio/    (Revio HiFi)                  code 9\n",
    "â”œâ”€â”€ pacbio_onso/          (Onso short-read â€” NOT HiFi)   code 6\n",
    "â”œâ”€â”€ ont_lsk114_duplex/    (R10.4.1 Duplex)               code 10\n",
    "â”œâ”€â”€ ont_ulk114/           (UL R10 standard)              code 4\n",
    "â”œâ”€â”€ ont_ulk114_hiacc/     (UL R10 HiAcc experimental)    code 11\n",
    "â”œâ”€â”€ ont_ulk114_dorado/    (UL R10 Dorado basecaller)     code 12\n",
    "â”œâ”€â”€ element_aviti/        (Aviti Standard + Long)         code 7\n",
    "â”œâ”€â”€ element_ultraq/       (UltraQ)                       code 8\n",
    "â””â”€â”€ reference/            (HG002v1.0.1.fasta.gz)\n",
    "```\n",
    "\n",
    "### Option 2: CHM13 BAMs (legacy)\n",
    "\n",
    "If using CHM13 data, upload to `ErrorSmith_bams/CHM13/` â€” the config cell detects both.\n",
    "\n",
    "### Option 3: Upload FASTQs â†’ align on Colab\n",
    "\n",
    "Set `ERRORSMITH_MODE = 'fastq'` and provide FASTQs in `Colab_Training/fastq/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04187108",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  ERRORSMITH CONFIGURATION â€” edit these before running\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Data source mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 'bam'  : Use pre-aligned BAMs on Google Drive (recommended)\n",
    "# 'fastq': Upload FASTQs to Google Drive, align on Colab\n",
    "ERRORSMITH_MODE = 'bam'\n",
    "\n",
    "# â”€â”€ BAM directory (all BAMs live here) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BAM_DIR = os.path.join(GDRIVE_DIR, 'ErrorSmith_bams')\n",
    "HG002_BAM_DIR = os.path.join(BAM_DIR, 'HG002')\n",
    "CHM13_BAM_DIR = os.path.join(BAM_DIR, 'CHM13')  # legacy, optional\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  HG002v1.0.1 BAM MAP (Q100 ground truth â€” primary training data)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Transferred via Non_Main_Commit_Files/hg002_transfer/transfer_hg002_bams.sh\n",
    "# Each chemistry key matches a CHEMISTRY_CODES entry (0â€“12).\n",
    "\n",
    "HG002_BAM_MAP = {\n",
    "    # Code 4: ONT ULK-114 R10.4.1 (ultra-long, standard basecaller)\n",
    "    'ont_ulk114_r1041': [\n",
    "        os.path.join(HG002_BAM_DIR, 'ont_ulk114', 'hg002v1.0_ont_r10_ul.pri.bam'),\n",
    "    ],\n",
    "    # Code 5: Illumina HiSeq 2500 (2Ã—250)\n",
    "    'illumina_hiseq2500': [\n",
    "        os.path.join(HG002_BAM_DIR, 'illumina', 'hg002v1.0.1_illumina2x250_hg002.dedup.bam'),\n",
    "    ],\n",
    "    # Code 6: PacBio Onso (short-read SBB â€” NOT HiFi, entirely different)\n",
    "    'pacbio_onso': [\n",
    "        os.path.join(HG002_BAM_DIR, 'pacbio_onso', 'hg002v1.0.1_onso_hg002_NIST_2024Q1.dedup.bam'),\n",
    "    ],\n",
    "    # Code 7: Element Aviti (standard + long read)\n",
    "    'element_aviti': [\n",
    "        os.path.join(HG002_BAM_DIR, 'element_aviti', 'hg002v1.0.1_element_hg002_avitistd.dedup.bam'),\n",
    "        os.path.join(HG002_BAM_DIR, 'element_aviti', 'hg002v1.0.1_hg002_element_avitilng.dedup.bam'),\n",
    "    ],\n",
    "    # Code 8: Element UltraQ\n",
    "    'element_ultraq': [\n",
    "        os.path.join(HG002_BAM_DIR, 'element_ultraq', 'hg002v1.0.1_element_hg002_ultraq_jun2024.1.dedup.bam'),\n",
    "    ],\n",
    "    # Code 9: PacBio HiFi Revio (distinct from Sequel II)\n",
    "    'pacbio_hifi_revio': [\n",
    "        os.path.join(HG002_BAM_DIR, 'pacbio_hifi_revio', 'hg002v1.0_hifi_revio_3cells.pri.bam'),\n",
    "    ],\n",
    "    # Code 10: ONT R10.4.1 Duplex basecalling\n",
    "    'ont_r1041_duplex': [\n",
    "        os.path.join(HG002_BAM_DIR, 'ont_lsk114_duplex', 'hg002v1.0_ont_r10_duplex.pri.bam'),\n",
    "    ],\n",
    "    # Code 11: ONT UL R10.4.1 High-Accuracy (experimental)\n",
    "    'ont_ulk114_r1041_hiacc': [\n",
    "        os.path.join(HG002_BAM_DIR, 'ont_ulk114_hiacc', 'all_pass.vhg002v1.bam'),\n",
    "    ],\n",
    "    # Code 12: ONT UL R10.4.1 Dorado basecaller\n",
    "    'ont_ulk114_r1041_dorado': [\n",
    "        os.path.join(HG002_BAM_DIR, 'ont_ulk114_dorado', 'hg002v1.0_ont_r10_ul_dorado.pri.bam'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  CHM13 BAM MAP (legacy â€” optional, merged if present)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CHM13_BAM_MAP = {\n",
    "    'pacbio_hifi_sequel2': [\n",
    "        os.path.join(CHM13_BAM_DIR, 'chm13_SRR11292120_HiFi.sorted.bam'),\n",
    "        os.path.join(CHM13_BAM_DIR, 'chm13_SRR11292121_HiFi.sorted.bam'),\n",
    "    ],\n",
    "    'ont_lsk110_r941':     None,   # TODO: map LSK-110 R9.4.1 reads â†’ CHM13\n",
    "    'ont_ulk001_r941': [\n",
    "        os.path.join(CHM13_BAM_DIR, 'chm13_SRR23513621_ONT_UL_R941.sorted.bam'),\n",
    "    ],\n",
    "    'ont_lsk114_r1041': [\n",
    "        os.path.join(CHM13_BAM_DIR, 'chm13_SRR35645112_ONT_SL_R1041.sorted.bam'),\n",
    "    ],\n",
    "    'ont_ulk114_r1041':    None,   # covered by HG002\n",
    "    'illumina_hiseq2500': [\n",
    "        os.path.join(CHM13_BAM_DIR, 'chm13_SRR1997411_Illumina_PCR_Free.sorted.bam'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# â”€â”€ Merge BAM maps (HG002 takes priority) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BAM_MAP = {}\n",
    "for chem in set(list(HG002_BAM_MAP.keys()) + list(CHM13_BAM_MAP.keys())):\n",
    "    hg002_paths = HG002_BAM_MAP.get(chem) or []\n",
    "    chm13_paths = CHM13_BAM_MAP.get(chem) or []\n",
    "    combined = [p for p in hg002_paths + chm13_paths if p]\n",
    "    if combined:\n",
    "        BAM_MAP[chem] = combined\n",
    "    else:\n",
    "        BAM_MAP[chem] = None\n",
    "\n",
    "# â”€â”€ FASTQ paths (for 'fastq' mode only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FASTQ_DIR = os.path.join(GDRIVE_DIR, 'fastq')\n",
    "\n",
    "FASTQ_MAP = {\n",
    "    'pacbio_hifi_sequel2': [\n",
    "        os.path.join(FASTQ_DIR, 'SRR11292120.fastq.gz'),\n",
    "        os.path.join(FASTQ_DIR, 'SRR11292121.fastq.gz'),\n",
    "    ],\n",
    "    'ont_lsk110_r941':     [],\n",
    "    'ont_ulk001_r941': [\n",
    "        os.path.join(FASTQ_DIR, 'chm13_ont_ul.fastq.gz'),\n",
    "    ],\n",
    "    'ont_lsk114_r1041':            [],\n",
    "    'ont_r1041_duplex':            [],\n",
    "    'ont_ulk114_r1041':            [],\n",
    "    'ont_ulk114_r1041_hiacc':      [],\n",
    "    'ont_ulk114_r1041_dorado':     [],\n",
    "    'illumina_hiseq2500': [\n",
    "        os.path.join(FASTQ_DIR, 'SRR1997411_1.fastq.gz'),\n",
    "        os.path.join(FASTQ_DIR, 'SRR1997411_2.fastq.gz'),\n",
    "    ],\n",
    "    'pacbio_onso':         [],     # HG002 BAMs already aligned\n",
    "    'pacbio_hifi_revio':   [],     # HG002 BAMs already aligned\n",
    "    'element_aviti':       [],     # HG002 BAMs already aligned\n",
    "    'element_ultraq':      [],     # HG002 BAMs already aligned\n",
    "}\n",
    "\n",
    "# â”€â”€ Reference genomes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "HG002_REF_PATH = os.path.join(GDRIVE_DIR, 'ErrorSmith_bams', 'HG002', 'reference', 'hg002v1.0.1.fasta')\n",
    "CHM13_REF_PATH = os.path.join(GDRIVE_DIR, 'chm13v2.0.fa')\n",
    "\n",
    "if os.path.exists(HG002_REF_PATH) or os.path.exists(HG002_REF_PATH + '.gz'):\n",
    "    REFERENCE_PATH = HG002_REF_PATH if os.path.exists(HG002_REF_PATH) else HG002_REF_PATH + '.gz'\n",
    "    print(f\"âœ“ Using HG002v1.0.1 reference (Q100)\")\n",
    "else:\n",
    "    REFERENCE_PATH = CHM13_REF_PATH\n",
    "    print(f\"â„¹ HG002 reference not found, using CHM13v2.0\")\n",
    "\n",
    "AUTO_DOWNLOAD_REF = True\n",
    "\n",
    "# â”€â”€ Subsampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ERRORSMITH_SUBSAMPLE = 5_000_000  # 5M bases/chemistry â†’ ~300k error rows total\n",
    "\n",
    "# â”€â”€ Chromosomes to process â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ERRORSMITH_CHROMS = [f'chr{i}' for i in range(1, 6)]\n",
    "\n",
    "# â”€â”€ Threads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ERRORSMITH_THREADS = 4\n",
    "\n",
    "# â”€â”€ Validate inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from generate_errorsmith_training_data import CHEMISTRY_CODES, CHEMISTRY_PRESETS\n",
    "\n",
    "print(f\"Mode: {ERRORSMITH_MODE}\\n\")\n",
    "\n",
    "if ERRORSMITH_MODE == 'bam':\n",
    "    print(\"Chemistry-specific BAMs:\")\n",
    "    for chem in CHEMISTRY_CODES:\n",
    "        paths = BAM_MAP.get(chem)\n",
    "        if paths is None:\n",
    "            print(f\"  â³ {chem:30s}: not yet mapped (placeholder)\")\n",
    "        elif isinstance(paths, list):\n",
    "            found = [p for p in paths if os.path.exists(p)]\n",
    "            missing = [p for p in paths if not os.path.exists(p)]\n",
    "            if found:\n",
    "                print(f\"  âœ“ {chem:30s}: {len(found)} BAM(s) found\")\n",
    "                for f in found:\n",
    "                    sz = os.path.getsize(f) / (1024**3)\n",
    "                    print(f\"      {os.path.basename(f)} ({sz:.1f} GB)\")\n",
    "            if missing:\n",
    "                for f in missing:\n",
    "                    print(f\"  âœ— {chem:30s}: not found â€” {os.path.basename(f)}\")\n",
    "\n",
    "elif ERRORSMITH_MODE == 'fastq':\n",
    "    print(\"Chemistry-specific FASTQs:\")\n",
    "    for chem in CHEMISTRY_CODES:\n",
    "        fqs = FASTQ_MAP.get(chem, [])\n",
    "        if not fqs:\n",
    "            print(f\"  â³ {chem:30s}: no FASTQs (skipping)\")\n",
    "            continue\n",
    "        found = [f for f in fqs if os.path.exists(f)]\n",
    "        missing = [f for f in fqs if not os.path.exists(f)]\n",
    "        if found:\n",
    "            print(f\"  âœ“ {chem:30s}: {len(found)} file(s) found\")\n",
    "        if missing:\n",
    "            for f in missing:\n",
    "                print(f\"  âœ— {chem:30s}: not found â€” {os.path.basename(f)}\")\n",
    "\n",
    "print(f\"\\n  Subsample:    {ERRORSMITH_SUBSAMPLE:,} bases/chemistry\")\n",
    "print(f\"  Chromosomes:  {ERRORSMITH_CHROMS}\")\n",
    "print(f\"  Reference:    {REFERENCE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2d923",
   "metadata": {},
   "source": [
    "## 3. Download Reference + Install Tools + Align FASTQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069406fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Download CHM13v2.0 reference if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from generate_errorsmith_training_data import download_reference, CHEMISTRY_PRESETS\n",
    "\n",
    "if not os.path.exists(REFERENCE_PATH) and AUTO_DOWNLOAD_REF:\n",
    "    print(\"Downloading CHM13v2.0 reference (800 MB)...\")\n",
    "    ref_dir = os.path.dirname(REFERENCE_PATH)\n",
    "    downloaded_ref = download_reference(Path(ref_dir))\n",
    "    REFERENCE_PATH = str(downloaded_ref)\n",
    "    print(f\"âœ“ Reference downloaded: {REFERENCE_PATH}\")\n",
    "elif os.path.exists(REFERENCE_PATH):\n",
    "    print(f\"âœ“ Reference found: {REFERENCE_PATH}\")\n",
    "else:\n",
    "    print(f\"âš  Reference not found at {REFERENCE_PATH}\")\n",
    "    print(f\"  Set AUTO_DOWNLOAD_REF = True to download automatically\")\n",
    "\n",
    "# â”€â”€ Install minimap2 + samtools (always needed for 'fastq' mode) â”€â”€â”€â”€\n",
    "if ERRORSMITH_MODE == 'fastq':\n",
    "    print(\"\\nInstalling minimap2, samtools...\")\n",
    "    !apt-get install -qq -y samtools minimap2\n",
    "    !which minimap2 && echo \"âœ“ minimap2\" || echo \"âœ— minimap2 missing\"\n",
    "    !which samtools && echo \"âœ“ samtools\" || echo \"âœ— samtools missing\"\n",
    "\n",
    "# â”€â”€ Align FASTQs â†’ sorted BAMs (for 'fastq' mode) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if ERRORSMITH_MODE == 'fastq':\n",
    "    import subprocess\n",
    "\n",
    "    BAM_OUT_DIR = os.path.join(ERRORSMITH_OUT, 'bams')\n",
    "    os.makedirs(BAM_OUT_DIR, exist_ok=True)\n",
    "\n",
    "    def align_fastqs(fastq_files, chemistry, ref_path, out_dir, threads=4):\n",
    "        \"\"\"Align FASTQ(s) with minimap2, sort + index with samtools.\"\"\"\n",
    "        preset = CHEMISTRY_PRESETS[chemistry]\n",
    "        bam_path = os.path.join(out_dir, f'chm13_{chemistry}.sorted.bam')\n",
    "        bai_path = bam_path + '.bai'\n",
    "\n",
    "        if os.path.exists(bam_path) and os.path.exists(bai_path):\n",
    "            print(f\"  âœ“ {chemistry} BAM already exists: {bam_path}\")\n",
    "            return bam_path\n",
    "\n",
    "        # Filter to only files that exist\n",
    "        existing = [f for f in fastq_files if os.path.exists(f)]\n",
    "        if not existing:\n",
    "            print(f\"  âœ— {chemistry}: no FASTQ files found, skipping\")\n",
    "            return None\n",
    "\n",
    "        print(f\"  Aligning {chemistry} ({len(existing)} file(s)) with minimap2 -x {preset}...\")\n",
    "        print(f\"    Files: {[os.path.basename(f) for f in existing]}\")\n",
    "\n",
    "        # minimap2 | samtools sort\n",
    "        mm_cmd = ['minimap2', '-ax', preset, '-t', str(threads), ref_path] + existing\n",
    "        sort_cmd = ['samtools', 'sort', '-@', str(threads), '-o', bam_path]\n",
    "\n",
    "        with subprocess.Popen(mm_cmd, stdout=subprocess.PIPE) as mm:\n",
    "            subprocess.run(sort_cmd, stdin=mm.stdout, check=True)\n",
    "\n",
    "        # Index\n",
    "        subprocess.run(['samtools', 'index', '-@', str(threads), bam_path], check=True)\n",
    "\n",
    "        # Report size\n",
    "        sz = os.path.getsize(bam_path) / (1024**3)\n",
    "        print(f\"  âœ“ {chemistry} BAM ready: {bam_path} ({sz:.1f} GB)\")\n",
    "        return bam_path\n",
    "\n",
    "    # Align each chemistry that has FASTQs\n",
    "    print(\"\\nâ”€â”€ Aligning FASTQs to CHM13v2.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    for chem, fqs in FASTQ_MAP.items():\n",
    "        if not fqs:\n",
    "            print(f\"  â³ {chem}: no FASTQs, skipping\")\n",
    "            continue\n",
    "        result = align_fastqs(fqs, chem, REFERENCE_PATH, BAM_OUT_DIR, ERRORSMITH_THREADS)\n",
    "        if result:\n",
    "            # Update BAM_MAP so the data generation cell finds the aligned BAMs\n",
    "            BAM_MAP[chem] = [result]\n",
    "\n",
    "    # Switch mode so the data generation cell uses the new BAMs\n",
    "    ERRORSMITH_MODE = 'bam'\n",
    "    print(\"\\nâœ“ Alignment complete â€” mode switched to 'bam' for data generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126c1ce",
   "metadata": {},
   "source": [
    "## 4. Generate ErrorSmith Training Data\n",
    "Processes CHM13 BAMs, parses CIGAR alignments, extracts per-base error features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adee741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generate ErrorSmith training CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from generate_errorsmith_training_data import generate_errorsmith_training_data\n",
    "\n",
    "# Build bam_map: chemistry â†’ single BAM path\n",
    "# For chemistries with multiple BAMs (e.g. HiFi has 2 parts),\n",
    "# the training script processes each BAM under the same chemistry code.\n",
    "# Here we flatten lists â†’ pass only the first BAM per chemistry for now.\n",
    "# To use ALL BAMs, merge them first or extend the script to accept lists.\n",
    "resolved_bam_map = {}\n",
    "for chem, paths in BAM_MAP.items():\n",
    "    if paths is None:\n",
    "        print(f\"  â³ {chem}: placeholder â€” skipping\")\n",
    "        continue\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            # Use chemistry + index suffix for multiple BAMs of same chemistry\n",
    "            key = chem\n",
    "            if key in resolved_bam_map:\n",
    "                # Multiple BAMs for same chemistry â€” append part number\n",
    "                i = 2\n",
    "                while f\"{chem}_part{i}\" in resolved_bam_map:\n",
    "                    i += 1\n",
    "                key = f\"{chem}_part{i}\"\n",
    "            resolved_bam_map[key] = p\n",
    "        else:\n",
    "            print(f\"  âœ— {chem}: BAM not found â€” {os.path.basename(p)}\")\n",
    "\n",
    "es_kwargs = {\n",
    "    'output_dir': Path(ERRORSMITH_OUT),\n",
    "    'reference_path': REFERENCE_PATH,\n",
    "    'bam_map': resolved_bam_map,\n",
    "    'subsample': ERRORSMITH_SUBSAMPLE,\n",
    "    'chroms': ERRORSMITH_CHROMS,\n",
    "    'threads': ERRORSMITH_THREADS,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "print(f\"\\nStarting ErrorSmith data generation...\")\n",
    "print(f\"  Mode:        {ERRORSMITH_MODE}\")\n",
    "print(f\"  Chemistries: {list(resolved_bam_map.keys())}\")\n",
    "print(f\"  BAM count:   {len(resolved_bam_map)}\")\n",
    "print()\n",
    "\n",
    "t0 = time.time()\n",
    "errorsmith_csv = generate_errorsmith_training_data(**es_kwargs)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"\\nâœ… ErrorSmith training CSV: {errorsmith_csv}\")\n",
    "print(f\"   Elapsed: {elapsed/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inspect ErrorSmith data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from generate_errorsmith_training_data import CHEMISTRY_NAMES\n",
    "\n",
    "df_es = pd.read_csv(errorsmith_csv)\n",
    "print(f\"Shape: {df_es.shape}\")\n",
    "\n",
    "# Class distribution\n",
    "ERROR_TYPE_NAMES = {0: 'correct', 1: 'substitution', 2: 'insertion', 3: 'deletion', 4: 'homopolymer_error'}\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in df_es['error_type'].value_counts().sort_index().items():\n",
    "    pct = count / len(df_es) * 100\n",
    "    name = ERROR_TYPE_NAMES.get(label, f'unknown_{label}')\n",
    "    print(f\"  {label} ({name:20s}): {count:>10,} ({pct:5.1f}%)\")\n",
    "\n",
    "# Chemistry breakdown\n",
    "print(f\"\\nChemistry breakdown (technology_encoded):\")\n",
    "for code, count in df_es['technology_encoded'].value_counts().sort_index().items():\n",
    "    chem_name = CHEMISTRY_NAMES.get(int(code), f'unknown_{code}')\n",
    "    pct = count / len(df_es) * 100\n",
    "    print(f\"  {int(code)} ({chem_name:25s}): {count:>10,} ({pct:5.1f}%)\")\n",
    "\n",
    "if 'technology' in df_es.columns:\n",
    "    print(f\"\\nTechnology label breakdown:\")\n",
    "    print(df_es['technology'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nFeature summary:\")\n",
    "feature_cols = ['base_quality', 'mean_quality_window_5', 'position_in_read',\n",
    "                'gc_content_local', 'homopolymer_length', 'read_length']\n",
    "df_es[feature_cols].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390c804",
   "metadata": {},
   "source": [
    "## 5. Train ErrorSmith Classifier\n",
    "XGBoost multiclass classifier with **Optuna HP search** + **hybrid resampling** (undersample majority `correct` class, oversample minority error types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ErrorSmith training with Optuna + hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import optuna\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# â”€â”€ Chemistry binary feature decomposition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 10 binary axes derived from Flow_Cells_and_Chemistries.md.\n",
    "# Let XGBoost share signal within technology families.\n",
    "CHEMISTRY_FEATURE_NAMES = [\n",
    "    'is_ont', 'is_pacbio_hifi', 'is_pacbio_onso', 'is_illumina',\n",
    "    'is_element', 'is_long_read', 'is_short_read', 'is_r10',\n",
    "    'is_ultralong', 'is_duplex',\n",
    "]\n",
    "# fmt: off\n",
    "CHEMISTRY_FEATURES = {\n",
    "    #                        ont  hifi onso ilmn elem long shrt r10  ul   dup\n",
    "    0:  [0,   1,   0,   0,   0,   1,   0,   0,   0,   0],   # pacbio_hifi_sequel2\n",
    "    1:  [1,   0,   0,   0,   0,   1,   0,   0,   0,   0],   # ont_lsk110_r941\n",
    "    2:  [1,   0,   0,   0,   0,   1,   0,   0,   1,   0],   # ont_ulk001_r941\n",
    "    3:  [1,   0,   0,   0,   0,   1,   0,   1,   0,   0],   # ont_lsk114_r1041\n",
    "    4:  [1,   0,   0,   0,   0,   1,   0,   1,   1,   0],   # ont_ulk114_r1041\n",
    "    5:  [0,   0,   0,   1,   0,   0,   1,   0,   0,   0],   # illumina_hiseq2500\n",
    "    6:  [0,   0,   1,   0,   0,   0,   1,   0,   0,   0],   # pacbio_onso\n",
    "    7:  [0,   0,   0,   0,   1,   0,   1,   0,   0,   0],   # element_aviti\n",
    "    8:  [0,   0,   0,   0,   1,   0,   1,   0,   0,   0],   # element_ultraq\n",
    "    9:  [0,   1,   0,   0,   0,   1,   0,   0,   0,   0],   # pacbio_hifi_revio\n",
    "    10: [1,   0,   0,   0,   0,   1,   0,   1,   0,   1],   # ont_r1041_duplex\n",
    "    11: [1,   0,   0,   0,   0,   1,   0,   1,   1,   0],   # ont_ulk114_r1041_hiacc\n",
    "    12: [1,   0,   0,   0,   0,   1,   0,   1,   1,   0],   # ont_ulk114_r1041_dorado\n",
    "}\n",
    "# fmt: on\n",
    "\n",
    "# 26 error features (16 original + 10 chemistry binary axes)\n",
    "ERRORSMITH_FEATURES = [\n",
    "    'base_quality', 'mean_quality_window_5', 'mean_quality_window_20',\n",
    "    'position_in_read', 'read_length',\n",
    "    'gc_content_local', 'gc_content_read',\n",
    "    'homopolymer_length', 'homopolymer_base', 'distance_to_hp',\n",
    "    'trinucleotide_context', 'pentanucleotide_context',\n",
    "    'technology_encoded',       # 0-12 granular chemistry code\n",
    "    # 10 binary chemistry axes (shared within technology families)\n",
    "    'is_ont', 'is_pacbio_hifi', 'is_pacbio_onso', 'is_illumina',\n",
    "    'is_element', 'is_long_read', 'is_short_read', 'is_r10',\n",
    "    'is_ultralong', 'is_duplex',\n",
    "    # Reference context\n",
    "    'ref_gc_window_50', 'ref_repeat_flag', 'ref_homopolymer_length',\n",
    "]\n",
    "\n",
    "ES_LABEL = 'error_type'\n",
    "ES_N_OPTUNA_TRIALS = 25\n",
    "ES_N_CV_FOLDS = 5\n",
    "\n",
    "\n",
    "def hybrid_resample(X, y, max_majority=100_000, random_state=42):\n",
    "    \"\"\"\n",
    "    Hybrid resampling: undersample majority to max_majority,\n",
    "    oversample minorities to median class size.\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    class_counts = dict(zip(classes, counts))\n",
    "\n",
    "    # Cap majority class\n",
    "    capped = {c: min(n, max_majority) for c, n in class_counts.items()}\n",
    "    median_size = int(np.median(list(capped.values())))\n",
    "\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in classes:\n",
    "        mask = y == cls\n",
    "        X_cls, y_cls = X[mask], y[mask]\n",
    "        target = max(median_size, min(len(X_cls), max_majority))\n",
    "\n",
    "        if len(X_cls) > target:\n",
    "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
    "                                  random_state=random_state, replace=False)\n",
    "        elif len(X_cls) < target:\n",
    "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
    "                                  random_state=random_state, replace=True)\n",
    "        else:\n",
    "            X_rs, y_rs = X_cls, y_cls\n",
    "\n",
    "        X_resampled.append(X_rs)\n",
    "        y_resampled.append(y_rs)\n",
    "\n",
    "    return np.vstack(X_resampled), np.concatenate(y_resampled)\n",
    "\n",
    "\n",
    "def es_optuna_objective(trial, X, y):\n",
    "    \"\"\"Optuna objective for ErrorSmith multiclass classifier.\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 14),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'tree_method': XGB_TREE,\n",
    "        'device': XGB_DEVICE,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# â”€â”€ Prepare data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_es = pd.read_csv(errorsmith_csv)\n",
    "\n",
    "# Backward compat: derive binary features if CSV was generated before\n",
    "# the chemistry feature decomposition was added.\n",
    "if 'is_ont' not in df_es.columns:\n",
    "    print(\"âš  CSV lacks binary chemistry columns â€” deriving from technology_encoded\")\n",
    "    for idx, row in df_es.iterrows():\n",
    "        code = int(row['technology_encoded'])\n",
    "        vec = CHEMISTRY_FEATURES.get(code, [0]*len(CHEMISTRY_FEATURE_NAMES))\n",
    "        for fname, val in zip(CHEMISTRY_FEATURE_NAMES, vec):\n",
    "            df_es.at[idx, fname] = val\n",
    "    # Ensure int type\n",
    "    for fname in CHEMISTRY_FEATURE_NAMES:\n",
    "        df_es[fname] = df_es[fname].astype(int)\n",
    "\n",
    "X_es = df_es[ERRORSMITH_FEATURES].fillna(0).values\n",
    "y_es = df_es[ES_LABEL].values\n",
    "\n",
    "print(f\"Raw data: {X_es.shape[0]:,} samples Ã— {X_es.shape[1]} features\")\n",
    "print(f\"Class distribution: {dict(Counter(y_es))}\")\n",
    "\n",
    "# Show which chemistries are represented\n",
    "unique_chems = sorted(df_es['technology_encoded'].unique())\n",
    "print(f\"Chemistries present: {[CHEMISTRY_NAMES.get(int(c), f'unk_{c}') for c in unique_chems]}\")\n",
    "\n",
    "# â”€â”€ Hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_balanced, y_balanced = hybrid_resample(X_es, y_es, max_majority=100_000)\n",
    "print(f\"\\nAfter hybrid resampling: {X_balanced.shape[0]:,} samples\")\n",
    "print(f\"Balanced distribution:  {dict(Counter(y_balanced))}\")\n",
    "\n",
    "# â”€â”€ Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nRunning Optuna ({ES_N_OPTUNA_TRIALS} trials)...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: es_optuna_objective(trial, X_balanced, y_balanced),\n",
    "               n_trials=ES_N_OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params['tree_method'] = XGB_TREE\n",
    "best_params['device'] = XGB_DEVICE\n",
    "best_params['objective'] = 'multi:softprob'\n",
    "best_params['num_class'] = 5\n",
    "best_params['eval_metric'] = 'mlogloss'\n",
    "best_params['random_state'] = SEED\n",
    "\n",
    "print(f\"\\nBest F1-macro: {study.best_value:.4f}\")\n",
    "print(f\"Best params: depth={best_params['max_depth']}, \"\n",
    "      f\"lr={best_params['learning_rate']:.4f}, \"\n",
    "      f\"n_est={best_params['n_estimators']}\")\n",
    "\n",
    "# â”€â”€ 5-fold CV with best params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nRunning {ES_N_CV_FOLDS}-fold CV...\")\n",
    "skf = StratifiedKFold(n_splits=ES_N_CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "cv_acc = []\n",
    "cv_f1 = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_balanced, y_balanced)):\n",
    "    X_train, X_val = X_balanced[train_idx], X_balanced[val_idx]\n",
    "    y_train, y_val = y_balanced[train_idx], y_balanced[val_idx]\n",
    "\n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    cv_acc.append(acc)\n",
    "    cv_f1.append(f1)\n",
    "    print(f\"  Fold {fold+1}: Accuracy={acc:.4f}, F1-macro={f1:.4f}\")\n",
    "\n",
    "print(f\"\\nCV Summary: Accuracy={np.mean(cv_acc):.4f}Â±{np.std(cv_acc):.4f}, \"\n",
    "      f\"F1-macro={np.mean(cv_f1):.4f}Â±{np.std(cv_f1):.4f}\")\n",
    "\n",
    "# â”€â”€ Final model (train on all balanced data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nTraining final model on all {len(y_balanced):,} samples...\")\n",
    "final_es_model = xgb.XGBClassifier(**best_params)\n",
    "final_es_model.fit(X_balanced, y_balanced)\n",
    "\n",
    "# Save\n",
    "errorsmith_save_dir = os.path.join(MODELS_OUT, 'errorsmith')\n",
    "os.makedirs(errorsmith_save_dir, exist_ok=True)\n",
    "\n",
    "pkl_path = os.path.join(errorsmith_save_dir, 'error_classifier.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(final_es_model, f)\n",
    "\n",
    "print(f\"\\nâœ“ Saved: {pkl_path}\")\n",
    "\n",
    "# â”€â”€ Classification report on held-out fold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n{'â•'*60}\")\n",
    "print(\"Classification Report (last CV fold):\")\n",
    "print(f\"{'â•'*60}\")\n",
    "class_names = ['correct', 'substitution', 'insertion', 'deletion', 'HP_error']\n",
    "print(classification_report(y_val, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "# Save results\n",
    "errorsmith_results = {\n",
    "    'best_params': best_params,\n",
    "    'cv_accuracy_mean': float(np.mean(cv_acc)),\n",
    "    'cv_accuracy_std': float(np.std(cv_acc)),\n",
    "    'cv_f1_macro_mean': float(np.mean(cv_f1)),\n",
    "    'cv_f1_macro_std': float(np.std(cv_f1)),\n",
    "    'n_samples_raw': int(len(y_es)),\n",
    "    'n_samples_balanced': int(len(y_balanced)),\n",
    "    'n_features': len(ERRORSMITH_FEATURES),\n",
    "    'class_distribution': dict(Counter(int(c) for c in y_balanced)),\n",
    "    'chemistries_used': [CHEMISTRY_NAMES.get(int(c), str(c)) for c in unique_chems],\n",
    "    'reference': 'HG002v1.0.1' if 'hg002' in REFERENCE_PATH.lower() else 'CHM13v2.0',\n",
    "    'feature_names': ERRORSMITH_FEATURES,\n",
    "}\n",
    "with open(os.path.join(errorsmith_save_dir, 'training_results.json'), 'w') as f:\n",
    "    json.dump(errorsmith_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n{'â•'*60}\")\n",
    "print(f\"  ErrorSmith training complete!\")\n",
    "print(f\"  Model saved to: {pkl_path}\")\n",
    "print(f\"  Features: {len(ERRORSMITH_FEATURES)} (16 base + 10 chemistry axes)\")\n",
    "print(f\"  Reference: {errorsmith_results['reference']}\")\n",
    "print(f\"  Chemistries used: {[CHEMISTRY_NAMES.get(int(c), str(c)) for c in unique_chems]}\")\n",
    "print(f\"{'â•'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9762c4",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Feature importance plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "fig.suptitle('Feature Importance â€” ErrorSmith Error Classifier', fontsize=16, fontweight='bold')\n",
    "\n",
    "es_importances = final_es_model.feature_importances_\n",
    "sorted_idx = np.argsort(es_importances)[-16:]  # All 16 features\n",
    "ax.barh(range(len(sorted_idx)), es_importances[sorted_idx], color='coral')\n",
    "ax.set_yticks(range(len(sorted_idx)))\n",
    "ax.set_yticklabels([ERRORSMITH_FEATURES[i] for i in sorted_idx], fontsize=10)\n",
    "ax.set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(os.path.join(MODELS_OUT, 'errorsmith_feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Feature importance plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18d9ad",
   "metadata": {},
   "source": [
    "## 7. Export Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Package and export to Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import tarfile, shutil, glob\n",
    "\n",
    "GDRIVE_TARBALL = os.path.join(GDRIVE_DIR, 'errorsmith_models.tar.gz')\n",
    "\n",
    "# List all model files\n",
    "model_files = glob.glob(f'{errorsmith_save_dir}/**/*', recursive=True)\n",
    "print(f\"Model files to export ({len(model_files)}):\")\n",
    "for f in sorted(model_files):\n",
    "    if os.path.isfile(f):\n",
    "        size_kb = os.path.getsize(f) / 1024\n",
    "        print(f\"  {os.path.relpath(f, errorsmith_save_dir):40s} {size_kb:>8.1f} KB\")\n",
    "\n",
    "# Create tarball\n",
    "print(f\"\\nCreating tarball...\")\n",
    "with tarfile.open(GDRIVE_TARBALL, 'w:gz') as tar:\n",
    "    tar.add(errorsmith_save_dir, arcname='errorsmith_models')\n",
    "\n",
    "tarball_size = os.path.getsize(GDRIVE_TARBALL) / (1024 * 1024)\n",
    "print(f\"\\nâœ… Exported to Google Drive:\")\n",
    "print(f\"   {GDRIVE_TARBALL}\")\n",
    "print(f\"   Size: {tarball_size:.1f} MB\")\n",
    "\n",
    "# Also copy to Drive for easy access\n",
    "if os.path.exists(GDRIVE_ERRORSMITH):\n",
    "    shutil.rmtree(GDRIVE_ERRORSMITH)\n",
    "shutil.copytree(errorsmith_save_dir, GDRIVE_ERRORSMITH)\n",
    "print(f\"   Copied â†’ {GDRIVE_ERRORSMITH}\")\n",
    "\n",
    "print(f\"\\nğŸ“¥ To download locally:\")\n",
    "print(f\"   1. Open Google Drive â†’ My Drive/Colab Notebooks/strandweaver/\")\n",
    "print(f\"   2. Download errorsmith_models.tar.gz\")\n",
    "print(f\"   3. Extract to strandweaver/trained_models/errorsmith/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e9638",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "chems_used = errorsmith_results.get('chemistries_used', [])\n",
    "ref_used = errorsmith_results.get('reference', 'unknown')\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘         ErrorSmith Model Training â€” Summary                     â•‘\")\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  1 XGBoost Multiclass Classifier:                                â•‘\")\n",
    "print(\"â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â•‘\")\n",
    "print(f\"â•‘    error_classifier       F1={errorsmith_results['cv_f1_macro_mean']:.3f}Â±\"\n",
    "      f\"{errorsmith_results['cv_f1_macro_std']:.3f}  \"\n",
    "      f\"Acc={errorsmith_results['cv_accuracy_mean']:.3f}    â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  Classes: correct | substitution | insertion |                   â•‘\")\n",
    "print(\"â•‘           deletion | homopolymer_error                           â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(f\"â•‘  Reference:    {ref_used:40s}           â•‘\")\n",
    "print(f\"â•‘  Chemistries trained ({len(chems_used)}):                       â•‘\")\n",
    "for c in chems_used:\n",
    "    print(f\"â•‘    â€¢ {c:55s}       â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  Chemistry taxonomy (13 codes):                                  â•‘\")\n",
    "print(\"â•‘     0: pacbio_hifi_sequel2       (HiFi Sequel II/IIe)            â•‘\")\n",
    "print(\"â•‘     1: ont_lsk110_r941           (Ligation, R9.4.1)              â•‘\")\n",
    "print(\"â•‘     2: ont_ulk001_r941           (Ultra-long, R9.4.1)            â•‘\")\n",
    "print(\"â•‘     3: ont_lsk114_r1041          (Ligation, R10.4.1)             â•‘\")\n",
    "print(\"â•‘     4: ont_ulk114_r1041          (Ultra-long, R10.4.1)           â•‘\")\n",
    "print(\"â•‘     5: illumina_hiseq2500        (HiSeq 2500)                   â•‘\")\n",
    "print(\"â•‘     6: pacbio_onso               (Onso short-read SBB)           â•‘\")\n",
    "print(\"â•‘     7: element_aviti             (Aviti standard/long)           â•‘\")\n",
    "print(\"â•‘     8: element_ultraq            (UltraQ)                        â•‘\")\n",
    "print(\"â•‘     9: pacbio_hifi_revio         (HiFi Revio)                    â•‘\")\n",
    "print(\"â•‘    10: ont_r1041_duplex          (R10.4.1 Duplex)                â•‘\")\n",
    "print(\"â•‘    11: ont_ulk114_r1041_hiacc    (UL R10.4.1 HiAcc exp.)        â•‘\")\n",
    "print(\"â•‘    12: ont_ulk114_r1041_dorado   (UL R10.4.1 Dorado)            â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  Output files on Google Drive:                                   â•‘\")\n",
    "print(f\"â•‘    {GDRIVE_DIR}/\")\n",
    "print(f\"â•‘    â”œâ”€â”€ errorsmith_models/    (1 .pkl + results JSON)\")\n",
    "print(f\"â•‘    â””â”€â”€ errorsmith_models.tar.gz\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•‘  Next steps:                                                     â•‘\")\n",
    "print(\"â•‘    1. Download models to local strandweaver/trained_models/      â•‘\")\n",
    "print(\"â•‘    2. Ensure K-Weaver models are also in place                   â•‘\")\n",
    "print(\"â•‘    3. Run: strandweaver core-assemble --use-ai <reads>           â•‘\")\n",
    "print(\"â•‘    4. All 7/7 AI modules now have trained weights! ğŸ‰            â•‘\")\n",
    "print(\"â•‘                                                                  â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

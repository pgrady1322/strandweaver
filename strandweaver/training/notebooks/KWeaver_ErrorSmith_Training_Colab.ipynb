{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f124edc",
   "metadata": {},
   "source": [
    "# ğŸ§¬ StrandWeaver â€” K-Weaver + ErrorSmith Training (Colab GPU)\n",
    "\n",
    "Train the **last 2 AI modules** to complete the StrandWeaver model suite:\n",
    "\n",
    "| Track | Module | Type | What it learns |\n",
    "|-------|--------|------|----------------|\n",
    "| **A** | **K-Weaver** | 4 XGBoost regressors | Predict optimal k-mer sizes (DBG, UL-overlap, extension, polish) from 19 read features |\n",
    "| **B** | **ErrorSmith** | 1 XGBoost classifier | Classify per-base errors (correct / sub / ins / del / HP-error) from 16 alignment features |\n",
    "\n",
    "**Track A** is fully self-contained â€” it synthesises genomes, simulates reads, sweeps k-values, and scores assemblies. No external data needed.\n",
    "\n",
    "**Track B** uses real CHM13 sequencing data. You can either:\n",
    "- Auto-download public SRA reads + align *(requires ~50 GB disk, ~2 h)*\n",
    "- Upload pre-aligned BAMs to Google Drive *(recommended)*\n",
    "\n",
    "### Setup\n",
    "1. `Runtime â†’ Change runtime type â†’ T4 GPU` (or A100 if available)\n",
    "2. Upload this notebook to Colab\n",
    "3. Run cells top-to-bottom â€” sections are clearly marked **[Track A]** and **[Track B]**\n",
    "\n",
    "### Runtime estimates (T4 GPU / High-RAM)\n",
    "| Config | K-Weaver (Track A) | ErrorSmith (Track B) | Total |\n",
    "|--------|--------------------|----------------------|-------|\n",
    "| Quick test | ~10 min (10 genomes) | ~5 min (500k bases) | ~15 min |\n",
    "| Standard | ~1-2 h (100 genomes) | ~30 min (5M bases) | ~2 h |\n",
    "| Full | ~3-4 h (200 genomes) | ~1-2 h (20M bases) | ~5 h |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fe34c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76325f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, sys, os\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ“ GPU detected: {result.stdout.strip()}\")\n",
    "    GPU_AVAILABLE = True\n",
    "else:\n",
    "    print(\"âš  No GPU detected â€” XGBoost will use CPU (still fast, just slower HP search)\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# Check RAM\n",
    "import psutil\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f\"âœ“ RAM: {ram_gb:.1f} GB {'(High-RAM âœ“)' if ram_gb > 20 else '(consider High-RAM runtime)'}\")\n",
    "print(f\"âœ“ Python: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ebf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q xgboost scikit-learn numpy pandas optuna pysam\n",
    "\n",
    "# Clone StrandWeaver (dev branch) and install\n",
    "!git clone -b dev https://github.com/pgrady1322/strandweaver.git /content/strandweaver 2>/dev/null || echo \"Already cloned\"\n",
    "!cd /content/strandweaver && pip install -e . --no-deps -q\n",
    "\n",
    "print(\"\\nâœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Mount Google Drive (for saving results) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# â”€â”€ Directory layout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GDRIVE_DIR     = '/content/drive/MyDrive/Colab Notebooks/strandweaver'\n",
    "KWEAVER_OUT    = '/content/kweaver_training'\n",
    "ERRORSMITH_OUT = '/content/errorsmith_training'\n",
    "MODELS_OUT     = '/content/trained_models'\n",
    "\n",
    "# Final export paths on Drive\n",
    "GDRIVE_KWEAVER  = os.path.join(GDRIVE_DIR, 'kweaver_models')\n",
    "GDRIVE_ERRORSMITH = os.path.join(GDRIVE_DIR, 'errorsmith_models')\n",
    "\n",
    "for d in [GDRIVE_DIR, KWEAVER_OUT, ERRORSMITH_OUT, MODELS_OUT]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Working dirs created\")\n",
    "print(f\"  K-Weaver output:   {KWEAVER_OUT}\")\n",
    "print(f\"  ErrorSmith output:  {ERRORSMITH_OUT}\")\n",
    "print(f\"  Models output:      {MODELS_OUT}\")\n",
    "print(f\"  Drive export:       {GDRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99270aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify StrandWeaver imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sys.path.insert(0, '/content/strandweaver')\n",
    "\n",
    "from strandweaver.preprocessing.kweaver_module import (\n",
    "    FeatureExtractor, ReadFeatures, KmerPrediction, KWeaverPredictor,\n",
    ")\n",
    "from strandweaver.io_utils import SeqRead\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger('colab_training')\n",
    "\n",
    "print(f\"âœ“ StrandWeaver imports OK\")\n",
    "print(f\"âœ“ XGBoost {xgb.__version__}\")\n",
    "print(f\"âœ“ NumPy {np.__version__}\")\n",
    "print(f\"âœ“ Pandas {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977ca8d",
   "metadata": {},
   "source": [
    "---\n",
    "# Track A: K-Weaver Training ğŸ”‘\n",
    "\n",
    "**Goal:** Train 4 XGBoost regressors that predict optimal k-mer sizes from read characteristics.\n",
    "\n",
    "| Model | Predicts | Typical range |\n",
    "|-------|----------|---------------|\n",
    "| `dbg_model.pkl` | DBG construction k | 15â€“127 |\n",
    "| `ul_overlap_model.pkl` | Ultra-long overlap k | 201â€“1001 |\n",
    "| `extension_model.pkl` | Contig extension k | 21â€“127 |\n",
    "| `polish_model.pkl` | Polishing k | 31â€“127 |\n",
    "\n",
    "**Pipeline:** Synthesise genomes â†’ simulate reads â†’ extract 19 ReadFeatures â†’ sweep k-values â†’ score assemblies â†’ pick best k â†’ train regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e729e",
   "metadata": {},
   "source": [
    "### 2A. Configuration\n",
    "Adjust these parameters before running. The **Standard** preset balances quality and runtime (~1-2 h on T4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  K-WEAVER CONFIGURATION â€” edit these before running\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Preset selector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Options: 'quick_test', 'standard', 'full'\n",
    "KWEAVER_PRESET = 'standard'\n",
    "\n",
    "PRESETS = {\n",
    "    'quick_test': {\n",
    "        'num_genomes': 10,\n",
    "        'min_genome_size': 500_000,\n",
    "        'max_genome_size': 2_000_000,\n",
    "        'technologies': ['hifi', 'ont', 'illumina'],\n",
    "        'k_values': [21, 31, 51, 77],\n",
    "    },\n",
    "    'standard': {\n",
    "        'num_genomes': 100,\n",
    "        'min_genome_size': 500_000,\n",
    "        'max_genome_size': 10_000_000,\n",
    "        'technologies': ['hifi', 'ont', 'illumina'],\n",
    "        'k_values': [15, 21, 31, 41, 51, 63, 77, 101],\n",
    "    },\n",
    "    'full': {\n",
    "        'num_genomes': 200,\n",
    "        'min_genome_size': 500_000,\n",
    "        'max_genome_size': 20_000_000,\n",
    "        'technologies': ['hifi', 'ont', 'illumina'],\n",
    "        'k_values': [15, 17, 21, 25, 31, 41, 51, 63, 77, 91, 101, 127],\n",
    "    },\n",
    "}\n",
    "\n",
    "kw_config = PRESETS[KWEAVER_PRESET]\n",
    "SEED = 42\n",
    "\n",
    "print(f\"âœ“ K-Weaver preset: {KWEAVER_PRESET}\")\n",
    "print(f\"  Genomes:      {kw_config['num_genomes']}\")\n",
    "print(f\"  Genome range: {kw_config['min_genome_size']:,} â€“ {kw_config['max_genome_size']:,} bp\")\n",
    "print(f\"  Technologies: {kw_config['technologies']}\")\n",
    "print(f\"  K sweep:      {kw_config['k_values']}\")\n",
    "print(f\"  Total samples: {kw_config['num_genomes'] * len(kw_config['technologies'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f0ced",
   "metadata": {},
   "source": [
    "### 3A. Generate K-Weaver Training Data\n",
    "This cell runs the full data generation pipeline. Progress is logged per-genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbfedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generate K-Weaver training CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# This calls the generate_kweaver_training_data.py script inline\n",
    "# so we can monitor progress in the notebook output.\n",
    "\n",
    "sys.path.insert(0, '/content/strandweaver/scripts')\n",
    "from generate_kweaver_training_data import generate_kweaver_training_data\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "kweaver_csv = generate_kweaver_training_data(\n",
    "    output_dir=Path(KWEAVER_OUT),\n",
    "    num_genomes=kw_config['num_genomes'],\n",
    "    min_genome_size=kw_config['min_genome_size'],\n",
    "    max_genome_size=kw_config['max_genome_size'],\n",
    "    technologies=kw_config['technologies'],\n",
    "    k_values=kw_config['k_values'],\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nâœ… K-Weaver training CSV: {kweaver_csv}\")\n",
    "print(f\"   Elapsed: {elapsed/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974eda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inspect the generated data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_kw = pd.read_csv(kweaver_csv)\n",
    "print(f\"Shape: {df_kw.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "for col in ['best_dbg_k', 'best_ul_k', 'best_extension_k', 'best_polish_k']:\n",
    "    print(f\"  {col}: mean={df_kw[col].mean():.1f}, std={df_kw[col].std():.1f}, \"\n",
    "          f\"range=[{df_kw[col].min()}, {df_kw[col].max()}]\")\n",
    "\n",
    "print(f\"\\nTechnology breakdown:\")\n",
    "print(df_kw['technology'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nFeature summary (first 5 rows):\")\n",
    "feature_cols = [\n",
    "    'mean_read_length', 'median_read_length', 'read_length_n50',\n",
    "    'mean_base_quality', 'estimated_error_rate', 'gc_content',\n",
    "    'read_type_encoded', 'num_reads',\n",
    "]\n",
    "df_kw[feature_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92365cc3",
   "metadata": {},
   "source": [
    "### 4A. Train K-Weaver XGBoost Regressors\n",
    "Trains 4 models with **Optuna Bayesian hyperparameter search** (20 trials each) + 5-fold CV on the best config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68987c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ K-Weaver training with Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import optuna\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# 19 ReadFeature columns (input features)\n",
    "KWEAVER_FEATURES = [\n",
    "    'mean_read_length', 'median_read_length', 'read_length_n50',\n",
    "    'min_read_length', 'max_read_length', 'read_length_std',\n",
    "    'mean_base_quality', 'median_base_quality', 'estimated_error_rate',\n",
    "    'total_bases', 'num_reads',\n",
    "    'estimated_genome_size', 'estimated_coverage',\n",
    "    'gc_content', 'gc_std',\n",
    "    'read_type_encoded', 'is_paired_end',\n",
    "    'kmer_spectrum_peak', 'kmer_diversity',\n",
    "]\n",
    "\n",
    "# 4 target columns (one model each)\n",
    "KWEAVER_TARGETS = {\n",
    "    'dbg_model':       'best_dbg_k',\n",
    "    'ul_overlap_model': 'best_ul_k',\n",
    "    'extension_model':  'best_extension_k',\n",
    "    'polish_model':     'best_polish_k',\n",
    "}\n",
    "\n",
    "N_OPTUNA_TRIALS = 20\n",
    "N_CV_FOLDS = 5\n",
    "\n",
    "# XGBoost device config\n",
    "XGB_DEVICE = 'cuda' if GPU_AVAILABLE else 'cpu'\n",
    "XGB_TREE = 'hist'  # gpu_hist is deprecated in XGBoost 2.x â€” 'hist' auto-uses GPU\n",
    "\n",
    "\n",
    "def optuna_objective(trial, X, y):\n",
    "    \"\"\"Optuna objective for XGBoost regression HP search.\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'tree_method': XGB_TREE,\n",
    "        'device': XGB_DEVICE,\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# â”€â”€ Prepare data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_kw = pd.read_csv(kweaver_csv)\n",
    "\n",
    "# Fill NaN in optional features (genome_size, coverage, kmer_spectrum, kmer_diversity)\n",
    "df_kw[KWEAVER_FEATURES] = df_kw[KWEAVER_FEATURES].fillna(0)\n",
    "\n",
    "X_all = df_kw[KWEAVER_FEATURES].values\n",
    "print(f\"Training data: {X_all.shape[0]} samples Ã— {X_all.shape[1]} features\\n\")\n",
    "\n",
    "# â”€â”€ Train each model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "kweaver_results = {}\n",
    "kweaver_save_dir = os.path.join(MODELS_OUT, 'kweaver')\n",
    "os.makedirs(kweaver_save_dir, exist_ok=True)\n",
    "\n",
    "for model_name, target_col in KWEAVER_TARGETS.items():\n",
    "    print(f\"{'â•'*60}\")\n",
    "    print(f\"  Training: {model_name} â†’ {target_col}\")\n",
    "    print(f\"{'â•'*60}\")\n",
    "\n",
    "    y = df_kw[target_col].values\n",
    "\n",
    "    # â”€â”€ Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(f\"  Running Optuna ({N_OPTUNA_TRIALS} trials)...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: optuna_objective(trial, X_all, y),\n",
    "                   n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params['tree_method'] = XGB_TREE\n",
    "    best_params['device'] = XGB_DEVICE\n",
    "    best_params['random_state'] = SEED\n",
    "\n",
    "    print(f\"  Best MSE (neg): {study.best_value:.4f}\")\n",
    "    print(f\"  Best params: depth={best_params['max_depth']}, \"\n",
    "          f\"lr={best_params['learning_rate']:.4f}, \"\n",
    "          f\"n_est={best_params['n_estimators']}\")\n",
    "\n",
    "    # â”€â”€ 5-fold CV with best params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(f\"  Running {N_CV_FOLDS}-fold CV with best params...\")\n",
    "    model = xgb.XGBRegressor(**best_params)\n",
    "    kf = KFold(n_splits=N_CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    cv_mse = []\n",
    "    cv_r2 = []\n",
    "    cv_mae = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
    "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "        cv_mse.append(mse)\n",
    "        cv_r2.append(r2)\n",
    "        cv_mae.append(mae)\n",
    "        print(f\"    Fold {fold+1}: MSE={mse:.2f}, MAE={mae:.2f}, RÂ²={r2:.4f}\")\n",
    "\n",
    "    print(f\"  CV Summary: MSE={np.mean(cv_mse):.2f}Â±{np.std(cv_mse):.2f}, \"\n",
    "          f\"RÂ²={np.mean(cv_r2):.4f}Â±{np.std(cv_r2):.4f}, \"\n",
    "          f\"MAE={np.mean(cv_mae):.2f}\")\n",
    "\n",
    "    # â”€â”€ Final model (train on all data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    final_model = xgb.XGBRegressor(**best_params)\n",
    "    final_model.fit(X_all, y)\n",
    "\n",
    "    # Save\n",
    "    pkl_path = os.path.join(kweaver_save_dir, f'{model_name}.pkl')\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    print(f\"  âœ“ Saved: {pkl_path}\")\n",
    "\n",
    "    kweaver_results[model_name] = {\n",
    "        'target': target_col,\n",
    "        'best_params': best_params,\n",
    "        'cv_mse_mean': float(np.mean(cv_mse)),\n",
    "        'cv_mse_std': float(np.std(cv_mse)),\n",
    "        'cv_r2_mean': float(np.mean(cv_r2)),\n",
    "        'cv_r2_std': float(np.std(cv_r2)),\n",
    "        'cv_mae_mean': float(np.mean(cv_mae)),\n",
    "        'cv_mae_std': float(np.std(cv_mae)),\n",
    "        'n_samples': len(y),\n",
    "    }\n",
    "    print()\n",
    "\n",
    "# Save results summary\n",
    "results_path = os.path.join(kweaver_save_dir, 'training_results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(kweaver_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n{'â•'*60}\")\n",
    "print(f\"  K-Weaver training complete!\")\n",
    "print(f\"  Models saved to: {kweaver_save_dir}\")\n",
    "print(f\"{'â•'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2154e",
   "metadata": {},
   "source": [
    "### 5A. Validate K-Weaver Models\n",
    "Quick sanity check: load the saved models and run inference on a synthetic sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Validate: load models and run sample predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Loading trained K-Weaver models for validation...\\n\")\n",
    "\n",
    "# Load all 4 models\n",
    "kw_models = {}\n",
    "for model_name in KWEAVER_TARGETS:\n",
    "    pkl_path = os.path.join(kweaver_save_dir, f'{model_name}.pkl')\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        kw_models[model_name] = pickle.load(f)\n",
    "    print(f\"  âœ“ Loaded {model_name}\")\n",
    "\n",
    "# Test predictions on 3 representative samples from training data\n",
    "print(f\"\\n{'â”€'*70}\")\n",
    "print(f\"{'Technology':<12} {'DBG k':>8} {'UL k':>8} {'Ext k':>8} {'Polish k':>8}\")\n",
    "print(f\"{'â”€'*70}\")\n",
    "\n",
    "test_samples = df_kw.groupby('technology').first().reset_index()\n",
    "for _, row in test_samples.iterrows():\n",
    "    x = row[KWEAVER_FEATURES].values.reshape(1, -1).astype(np.float64)\n",
    "    preds = {name: int(round(model.predict(x)[0])) for name, model in kw_models.items()}\n",
    "    tech = row['technology']\n",
    "    print(f\"{tech:<12} {preds['dbg_model']:>8} {preds['ul_overlap_model']:>8} \"\n",
    "          f\"{preds['extension_model']:>8} {preds['polish_model']:>8}\")\n",
    "\n",
    "print(f\"{'â”€'*70}\")\n",
    "print(f\"\\nâœ“ K-Weaver validation complete â€” predictions look reasonable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbdf33",
   "metadata": {},
   "source": [
    "---\n",
    "# Track B: ErrorSmith Training ğŸ”¬\n",
    "\n",
    "**Goal:** Train 1 XGBoost multiclass classifier for per-base error type prediction.\n",
    "\n",
    "| Class | Label | Description |\n",
    "|-------|-------|-------------|\n",
    "| 0 | `correct` | Base matches reference |\n",
    "| 1 | `substitution` | Wrong base |\n",
    "| 2 | `insertion` | Extra base in read |\n",
    "| 3 | `deletion` | Missing base in read |\n",
    "| 4 | `homopolymer_error` | Ins/del in homopolymer context |\n",
    "\n",
    "**Data source:** Real CHM13 T2T alignments (public SRA data)\n",
    "\n",
    "**Two options:**\n",
    "- **Option 1 (recommended):** Upload pre-aligned BAMs to Google Drive\n",
    "- **Option 2:** Auto-download from SRA *(requires sra-toolkit, minimap2, samtools â€” ~50 GB disk, ~2 h)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd886b",
   "metadata": {},
   "source": [
    "### 2B. Configuration & Data Source\n",
    "\n",
    "**Edit the cell below to match your setup.**\n",
    "\n",
    "If uploading BAMs to Google Drive, place them at:\n",
    "```\n",
    "My Drive/Colab Notebooks/strandweaver/chm13_hifi.sorted.bam    (+ .bai)\n",
    "My Drive/Colab Notebooks/strandweaver/chm13_ont.sorted.bam     (+ .bai)\n",
    "My Drive/Colab Notebooks/strandweaver/chm13_illumina.sorted.bam (+ .bai)\n",
    "```\n",
    "\n",
    "If using auto-download, the script will fetch reads from SRA and align them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  ERRORSMITH CONFIGURATION â€” edit these before running\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Data source mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 'drive'    : Use BAMs uploaded to Google Drive (recommended)\n",
    "# 'download' : Auto-download from SRA + align (requires disk space + tools)\n",
    "ERRORSMITH_MODE = 'drive'\n",
    "\n",
    "# â”€â”€ BAM paths (for 'drive' mode) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set to None for any technology you don't have\n",
    "HIFI_BAM     = os.path.join(GDRIVE_DIR, 'chm13_hifi.sorted.bam')\n",
    "ONT_BAM      = os.path.join(GDRIVE_DIR, 'chm13_ont.sorted.bam')\n",
    "ILLUMINA_BAM = os.path.join(GDRIVE_DIR, 'chm13_illumina.sorted.bam')\n",
    "\n",
    "# â”€â”€ Reference genome â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Will auto-download CHM13v2.0 if not found\n",
    "REFERENCE_PATH = os.path.join(GDRIVE_DIR, 'chm13v2.0.fa')\n",
    "AUTO_DOWNLOAD_REF = True  # Download if not found\n",
    "\n",
    "# â”€â”€ Subsampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Bases per technology â€” higher = more training data but slower\n",
    "ERRORSMITH_SUBSAMPLE = 5_000_000  # 5M bases/tech â†’ ~300k error rows total\n",
    "\n",
    "# â”€â”€ Chromosomes to process â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Default: chr1-chr5 (good coverage, manageable runtime)\n",
    "ERRORSMITH_CHROMS = [f'chr{i}' for i in range(1, 6)]\n",
    "\n",
    "# â”€â”€ Threads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ERRORSMITH_THREADS = 4\n",
    "\n",
    "# â”€â”€ Validate BAMs exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if ERRORSMITH_MODE == 'drive':\n",
    "    bam_status = {}\n",
    "    for name, path in [('HiFi', HIFI_BAM), ('ONT', ONT_BAM), ('Illumina', ILLUMINA_BAM)]:\n",
    "        exists = os.path.exists(path) if path else False\n",
    "        bam_status[name] = exists\n",
    "        status = 'âœ“' if exists else 'âœ— not found'\n",
    "        print(f\"  {name}: {status}  ({path})\")\n",
    "\n",
    "    available_techs = sum(bam_status.values())\n",
    "    if available_techs == 0:\n",
    "        print(f\"\\nâš  No BAMs found! Either:\")\n",
    "        print(f\"  1. Upload BAMs to {GDRIVE_DIR}\")\n",
    "        print(f\"  2. Set ERRORSMITH_MODE = 'download'\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ {available_techs}/3 technologies available\")\n",
    "else:\n",
    "    print(f\"âœ“ Mode: auto-download from SRA\")\n",
    "    print(f\"  This will download ~50 GB of data and requires:\")\n",
    "    print(f\"  sra-toolkit, minimap2, samtools\")\n",
    "\n",
    "print(f\"\\n  Subsample:    {ERRORSMITH_SUBSAMPLE:,} bases/tech\")\n",
    "print(f\"  Chromosomes:  {ERRORSMITH_CHROMS}\")\n",
    "print(f\"  Reference:    {REFERENCE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c56c9",
   "metadata": {},
   "source": [
    "### 3B. Download Reference + Install Tools (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80011db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Download CHM13v2.0 reference if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from generate_errorsmith_training_data import download_reference\n",
    "\n",
    "if not os.path.exists(REFERENCE_PATH) and AUTO_DOWNLOAD_REF:\n",
    "    print(\"Downloading CHM13v2.0 reference (800 MB)...\")\n",
    "    ref_dir = os.path.dirname(REFERENCE_PATH)\n",
    "    downloaded_ref = download_reference(Path(ref_dir))\n",
    "    REFERENCE_PATH = str(downloaded_ref)\n",
    "    print(f\"âœ“ Reference downloaded: {REFERENCE_PATH}\")\n",
    "elif os.path.exists(REFERENCE_PATH):\n",
    "    print(f\"âœ“ Reference found: {REFERENCE_PATH}\")\n",
    "else:\n",
    "    print(f\"âš  Reference not found at {REFERENCE_PATH}\")\n",
    "    print(f\"  Set AUTO_DOWNLOAD_REF = True to download automatically\")\n",
    "\n",
    "# â”€â”€ Install alignment tools for 'download' mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if ERRORSMITH_MODE == 'download':\n",
    "    print(\"\\nInstalling sra-toolkit, minimap2, samtools...\")\n",
    "    !apt-get install -qq -y sra-toolkit samtools\n",
    "    !pip install -q minimap2\n",
    "    # Verify\n",
    "    !which prefetch && echo \"âœ“ sra-toolkit\" || echo \"âœ— sra-toolkit missing\"\n",
    "    !which minimap2 && echo \"âœ“ minimap2\" || echo \"âœ— minimap2 missing\"\n",
    "    !which samtools && echo \"âœ“ samtools\" || echo \"âœ— samtools missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539dfdb",
   "metadata": {},
   "source": [
    "### 4B. Generate ErrorSmith Training Data\n",
    "Processes CHM13 BAMs, parses CIGAR alignments, extracts per-base error features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0368849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generate ErrorSmith training CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from generate_errorsmith_training_data import generate_errorsmith_training_data\n",
    "\n",
    "# Build BAM args based on what's available\n",
    "es_kwargs = {\n",
    "    'output_dir': Path(ERRORSMITH_OUT),\n",
    "    'reference_path': REFERENCE_PATH,\n",
    "    'subsample': ERRORSMITH_SUBSAMPLE,\n",
    "    'chroms': ERRORSMITH_CHROMS,\n",
    "    'threads': ERRORSMITH_THREADS,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "if ERRORSMITH_MODE == 'download':\n",
    "    es_kwargs['download'] = True\n",
    "else:\n",
    "    # Only pass BAMs that exist\n",
    "    if os.path.exists(HIFI_BAM):\n",
    "        es_kwargs['hifi_bam'] = HIFI_BAM\n",
    "    if os.path.exists(ONT_BAM):\n",
    "        es_kwargs['ont_bam'] = ONT_BAM\n",
    "    if os.path.exists(ILLUMINA_BAM):\n",
    "        es_kwargs['illumina_bam'] = ILLUMINA_BAM\n",
    "\n",
    "print(\"Starting ErrorSmith data generation...\")\n",
    "print(f\"  Mode: {ERRORSMITH_MODE}\")\n",
    "print(f\"  BAMs: {[k for k in ['hifi_bam','ont_bam','illumina_bam'] if k in es_kwargs]}\")\n",
    "print()\n",
    "\n",
    "t0 = time.time()\n",
    "errorsmith_csv = generate_errorsmith_training_data(**es_kwargs)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"\\nâœ… ErrorSmith training CSV: {errorsmith_csv}\")\n",
    "print(f\"   Elapsed: {elapsed/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inspect ErrorSmith data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_es = pd.read_csv(errorsmith_csv)\n",
    "print(f\"Shape: {df_es.shape}\")\n",
    "\n",
    "# Class distribution\n",
    "ERROR_TYPE_NAMES = {0: 'correct', 1: 'substitution', 2: 'insertion', 3: 'deletion', 4: 'homopolymer_error'}\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in df_es['error_type'].value_counts().sort_index().items():\n",
    "    pct = count / len(df_es) * 100\n",
    "    name = ERROR_TYPE_NAMES.get(label, f'unknown_{label}')\n",
    "    print(f\"  {label} ({name:20s}): {count:>10,} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTechnology breakdown:\")\n",
    "print(df_es['technology'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nFeature summary:\")\n",
    "feature_cols = ['base_quality', 'mean_quality_window_5', 'position_in_read',\n",
    "                'gc_content_local', 'homopolymer_length', 'read_length']\n",
    "df_es[feature_cols].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65072c",
   "metadata": {},
   "source": [
    "### 5B. Train ErrorSmith Classifier\n",
    "XGBoost multiclass classifier with **Optuna HP search** + **hybrid resampling** (undersample majority `correct` class, oversample minority error types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4362f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ErrorSmith training with Optuna + hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "# 16 error features\n",
    "ERRORSMITH_FEATURES = [\n",
    "    'base_quality', 'mean_quality_window_5', 'mean_quality_window_20',\n",
    "    'position_in_read', 'read_length',\n",
    "    'gc_content_local', 'gc_content_read',\n",
    "    'homopolymer_length', 'homopolymer_base', 'distance_to_hp',\n",
    "    'trinucleotide_context', 'pentanucleotide_context',\n",
    "    'technology_encoded',\n",
    "    'ref_gc_window_50', 'ref_repeat_flag', 'ref_homopolymer_length',\n",
    "]\n",
    "\n",
    "ES_LABEL = 'error_type'\n",
    "ES_N_OPTUNA_TRIALS = 25\n",
    "ES_N_CV_FOLDS = 5\n",
    "\n",
    "\n",
    "def hybrid_resample(X, y, max_majority=100_000, random_state=42):\n",
    "    \"\"\"\n",
    "    Hybrid resampling: undersample majority to max_majority,\n",
    "    oversample minorities to median class size.\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    class_counts = dict(zip(classes, counts))\n",
    "\n",
    "    # Cap majority class\n",
    "    capped = {c: min(n, max_majority) for c, n in class_counts.items()}\n",
    "    median_size = int(np.median(list(capped.values())))\n",
    "\n",
    "    X_resampled, y_resampled = [], []\n",
    "    for cls in classes:\n",
    "        mask = y == cls\n",
    "        X_cls, y_cls = X[mask], y[mask]\n",
    "        target = max(median_size, min(len(X_cls), max_majority))\n",
    "\n",
    "        if len(X_cls) > target:\n",
    "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
    "                                  random_state=random_state, replace=False)\n",
    "        elif len(X_cls) < target:\n",
    "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
    "                                  random_state=random_state, replace=True)\n",
    "        else:\n",
    "            X_rs, y_rs = X_cls, y_cls\n",
    "\n",
    "        X_resampled.append(X_rs)\n",
    "        y_resampled.append(y_rs)\n",
    "\n",
    "    return np.vstack(X_resampled), np.concatenate(y_resampled)\n",
    "\n",
    "\n",
    "def es_optuna_objective(trial, X, y):\n",
    "    \"\"\"Optuna objective for ErrorSmith multiclass classifier.\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 14),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'tree_method': XGB_TREE,\n",
    "        'device': XGB_DEVICE,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "# â”€â”€ Prepare data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_es = pd.read_csv(errorsmith_csv)\n",
    "X_es = df_es[ERRORSMITH_FEATURES].fillna(0).values\n",
    "y_es = df_es[ES_LABEL].values\n",
    "\n",
    "print(f\"Raw data: {X_es.shape[0]:,} samples Ã— {X_es.shape[1]} features\")\n",
    "print(f\"Class distribution: {dict(Counter(y_es))}\")\n",
    "\n",
    "# â”€â”€ Hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_balanced, y_balanced = hybrid_resample(X_es, y_es, max_majority=100_000)\n",
    "print(f\"\\nAfter hybrid resampling: {X_balanced.shape[0]:,} samples\")\n",
    "print(f\"Balanced distribution:  {dict(Counter(y_balanced))}\")\n",
    "\n",
    "# â”€â”€ Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nRunning Optuna ({ES_N_OPTUNA_TRIALS} trials)...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: es_optuna_objective(trial, X_balanced, y_balanced),\n",
    "               n_trials=ES_N_OPTUNA_TRIALS, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params['tree_method'] = XGB_TREE\n",
    "best_params['device'] = XGB_DEVICE\n",
    "best_params['objective'] = 'multi:softprob'\n",
    "best_params['num_class'] = 5\n",
    "best_params['eval_metric'] = 'mlogloss'\n",
    "best_params['random_state'] = SEED\n",
    "\n",
    "print(f\"\\nBest F1-macro: {study.best_value:.4f}\")\n",
    "print(f\"Best params: depth={best_params['max_depth']}, \"\n",
    "      f\"lr={best_params['learning_rate']:.4f}, \"\n",
    "      f\"n_est={best_params['n_estimators']}\")\n",
    "\n",
    "# â”€â”€ 5-fold CV with best params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nRunning {ES_N_CV_FOLDS}-fold CV...\")\n",
    "skf = StratifiedKFold(n_splits=ES_N_CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "cv_acc = []\n",
    "cv_f1 = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_balanced, y_balanced)):\n",
    "    X_train, X_val = X_balanced[train_idx], X_balanced[val_idx]\n",
    "    y_train, y_val = y_balanced[train_idx], y_balanced[val_idx]\n",
    "\n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    cv_acc.append(acc)\n",
    "    cv_f1.append(f1)\n",
    "    print(f\"  Fold {fold+1}: Accuracy={acc:.4f}, F1-macro={f1:.4f}\")\n",
    "\n",
    "print(f\"\\nCV Summary: Accuracy={np.mean(cv_acc):.4f}Â±{np.std(cv_acc):.4f}, \"\n",
    "      f\"F1-macro={np.mean(cv_f1):.4f}Â±{np.std(cv_f1):.4f}\")\n",
    "\n",
    "# â”€â”€ Final model (train on all balanced data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nTraining final model on all {len(y_balanced):,} samples...\")\n",
    "final_es_model = xgb.XGBClassifier(**best_params)\n",
    "final_es_model.fit(X_balanced, y_balanced)\n",
    "\n",
    "# Save\n",
    "errorsmith_save_dir = os.path.join(MODELS_OUT, 'errorsmith')\n",
    "os.makedirs(errorsmith_save_dir, exist_ok=True)\n",
    "\n",
    "pkl_path = os.path.join(errorsmith_save_dir, 'error_classifier.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(final_es_model, f)\n",
    "\n",
    "print(f\"\\nâœ“ Saved: {pkl_path}\")\n",
    "\n",
    "# â”€â”€ Classification report on held-out fold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n{'â•'*60}\")\n",
    "print(\"Classification Report (last CV fold):\")\n",
    "print(f\"{'â•'*60}\")\n",
    "class_names = ['correct', 'substitution', 'insertion', 'deletion', 'HP_error']\n",
    "print(classification_report(y_val, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "# Save results\n",
    "errorsmith_results = {\n",
    "    'best_params': best_params,\n",
    "    'cv_accuracy_mean': float(np.mean(cv_acc)),\n",
    "    'cv_accuracy_std': float(np.std(cv_acc)),\n",
    "    'cv_f1_macro_mean': float(np.mean(cv_f1)),\n",
    "    'cv_f1_macro_std': float(np.std(cv_f1)),\n",
    "    'n_samples_raw': int(len(y_es)),\n",
    "    'n_samples_balanced': int(len(y_balanced)),\n",
    "    'class_distribution': dict(Counter(int(c) for c in y_balanced)),\n",
    "}\n",
    "with open(os.path.join(errorsmith_save_dir, 'training_results.json'), 'w') as f:\n",
    "    json.dump(errorsmith_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n{'â•'*60}\")\n",
    "print(f\"  ErrorSmith training complete!\")\n",
    "print(f\"  Model saved to: {pkl_path}\")\n",
    "print(f\"{'â•'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db1b16",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Feature Importance Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Feature importance plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Feature Importance â€” K-Weaver + ErrorSmith', fontsize=16, fontweight='bold')\n",
    "\n",
    "# K-Weaver models (top row + bottom-left)\n",
    "for idx, (model_name, target_col) in enumerate(KWEAVER_TARGETS.items()):\n",
    "    ax = axes[idx // 3][idx % 3]\n",
    "    model = kw_models[model_name]\n",
    "    importances = model.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)[-10:]  # Top 10\n",
    "    ax.barh(range(10), importances[sorted_idx], color='steelblue')\n",
    "    ax.set_yticks(range(10))\n",
    "    ax.set_yticklabels([KWEAVER_FEATURES[i] for i in sorted_idx], fontsize=9)\n",
    "    ax.set_title(f'K-Weaver: {model_name}', fontsize=11)\n",
    "    ax.set_xlabel('Importance')\n",
    "\n",
    "# ErrorSmith (bottom-right)\n",
    "ax = axes[1][2]\n",
    "es_importances = final_es_model.feature_importances_\n",
    "sorted_idx = np.argsort(es_importances)[-10:]\n",
    "ax.barh(range(10), es_importances[sorted_idx], color='coral')\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_yticklabels([ERRORSMITH_FEATURES[i] for i in sorted_idx], fontsize=9)\n",
    "ax.set_title('ErrorSmith: error_classifier', fontsize=11)\n",
    "ax.set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(os.path.join(MODELS_OUT, 'feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Feature importance plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ad926",
   "metadata": {},
   "source": [
    "## 7. Export Models to Google Drive\n",
    "Package all trained models into a tarball and copy to Google Drive for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Package and export to Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import tarfile, shutil, glob\n",
    "\n",
    "GDRIVE_TARBALL = os.path.join(GDRIVE_DIR, 'kweaver_errorsmith_models.tar.gz')\n",
    "\n",
    "# List all model files\n",
    "model_files = glob.glob(f'{MODELS_OUT}/**/*', recursive=True)\n",
    "print(f\"Model files to export ({len(model_files)}):\")\n",
    "for f in sorted(model_files):\n",
    "    if os.path.isfile(f):\n",
    "        size_kb = os.path.getsize(f) / 1024\n",
    "        print(f\"  {os.path.relpath(f, MODELS_OUT):40s} {size_kb:>8.1f} KB\")\n",
    "\n",
    "# Create tarball\n",
    "print(f\"\\nCreating tarball...\")\n",
    "with tarfile.open(GDRIVE_TARBALL, 'w:gz') as tar:\n",
    "    tar.add(MODELS_OUT, arcname='trained_models')\n",
    "\n",
    "tarball_size = os.path.getsize(GDRIVE_TARBALL) / (1024 * 1024)\n",
    "print(f\"\\nâœ… Exported to Google Drive:\")\n",
    "print(f\"   {GDRIVE_TARBALL}\")\n",
    "print(f\"   Size: {tarball_size:.1f} MB\")\n",
    "\n",
    "# Also copy individual model dirs to Drive for easy access\n",
    "for subdir in ['kweaver', 'errorsmith']:\n",
    "    src = os.path.join(MODELS_OUT, subdir)\n",
    "    dst = os.path.join(GDRIVE_DIR, f'{subdir}_models')\n",
    "    if os.path.isdir(src):\n",
    "        if os.path.exists(dst):\n",
    "            shutil.rmtree(dst)\n",
    "        shutil.copytree(src, dst)\n",
    "        print(f\"   Copied {subdir}/ â†’ {dst}\")\n",
    "\n",
    "print(f\"\\nğŸ“¥ To download locally:\")\n",
    "print(f\"   1. Open Google Drive â†’ My Drive/Colab Notebooks/strandweaver/\")\n",
    "print(f\"   2. Download kweaver_errorsmith_models.tar.gz\")\n",
    "print(f\"   3. Extract to strandweaver/trained_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96906845",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0263d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘          StrandWeaver Model Training â€” Summary              â•‘\")\n",
    "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "print(\"â•‘                                                              â•‘\")\n",
    "print(\"â•‘  TRACK A: K-Weaver (4 regressors)                           â•‘\")\n",
    "print(\"â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â•‘\")\n",
    "for name, res in kweaver_results.items():\n",
    "    print(f\"â•‘    {name:20s}  RÂ²={res['cv_r2_mean']:.3f}Â±{res['cv_r2_std']:.3f}  \"\n",
    "          f\"MAE={res['cv_mae_mean']:.1f}  â•‘\")\n",
    "print(\"â•‘                                                              â•‘\")\n",
    "print(\"â•‘  TRACK B: ErrorSmith (1 classifier)                         â•‘\")\n",
    "print(\"â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â•‘\")\n",
    "print(f\"â•‘    error_classifier       F1={errorsmith_results['cv_f1_macro_mean']:.3f}Â±\"\n",
    "      f\"{errorsmith_results['cv_f1_macro_std']:.3f}  \"\n",
    "      f\"Acc={errorsmith_results['cv_accuracy_mean']:.3f}  â•‘\")\n",
    "print(\"â•‘                                                              â•‘\")\n",
    "print(\"â•‘  Output files on Google Drive:                               â•‘\")\n",
    "print(f\"â•‘    {GDRIVE_DIR}/\")\n",
    "print(f\"â•‘    â”œâ”€â”€ kweaver_models/       (4 .pkl files)\")\n",
    "print(f\"â•‘    â”œâ”€â”€ errorsmith_models/    (1 .pkl file)\")\n",
    "print(f\"â•‘    â””â”€â”€ kweaver_errorsmith_models.tar.gz\")\n",
    "print(\"â•‘                                                              â•‘\")\n",
    "print(\"â•‘  Next steps:                                                 â•‘\")\n",
    "print(\"â•‘    1. Download models to local strandweaver/trained_models/  â•‘\")\n",
    "print(\"â•‘    2. Run: strandweaver core-assemble --use-ai <reads>       â•‘\")\n",
    "print(\"â•‘    3. All 7/7 AI modules now have trained weights! ğŸ‰        â•‘\")\n",
    "print(\"â•‘                                                              â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

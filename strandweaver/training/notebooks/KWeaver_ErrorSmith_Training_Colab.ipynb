{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f124edc",
      "metadata": {
        "id": "6f124edc"
      },
      "source": [
        "# ğŸ§¬ StrandWeaver â€” K-Weaver + ErrorSmith Training (Colab GPU)\n",
        "\n",
        "Train the **last 2 AI modules** to complete the StrandWeaver model suite:\n",
        "\n",
        "| Track | Module | Type | What it learns |\n",
        "|-------|--------|------|----------------|\n",
        "| **A** | **K-Weaver** | 4 XGBoost regressors | Predict optimal k-mer sizes (DBG, UL-overlap, extension, polish) from 19 read features |\n",
        "| **B** | **ErrorSmith** | 1 XGBoost classifier | Classify per-base errors (correct / sub / ins / del / HP-error) from 16 alignment features |\n",
        "\n",
        "**Track A** is fully self-contained â€” it synthesises genomes, simulates reads, sweeps k-values, and scores assemblies. No external data needed.\n",
        "\n",
        "**Track B** uses real CHM13 sequencing data. You can either:\n",
        "- Auto-download public SRA reads + align *(requires ~50 GB disk, ~2 h)*\n",
        "- Upload pre-aligned BAMs to Google Drive *(recommended)*\n",
        "\n",
        "### Setup\n",
        "1. `Runtime â†’ Change runtime type â†’ T4 GPU` (or A100 if available)\n",
        "2. Upload this notebook to Colab\n",
        "3. Run cells top-to-bottom â€” sections are clearly marked **[Track A]** and **[Track B]**\n",
        "\n",
        "### Runtime estimates (T4 GPU / High-RAM)\n",
        "| Config | K-Weaver (Track A) | ErrorSmith (Track B) | Total |\n",
        "|--------|--------------------|----------------------|-------|\n",
        "| Quick test | ~10 min (10 genomes) | ~5 min (500k bases) | ~15 min |\n",
        "| Standard | ~1-2 h (100 genomes) | ~30 min (5M bases) | ~2 h |\n",
        "| Full | ~3-4 h (200 genomes) | ~1-2 h (20M bases) | ~5 h |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29fe34c",
      "metadata": {
        "id": "f29fe34c"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "76325f96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76325f96",
        "outputId": "ef3e9032-6996-4904-a0ca-e5d3fef0eb54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ GPU detected: Tesla T4, 15360 MiB, 580.82.07\n",
            "âœ“ RAM: 13.6 GB (consider High-RAM runtime)\n",
            "âœ“ Python: 3.12.12\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€ Verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import subprocess, sys, os\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "if result.returncode == 0:\n",
        "    print(f\"âœ“ GPU detected: {result.stdout.strip()}\")\n",
        "    GPU_AVAILABLE = True\n",
        "else:\n",
        "    print(\"âš  No GPU detected â€” XGBoost will use CPU (still fast, just slower HP search)\")\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "# Check RAM\n",
        "import psutil\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print(f\"âœ“ RAM: {ram_gb:.1f} GB {'(High-RAM âœ“)' if ram_gb > 20 else '(consider High-RAM runtime)'}\")\n",
        "print(f\"âœ“ Python: {sys.version.split()[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a4ebf44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a4ebf44",
        "outputId": "e78f70bb-2777-4d10-eb51-d6fc3fa4b0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already cloned\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for strandweaver (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "âœ“ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "!pip install -q xgboost scikit-learn numpy pandas optuna pysam biopython\n",
        "\n",
        "# Clone StrandWeaver (dev branch) and install â€” pull latest on re-runs\n",
        "!git clone -b dev https://github.com/pgrady1322/strandweaver.git /content/strandweaver 2>/dev/null || \\\n",
        "    (cd /content/strandweaver && git pull origin dev)\n",
        "!cd /content/strandweaver && pip install -e . -q\n",
        "\n",
        "print(\"\\nâœ“ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1a75bf10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a75bf10",
        "outputId": "99716dd9-38cf-4b2d-fa43-fea1f4dfbf7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Google Drive mounted\n",
            "âœ“ Working dirs created\n",
            "  K-Weaver output:   /content/kweaver_training\n",
            "  ErrorSmith output:  /content/errorsmith_training\n",
            "  Models output:      /content/trained_models\n",
            "  Drive export:       /content/drive/MyDrive/Colab Notebooks/strandweaver\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€ Mount Google Drive (for saving results) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# â”€â”€ Directory layout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "GDRIVE_DIR     = '/content/drive/MyDrive/Colab Notebooks/strandweaver'\n",
        "KWEAVER_OUT    = '/content/kweaver_training'\n",
        "ERRORSMITH_OUT = '/content/errorsmith_training'\n",
        "MODELS_OUT     = '/content/trained_models'\n",
        "\n",
        "# Final export paths on Drive\n",
        "GDRIVE_KWEAVER  = os.path.join(GDRIVE_DIR, 'kweaver_models')\n",
        "GDRIVE_ERRORSMITH = os.path.join(GDRIVE_DIR, 'errorsmith_models')\n",
        "\n",
        "for d in [GDRIVE_DIR, KWEAVER_OUT, ERRORSMITH_OUT, MODELS_OUT]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(f\"âœ“ Google Drive mounted\")\n",
        "print(f\"âœ“ Working dirs created\")\n",
        "print(f\"  K-Weaver output:   {KWEAVER_OUT}\")\n",
        "print(f\"  ErrorSmith output:  {ERRORSMITH_OUT}\")\n",
        "print(f\"  Models output:      {MODELS_OUT}\")\n",
        "print(f\"  Drive export:       {GDRIVE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "99270aa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99270aa4",
        "outputId": "75936a4a-2fa0-4553-8e7d-c19ce3c4c961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ StrandWeaver imports OK\n",
            "âœ“ XGBoost 3.1.3\n",
            "âœ“ NumPy 2.0.2\n",
            "âœ“ Pandas 2.2.2\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€ Verify StrandWeaver imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "sys.path.insert(0, '/content/strandweaver')\n",
        "\n",
        "from strandweaver.preprocessing.kweaver_module import (\n",
        "    FeatureExtractor, ReadFeatures, KmerPrediction, KWeaverPredictor,\n",
        ")\n",
        "from strandweaver.io_utils import SeqRead\n",
        "\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
        "logger = logging.getLogger('colab_training')\n",
        "\n",
        "print(f\"âœ“ StrandWeaver imports OK\")\n",
        "print(f\"âœ“ XGBoost {xgb.__version__}\")\n",
        "print(f\"âœ“ NumPy {np.__version__}\")\n",
        "print(f\"âœ“ Pandas {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4977ca8d",
      "metadata": {
        "id": "4977ca8d"
      },
      "source": [
        "---\n",
        "# Track A: K-Weaver Training ğŸ”‘\n",
        "\n",
        "**Goal:** Train 4 XGBoost regressors that predict optimal k-mer sizes from read characteristics.\n",
        "\n",
        "| Model | Predicts | Typical range |\n",
        "|-------|----------|---------------|\n",
        "| `dbg_model.pkl` | DBG construction k | 15â€“127 |\n",
        "| `ul_overlap_model.pkl` | Ultra-long overlap k | 201â€“1001 |\n",
        "| `extension_model.pkl` | Contig extension k | 21â€“127 |\n",
        "| `polish_model.pkl` | Polishing k | 31â€“127 |\n",
        "\n",
        "**Pipeline:** Synthesise genomes â†’ simulate reads â†’ extract 19 ReadFeatures â†’ sweep k-values â†’ score assemblies â†’ pick best k â†’ train regressors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c1e729e",
      "metadata": {
        "id": "3c1e729e"
      },
      "source": [
        "### 2A. Configuration\n",
        "Adjust these parameters before running. The **Standard** preset balances quality and runtime (~1-2 h on T4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493c2fe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "493c2fe4",
        "outputId": "5b8e88d2-3c08-4a13-a230-f39c3c85cd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ K-Weaver preset: full\n",
            "  Genomes:      200\n",
            "  Genome range: 500,000 â€“ 20,000,000 bp\n",
            "  Technologies: ['hifi', 'ont', 'illumina']\n",
            "  K sweep:      [15, 17, 21, 25, 31, 41, 51, 63, 77, 91, 101, 127]\n",
            "  Total samples: 600\n"
          ]
        }
      ],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  K-WEAVER CONFIGURATION â€” edit these before running\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# â”€â”€ Preset selector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Options: 'quick_test', 'standard', 'full'\n",
        "#\n",
        "# Timing estimates (Colab free tier, T4 GPU):\n",
        "#   quick_test :  ~3 min  ( 10 genomes Ã— 3 techs Ã— 4 k =   120 k-scores)\n",
        "#   standard   : ~15 min  ( 50 genomes Ã— 3 techs Ã— 6 k =   900 k-scores)\n",
        "#   full       : ~30 min  (100 genomes Ã— 3 techs Ã— 8 k = 2,400 k-scores)\n",
        "#\n",
        "# k-sweep uses a fast k-mer spectrum proxy (~0.2 s/k), NOT full\n",
        "# DBG assembly (~45 s/k).  Even 'full' fits comfortably in one session.\n",
        "KWEAVER_PRESET = 'full'\n",
        "\n",
        "PRESETS = {\n",
        "    'quick_test': {\n",
        "        'num_genomes': 10,\n",
        "        'min_genome_size': 500_000,\n",
        "        'max_genome_size': 2_000_000,\n",
        "        'technologies': ['hifi', 'ont', 'illumina'],\n",
        "        'k_values': [21, 31, 51, 77],\n",
        "    },\n",
        "    'standard': {\n",
        "        'num_genomes': 50,\n",
        "        'min_genome_size': 500_000,\n",
        "        'max_genome_size': 5_000_000,\n",
        "        'technologies': ['hifi', 'ont', 'illumina'],\n",
        "        'k_values': [15, 21, 31, 51, 77, 101],\n",
        "    },\n",
        "    'full': {\n",
        "        'num_genomes': 100,\n",
        "        'min_genome_size': 500_000,\n",
        "        'max_genome_size': 5_000_000,\n",
        "        'technologies': ['hifi', 'ont', 'illumina'],\n",
        "        'k_values': [15, 21, 31, 41, 51, 63, 77, 101],\n",
        "    },\n",
        "}\n",
        "\n",
        "kw_config = PRESETS[KWEAVER_PRESET]\n",
        "SEED = 42\n",
        "\n",
        "print(f\"âœ“ K-Weaver preset: {KWEAVER_PRESET}\")\n",
        "print(f\"  Genomes:      {kw_config['num_genomes']}\")\n",
        "print(f\"  Genome range: {kw_config['min_genome_size']:,} â€“ {kw_config['max_genome_size']:,} bp\")\n",
        "print(f\"  Technologies: {kw_config['technologies']}\")\n",
        "print(f\"  K sweep:      {kw_config['k_values']}\")\n",
        "print(f\"  Total samples: {kw_config['num_genomes'] * len(kw_config['technologies'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9f0ced",
      "metadata": {
        "id": "dd9f0ced"
      },
      "source": [
        "### 3A. Generate K-Weaver Training Data\n",
        "This cell runs the full data generation pipeline. Progress is logged per-genome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e3fbfedb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e3fbfedb",
        "outputId": "1bf072e1-0897-4633-b31b-6a9b3b622583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:strandweaver.assembly_core.dbg_engine_module:GPU k-mer extraction unavailable: name 'GPUAvailability' is not defined\n",
            "WARNING:strandweaver.assembly_core.dbg_engine_module:GPU graph construction unavailable: name 'GPUAvailability' is not defined\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2419180301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m kweaver_csv = generate_kweaver_training_data(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKWEAVER_OUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnum_genomes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkw_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_genomes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/training/scripts/generate_kweaver_training_data.py\u001b[0m in \u001b[0;36mgenerate_kweaver_training_data\u001b[0;34m(output_dir, num_genomes, min_genome_size, max_genome_size, technologies, k_values, seed, threads)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    Sweeping k âˆˆ {valid_ks}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m                 best_dbg_k, best_dbg_score, _ = _find_best_k(\n\u001b[0m\u001b[1;32m    614\u001b[0m                     \u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_read_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_rl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 )\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/training/scripts/generate_kweaver_training_data.py\u001b[0m in \u001b[0;36m_find_best_k\u001b[0;34m(reads, reference, k_values, min_read_length)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    Assembling at k={k}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mcontigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_assembly_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score_assembly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mall_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/training/scripts/generate_kweaver_training_data.py\u001b[0m in \u001b[0;36m_run_assembly_at_k\u001b[0;34m(reads, k, min_cov)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mdbg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dbg_from_long_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_coverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  DBG build failed at k={k}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/assembly_core/dbg_engine_module.py\u001b[0m in \u001b[0;36mbuild_dbg_from_long_reads\u001b[0;34m(long_reads, base_k, min_coverage, ml_k_model, hic_phase_info, ul_support_map, ai_annotations)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \"\"\"\n\u001b[1;32m   1412\u001b[0m     \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeBruijnGraphBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_coverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_coverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m     return builder.build_dbg_from_long_reads(\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0mlong_reads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml_k_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mml_k_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0mhic_phase_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhic_phase_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/assembly_core/dbg_engine_module.py\u001b[0m in \u001b[0;36mbuild_dbg_from_long_reads\u001b[0;34m(self, long_reads, ml_k_model, hic_phase_info, ul_support_map, ai_annotations)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Step 1: Extract and count k-mers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mkmer_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_kmers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_reads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracted {len(kmer_counts)} unique {self.base_k}-mers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/assembly_core/dbg_engine_module.py\u001b[0m in \u001b[0;36m_extract_kmers\u001b[0;34m(self, reads)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;31m# Use canonical k-mer (lexicographically smaller of fwd/rev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mcanon_kmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_canonical_kmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                 \u001b[0mkmer_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcanon_kmer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/assembly_core/dbg_engine_module.py\u001b[0m in \u001b[0;36m_canonical_kmer\u001b[0;34m(self, kmer)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_canonical_kmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;34m\"\"\"Return canonical k-mer (lex-min of kmer and reverse complement).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reverse_complement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/strandweaver/strandweaver/assembly_core/dbg_engine_module.py\u001b[0m in \u001b[0;36m_reverse_complement\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;34m\"\"\"Return reverse complement of DNA sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mcomplement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'T'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'G'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'G'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_raw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmer_counts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDBGGraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# â”€â”€ Generate K-Weaver training CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# This calls the generate_kweaver_training_data.py script inline\n",
        "# so we can monitor progress in the notebook output.\n",
        "\n",
        "sys.path.insert(0, '/content/strandweaver/strandweaver/training/scripts/')\n",
        "from generate_kweaver_training_data import generate_kweaver_training_data\n",
        "from pathlib import Path\n",
        "\n",
        "import time\n",
        "t0 = time.time()\n",
        "\n",
        "kweaver_csv = generate_kweaver_training_data(\n",
        "    output_dir=Path(KWEAVER_OUT),\n",
        "    num_genomes=kw_config['num_genomes'],\n",
        "    min_genome_size=kw_config['min_genome_size'],\n",
        "    max_genome_size=kw_config['max_genome_size'],\n",
        "    technologies=kw_config['technologies'],\n",
        "    k_values=kw_config['k_values'],\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f\"\\nâœ… K-Weaver training CSV: {kweaver_csv}\")\n",
        "print(f\"   Elapsed: {elapsed/60:.1f} min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974eda53",
      "metadata": {
        "id": "974eda53"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Inspect the generated data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_kw = pd.read_csv(kweaver_csv)\n",
        "print(f\"Shape: {df_kw.shape}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "for col in ['best_dbg_k', 'best_ul_k', 'best_extension_k', 'best_polish_k']:\n",
        "    print(f\"  {col}: mean={df_kw[col].mean():.1f}, std={df_kw[col].std():.1f}, \"\n",
        "          f\"range=[{df_kw[col].min()}, {df_kw[col].max()}]\")\n",
        "\n",
        "print(f\"\\nTechnology breakdown:\")\n",
        "print(df_kw['technology'].value_counts().to_string())\n",
        "\n",
        "print(f\"\\nFeature summary (first 5 rows):\")\n",
        "feature_cols = [\n",
        "    'mean_read_length', 'median_read_length', 'read_length_n50',\n",
        "    'mean_base_quality', 'estimated_error_rate', 'gc_content',\n",
        "    'read_type_encoded', 'num_reads',\n",
        "]\n",
        "df_kw[feature_cols].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92365cc3",
      "metadata": {
        "id": "92365cc3"
      },
      "source": [
        "### 4A. Train K-Weaver XGBoost Regressors\n",
        "Trains 4 models with **Optuna Bayesian hyperparameter search** (20 trials each) + 5-fold CV on the best config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68987c6",
      "metadata": {
        "id": "e68987c6"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ K-Weaver training with Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import optuna\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# 19 ReadFeature columns (input features)\n",
        "KWEAVER_FEATURES = [\n",
        "    'mean_read_length', 'median_read_length', 'read_length_n50',\n",
        "    'min_read_length', 'max_read_length', 'read_length_std',\n",
        "    'mean_base_quality', 'median_base_quality', 'estimated_error_rate',\n",
        "    'total_bases', 'num_reads',\n",
        "    'estimated_genome_size', 'estimated_coverage',\n",
        "    'gc_content', 'gc_std',\n",
        "    'read_type_encoded', 'is_paired_end',\n",
        "    'kmer_spectrum_peak', 'kmer_diversity',\n",
        "]\n",
        "\n",
        "# 4 target columns (one model each)\n",
        "KWEAVER_TARGETS = {\n",
        "    'dbg_model':       'best_dbg_k',\n",
        "    'ul_overlap_model': 'best_ul_k',\n",
        "    'extension_model':  'best_extension_k',\n",
        "    'polish_model':     'best_polish_k',\n",
        "}\n",
        "\n",
        "N_OPTUNA_TRIALS = 20\n",
        "N_CV_FOLDS = 5\n",
        "\n",
        "# XGBoost device config\n",
        "XGB_DEVICE = 'cuda' if GPU_AVAILABLE else 'cpu'\n",
        "XGB_TREE = 'hist'  # gpu_hist is deprecated in XGBoost 2.x â€” 'hist' auto-uses GPU\n",
        "\n",
        "\n",
        "def optuna_objective(trial, X, y):\n",
        "    \"\"\"Optuna objective for XGBoost regression HP search.\"\"\"\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
        "        'tree_method': XGB_TREE,\n",
        "        'device': XGB_DEVICE,\n",
        "        'random_state': SEED,\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "    scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "    return scores.mean()\n",
        "\n",
        "\n",
        "# â”€â”€ Prepare data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_kw = pd.read_csv(kweaver_csv)\n",
        "\n",
        "# Fill NaN in optional features (genome_size, coverage, kmer_spectrum, kmer_diversity)\n",
        "df_kw[KWEAVER_FEATURES] = df_kw[KWEAVER_FEATURES].fillna(0)\n",
        "\n",
        "X_all = df_kw[KWEAVER_FEATURES].values\n",
        "print(f\"Training data: {X_all.shape[0]} samples Ã— {X_all.shape[1]} features\\n\")\n",
        "\n",
        "# â”€â”€ Train each model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "kweaver_results = {}\n",
        "kweaver_save_dir = os.path.join(MODELS_OUT, 'kweaver')\n",
        "os.makedirs(kweaver_save_dir, exist_ok=True)\n",
        "\n",
        "for model_name, target_col in KWEAVER_TARGETS.items():\n",
        "    print(f\"{'â•'*60}\")\n",
        "    print(f\"  Training: {model_name} â†’ {target_col}\")\n",
        "    print(f\"{'â•'*60}\")\n",
        "\n",
        "    y = df_kw[target_col].values\n",
        "\n",
        "    # â”€â”€ Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    print(f\"  Running Optuna ({N_OPTUNA_TRIALS} trials)...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(lambda trial: optuna_objective(trial, X_all, y),\n",
        "                   n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_params['tree_method'] = XGB_TREE\n",
        "    best_params['device'] = XGB_DEVICE\n",
        "    best_params['random_state'] = SEED\n",
        "\n",
        "    print(f\"  Best MSE (neg): {study.best_value:.4f}\")\n",
        "    print(f\"  Best params: depth={best_params['max_depth']}, \"\n",
        "          f\"lr={best_params['learning_rate']:.4f}, \"\n",
        "          f\"n_est={best_params['n_estimators']}\")\n",
        "\n",
        "    # â”€â”€ 5-fold CV with best params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    print(f\"  Running {N_CV_FOLDS}-fold CV with best params...\")\n",
        "    model = xgb.XGBRegressor(**best_params)\n",
        "    kf = KFold(n_splits=N_CV_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "    cv_mse = []\n",
        "    cv_r2 = []\n",
        "    cv_mae = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_all)):\n",
        "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        mse = mean_squared_error(y_val, y_pred)\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "        mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "        cv_mse.append(mse)\n",
        "        cv_r2.append(r2)\n",
        "        cv_mae.append(mae)\n",
        "        print(f\"    Fold {fold+1}: MSE={mse:.2f}, MAE={mae:.2f}, RÂ²={r2:.4f}\")\n",
        "\n",
        "    print(f\"  CV Summary: MSE={np.mean(cv_mse):.2f}Â±{np.std(cv_mse):.2f}, \"\n",
        "          f\"RÂ²={np.mean(cv_r2):.4f}Â±{np.std(cv_r2):.4f}, \"\n",
        "          f\"MAE={np.mean(cv_mae):.2f}\")\n",
        "\n",
        "    # â”€â”€ Final model (train on all data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    final_model = xgb.XGBRegressor(**best_params)\n",
        "    final_model.fit(X_all, y)\n",
        "\n",
        "    # Save\n",
        "    pkl_path = os.path.join(kweaver_save_dir, f'{model_name}.pkl')\n",
        "    with open(pkl_path, 'wb') as f:\n",
        "        pickle.dump(final_model, f)\n",
        "    print(f\"  âœ“ Saved: {pkl_path}\")\n",
        "\n",
        "    kweaver_results[model_name] = {\n",
        "        'target': target_col,\n",
        "        'best_params': best_params,\n",
        "        'cv_mse_mean': float(np.mean(cv_mse)),\n",
        "        'cv_mse_std': float(np.std(cv_mse)),\n",
        "        'cv_r2_mean': float(np.mean(cv_r2)),\n",
        "        'cv_r2_std': float(np.std(cv_r2)),\n",
        "        'cv_mae_mean': float(np.mean(cv_mae)),\n",
        "        'cv_mae_std': float(np.std(cv_mae)),\n",
        "        'n_samples': len(y),\n",
        "    }\n",
        "    print()\n",
        "\n",
        "# Save results summary\n",
        "results_path = os.path.join(kweaver_save_dir, 'training_results.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(kweaver_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\n{'â•'*60}\")\n",
        "print(f\"  K-Weaver training complete!\")\n",
        "print(f\"  Models saved to: {kweaver_save_dir}\")\n",
        "print(f\"{'â•'*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bda2154e",
      "metadata": {
        "id": "bda2154e"
      },
      "source": [
        "### 5A. Validate K-Weaver Models\n",
        "Quick sanity check: load the saved models and run inference on a synthetic sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5525325b",
      "metadata": {
        "id": "5525325b"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Validate: load models and run sample predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"Loading trained K-Weaver models for validation...\\n\")\n",
        "\n",
        "# Load all 4 models\n",
        "kw_models = {}\n",
        "for model_name in KWEAVER_TARGETS:\n",
        "    pkl_path = os.path.join(kweaver_save_dir, f'{model_name}.pkl')\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        kw_models[model_name] = pickle.load(f)\n",
        "    print(f\"  âœ“ Loaded {model_name}\")\n",
        "\n",
        "# UL applicability threshold (must match kweaver_module.UL_MIN_N50)\n",
        "UL_MIN_N50 = 50_000\n",
        "\n",
        "# Test predictions on 3 representative samples from training data\n",
        "print(f\"\\n{'â”€'*70}\")\n",
        "print(f\"{'Technology':<12} {'DBG k':>8} {'UL k':>8} {'Ext k':>8} {'Polish k':>8}  UL status\")\n",
        "print(f\"{'â”€'*70}\")\n",
        "\n",
        "test_samples = df_kw.groupby('technology').first().reset_index()\n",
        "for _, row in test_samples.iterrows():\n",
        "    x = row[KWEAVER_FEATURES].values.reshape(1, -1).astype(np.float64)\n",
        "    preds = {name: int(round(model.predict(x)[0])) for name, model in kw_models.items()}\n",
        "    tech = row['technology']\n",
        "\n",
        "    # Determine UL applicability from read_length_n50 feature\n",
        "    n50 = row.get('read_length_n50', 0)\n",
        "    if n50 >= UL_MIN_N50:\n",
        "        ul_status = \"âœ“ applicable\"\n",
        "    else:\n",
        "        ul_status = f\"âš  advisory (N50={n50:,.0f} bp < 50 Kb)\"\n",
        "\n",
        "    print(f\"{tech:<12} {preds['dbg_model']:>8} {preds['ul_overlap_model']:>8} \"\n",
        "          f\"{preds['extension_model']:>8} {preds['polish_model']:>8}  {ul_status}\")\n",
        "\n",
        "print(f\"{'â”€'*70}\")\n",
        "print(f\"\\nâœ“ K-Weaver validation complete â€” predictions look reasonable!\")\n",
        "print(f\"\\n  â„¹ UL overlap k is only reliable for ultra-long reads (N50 â‰¥ 50 Kb).\")\n",
        "print(f\"    Reads with shorter N50 should not be fed into the UL overlap slot;\"  )\n",
        "print(f\"    doing so is likely to produce mis-joins and fragmented scaffolds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98bbdf33",
      "metadata": {
        "id": "98bbdf33"
      },
      "source": [
        "---\n",
        "# Track B: ErrorSmith Training ğŸ”¬\n",
        "\n",
        "**Goal:** Train 1 XGBoost multiclass classifier for per-base error type prediction.\n",
        "\n",
        "| Class | Label | Description |\n",
        "|-------|-------|-------------|\n",
        "| 0 | `correct` | Base matches reference |\n",
        "| 1 | `substitution` | Wrong base |\n",
        "| 2 | `insertion` | Extra base in read |\n",
        "| 3 | `deletion` | Missing base in read |\n",
        "| 4 | `homopolymer_error` | Ins/del in homopolymer context |\n",
        "\n",
        "**Data source:** Real CHM13 T2T alignments (public SRA data)\n",
        "\n",
        "**Two options:**\n",
        "- **Option 1 (recommended):** Upload pre-aligned BAMs to Google Drive\n",
        "- **Option 2:** Auto-download from SRA *(requires sra-toolkit, minimap2, samtools â€” ~50 GB disk, ~2 h)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ccd886b",
      "metadata": {
        "id": "8ccd886b"
      },
      "source": [
        "### 2B. Configuration & Data Source\n",
        "\n",
        "**Edit the cell below to match your setup.**\n",
        "\n",
        "If uploading BAMs to Google Drive, place them at:\n",
        "```\n",
        "My Drive/Colab Notebooks/strandweaver/chm13_hifi.sorted.bam    (+ .bai)\n",
        "My Drive/Colab Notebooks/strandweaver/chm13_ont.sorted.bam     (+ .bai)\n",
        "My Drive/Colab Notebooks/strandweaver/chm13_illumina.sorted.bam (+ .bai)\n",
        "```\n",
        "\n",
        "If using auto-download, the script will fetch reads from SRA and align them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e947cccc",
      "metadata": {
        "id": "e947cccc"
      },
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  ERRORSMITH CONFIGURATION â€” edit these before running\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# â”€â”€ Data source mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 'drive'    : Use BAMs uploaded to Google Drive (recommended)\n",
        "# 'download' : Auto-download from SRA + align (requires disk space + tools)\n",
        "ERRORSMITH_MODE = 'download'\n",
        "\n",
        "# â”€â”€ BAM paths (for 'drive' mode) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Set to None for any technology you don't have\n",
        "HIFI_BAM     = os.path.join(GDRIVE_DIR, 'chm13_hifi.sorted.bam')\n",
        "ONT_BAM      = os.path.join(GDRIVE_DIR, 'chm13_ont.sorted.bam')\n",
        "ILLUMINA_BAM = os.path.join(GDRIVE_DIR, 'chm13_illumina.sorted.bam')\n",
        "\n",
        "# â”€â”€ Reference genome â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Will auto-download CHM13v2.0 if not found\n",
        "REFERENCE_PATH = os.path.join(GDRIVE_DIR, 'chm13v2.0.fa')\n",
        "AUTO_DOWNLOAD_REF = True  # Download if not found\n",
        "\n",
        "# â”€â”€ Subsampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Bases per technology â€” higher = more training data but slower\n",
        "ERRORSMITH_SUBSAMPLE = 5_000_000  # 5M bases/tech â†’ ~300k error rows total\n",
        "\n",
        "# â”€â”€ Chromosomes to process â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Default: chr1-chr5 (good coverage, manageable runtime)\n",
        "ERRORSMITH_CHROMS = [f'chr{i}' for i in range(1, 6)]\n",
        "\n",
        "# â”€â”€ Threads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ERRORSMITH_THREADS = 4\n",
        "\n",
        "# â”€â”€ Validate BAMs exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if ERRORSMITH_MODE == 'drive':\n",
        "    bam_status = {}\n",
        "    for name, path in [('HiFi', HIFI_BAM), ('ONT', ONT_BAM), ('Illumina', ILLUMINA_BAM)]:\n",
        "        exists = os.path.exists(path) if path else False\n",
        "        bam_status[name] = exists\n",
        "        status = 'âœ“' if exists else 'âœ— not found'\n",
        "        print(f\"  {name}: {status}  ({path})\")\n",
        "\n",
        "    available_techs = sum(bam_status.values())\n",
        "    if available_techs == 0:\n",
        "        print(f\"\\nâš  No BAMs found! Either:\")\n",
        "        print(f\"  1. Upload BAMs to {GDRIVE_DIR}\")\n",
        "        print(f\"  2. Set ERRORSMITH_MODE = 'download'\")\n",
        "    else:\n",
        "        print(f\"\\nâœ“ {available_techs}/3 technologies available\")\n",
        "else:\n",
        "    print(f\"âœ“ Mode: auto-download from SRA\")\n",
        "    print(f\"  This will download ~50 GB of data and requires:\")\n",
        "    print(f\"  sra-toolkit, minimap2, samtools\")\n",
        "\n",
        "print(f\"\\n  Subsample:    {ERRORSMITH_SUBSAMPLE:,} bases/tech\")\n",
        "print(f\"  Chromosomes:  {ERRORSMITH_CHROMS}\")\n",
        "print(f\"  Reference:    {REFERENCE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142c56c9",
      "metadata": {
        "id": "142c56c9"
      },
      "source": [
        "### 3B. Download Reference + Install Tools (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80011db",
      "metadata": {
        "id": "a80011db"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Download CHM13v2.0 reference if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from generate_errorsmith_training_data import download_reference\n",
        "\n",
        "if not os.path.exists(REFERENCE_PATH) and AUTO_DOWNLOAD_REF:\n",
        "    print(\"Downloading CHM13v2.0 reference (800 MB)...\")\n",
        "    ref_dir = os.path.dirname(REFERENCE_PATH)\n",
        "    downloaded_ref = download_reference(Path(ref_dir))\n",
        "    REFERENCE_PATH = str(downloaded_ref)\n",
        "    print(f\"âœ“ Reference downloaded: {REFERENCE_PATH}\")\n",
        "elif os.path.exists(REFERENCE_PATH):\n",
        "    print(f\"âœ“ Reference found: {REFERENCE_PATH}\")\n",
        "else:\n",
        "    print(f\"âš  Reference not found at {REFERENCE_PATH}\")\n",
        "    print(f\"  Set AUTO_DOWNLOAD_REF = True to download automatically\")\n",
        "\n",
        "# â”€â”€ Install alignment tools for 'download' mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if ERRORSMITH_MODE == 'download':\n",
        "    print(\"\\nInstalling sra-toolkit, minimap2, samtools...\")\n",
        "    !apt-get install -qq -y sra-toolkit samtools\n",
        "    !pip install -q minimap2\n",
        "    # Verify\n",
        "    !which prefetch && echo \"âœ“ sra-toolkit\" || echo \"âœ— sra-toolkit missing\"\n",
        "    !which minimap2 && echo \"âœ“ minimap2\" || echo \"âœ— minimap2 missing\"\n",
        "    !which samtools && echo \"âœ“ samtools\" || echo \"âœ— samtools missing\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a539dfdb",
      "metadata": {
        "id": "a539dfdb"
      },
      "source": [
        "### 4B. Generate ErrorSmith Training Data\n",
        "Processes CHM13 BAMs, parses CIGAR alignments, extracts per-base error features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0368849",
      "metadata": {
        "id": "b0368849"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Generate ErrorSmith training CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from generate_errorsmith_training_data import generate_errorsmith_training_data\n",
        "\n",
        "# Build BAM args based on what's available\n",
        "es_kwargs = {\n",
        "    'output_dir': Path(ERRORSMITH_OUT),\n",
        "    'reference_path': REFERENCE_PATH,\n",
        "    'subsample': ERRORSMITH_SUBSAMPLE,\n",
        "    'chroms': ERRORSMITH_CHROMS,\n",
        "    'threads': ERRORSMITH_THREADS,\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "if ERRORSMITH_MODE == 'download':\n",
        "    es_kwargs['download'] = True\n",
        "else:\n",
        "    # Only pass BAMs that exist\n",
        "    if os.path.exists(HIFI_BAM):\n",
        "        es_kwargs['hifi_bam'] = HIFI_BAM\n",
        "    if os.path.exists(ONT_BAM):\n",
        "        es_kwargs['ont_bam'] = ONT_BAM\n",
        "    if os.path.exists(ILLUMINA_BAM):\n",
        "        es_kwargs['illumina_bam'] = ILLUMINA_BAM\n",
        "\n",
        "print(\"Starting ErrorSmith data generation...\")\n",
        "print(f\"  Mode: {ERRORSMITH_MODE}\")\n",
        "print(f\"  BAMs: {[k for k in ['hifi_bam','ont_bam','illumina_bam'] if k in es_kwargs]}\")\n",
        "print()\n",
        "\n",
        "t0 = time.time()\n",
        "errorsmith_csv = generate_errorsmith_training_data(**es_kwargs)\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "print(f\"\\nâœ… ErrorSmith training CSV: {errorsmith_csv}\")\n",
        "print(f\"   Elapsed: {elapsed/60:.1f} min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52b5e29",
      "metadata": {
        "id": "b52b5e29"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Inspect ErrorSmith data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_es = pd.read_csv(errorsmith_csv)\n",
        "print(f\"Shape: {df_es.shape}\")\n",
        "\n",
        "# Class distribution\n",
        "ERROR_TYPE_NAMES = {0: 'correct', 1: 'substitution', 2: 'insertion', 3: 'deletion', 4: 'homopolymer_error'}\n",
        "print(f\"\\nClass distribution:\")\n",
        "for label, count in df_es['error_type'].value_counts().sort_index().items():\n",
        "    pct = count / len(df_es) * 100\n",
        "    name = ERROR_TYPE_NAMES.get(label, f'unknown_{label}')\n",
        "    print(f\"  {label} ({name:20s}): {count:>10,} ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nTechnology breakdown:\")\n",
        "print(df_es['technology'].value_counts().to_string())\n",
        "\n",
        "print(f\"\\nFeature summary:\")\n",
        "feature_cols = ['base_quality', 'mean_quality_window_5', 'position_in_read',\n",
        "                'gc_content_local', 'homopolymer_length', 'read_length']\n",
        "df_es[feature_cols].describe().round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c65072c",
      "metadata": {
        "id": "7c65072c"
      },
      "source": [
        "### 5B. Train ErrorSmith Classifier\n",
        "XGBoost multiclass classifier with **Optuna HP search** + **hybrid resampling** (undersample majority `correct` class, oversample minority error types)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4362f5",
      "metadata": {
        "id": "7a4362f5"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ ErrorSmith training with Optuna + hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from collections import Counter\n",
        "\n",
        "# 16 error features\n",
        "ERRORSMITH_FEATURES = [\n",
        "    'base_quality', 'mean_quality_window_5', 'mean_quality_window_20',\n",
        "    'position_in_read', 'read_length',\n",
        "    'gc_content_local', 'gc_content_read',\n",
        "    'homopolymer_length', 'homopolymer_base', 'distance_to_hp',\n",
        "    'trinucleotide_context', 'pentanucleotide_context',\n",
        "    'technology_encoded',\n",
        "    'ref_gc_window_50', 'ref_repeat_flag', 'ref_homopolymer_length',\n",
        "]\n",
        "\n",
        "ES_LABEL = 'error_type'\n",
        "ES_N_OPTUNA_TRIALS = 25\n",
        "ES_N_CV_FOLDS = 5\n",
        "\n",
        "\n",
        "def hybrid_resample(X, y, max_majority=100_000, random_state=42):\n",
        "    \"\"\"\n",
        "    Hybrid resampling: undersample majority to max_majority,\n",
        "    oversample minorities to median class size.\n",
        "    \"\"\"\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    class_counts = dict(zip(classes, counts))\n",
        "\n",
        "    # Cap majority class\n",
        "    capped = {c: min(n, max_majority) for c, n in class_counts.items()}\n",
        "    median_size = int(np.median(list(capped.values())))\n",
        "\n",
        "    X_resampled, y_resampled = [], []\n",
        "    for cls in classes:\n",
        "        mask = y == cls\n",
        "        X_cls, y_cls = X[mask], y[mask]\n",
        "        target = max(median_size, min(len(X_cls), max_majority))\n",
        "\n",
        "        if len(X_cls) > target:\n",
        "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
        "                                  random_state=random_state, replace=False)\n",
        "        elif len(X_cls) < target:\n",
        "            X_rs, y_rs = resample(X_cls, y_cls, n_samples=target,\n",
        "                                  random_state=random_state, replace=True)\n",
        "        else:\n",
        "            X_rs, y_rs = X_cls, y_cls\n",
        "\n",
        "        X_resampled.append(X_rs)\n",
        "        y_resampled.append(y_rs)\n",
        "\n",
        "    return np.vstack(X_resampled), np.concatenate(y_resampled)\n",
        "\n",
        "\n",
        "def es_optuna_objective(trial, X, y):\n",
        "    \"\"\"Optuna objective for ErrorSmith multiclass classifier.\"\"\"\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 14),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
        "        'tree_method': XGB_TREE,\n",
        "        'device': XGB_DEVICE,\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 5,\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'random_state': SEED,\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "    scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')\n",
        "    return scores.mean()\n",
        "\n",
        "\n",
        "# â”€â”€ Prepare data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_es = pd.read_csv(errorsmith_csv)\n",
        "X_es = df_es[ERRORSMITH_FEATURES].fillna(0).values\n",
        "y_es = df_es[ES_LABEL].values\n",
        "\n",
        "print(f\"Raw data: {X_es.shape[0]:,} samples Ã— {X_es.shape[1]} features\")\n",
        "print(f\"Class distribution: {dict(Counter(y_es))}\")\n",
        "\n",
        "# â”€â”€ Hybrid resampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "X_balanced, y_balanced = hybrid_resample(X_es, y_es, max_majority=100_000)\n",
        "print(f\"\\nAfter hybrid resampling: {X_balanced.shape[0]:,} samples\")\n",
        "print(f\"Balanced distribution:  {dict(Counter(y_balanced))}\")\n",
        "\n",
        "# â”€â”€ Optuna HP search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(f\"\\nRunning Optuna ({ES_N_OPTUNA_TRIALS} trials)...\")\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(lambda trial: es_optuna_objective(trial, X_balanced, y_balanced),\n",
        "               n_trials=ES_N_OPTUNA_TRIALS, show_progress_bar=True)\n",
        "\n",
        "best_params = study.best_params\n",
        "best_params['tree_method'] = XGB_TREE\n",
        "best_params['device'] = XGB_DEVICE\n",
        "best_params['objective'] = 'multi:softprob'\n",
        "best_params['num_class'] = 5\n",
        "best_params['eval_metric'] = 'mlogloss'\n",
        "best_params['random_state'] = SEED\n",
        "\n",
        "print(f\"\\nBest F1-macro: {study.best_value:.4f}\")\n",
        "print(f\"Best params: depth={best_params['max_depth']}, \"\n",
        "      f\"lr={best_params['learning_rate']:.4f}, \"\n",
        "      f\"n_est={best_params['n_estimators']}\")\n",
        "\n",
        "# â”€â”€ 5-fold CV with best params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(f\"\\nRunning {ES_N_CV_FOLDS}-fold CV...\")\n",
        "skf = StratifiedKFold(n_splits=ES_N_CV_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "cv_acc = []\n",
        "cv_f1 = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_balanced, y_balanced)):\n",
        "    X_train, X_val = X_balanced[train_idx], X_balanced[val_idx]\n",
        "    y_train, y_val = y_balanced[train_idx], y_balanced[val_idx]\n",
        "\n",
        "    model = xgb.XGBClassifier(**best_params)\n",
        "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred, average='macro')\n",
        "    cv_acc.append(acc)\n",
        "    cv_f1.append(f1)\n",
        "    print(f\"  Fold {fold+1}: Accuracy={acc:.4f}, F1-macro={f1:.4f}\")\n",
        "\n",
        "print(f\"\\nCV Summary: Accuracy={np.mean(cv_acc):.4f}Â±{np.std(cv_acc):.4f}, \"\n",
        "      f\"F1-macro={np.mean(cv_f1):.4f}Â±{np.std(cv_f1):.4f}\")\n",
        "\n",
        "# â”€â”€ Final model (train on all balanced data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(f\"\\nTraining final model on all {len(y_balanced):,} samples...\")\n",
        "final_es_model = xgb.XGBClassifier(**best_params)\n",
        "final_es_model.fit(X_balanced, y_balanced)\n",
        "\n",
        "# Save\n",
        "errorsmith_save_dir = os.path.join(MODELS_OUT, 'errorsmith')\n",
        "os.makedirs(errorsmith_save_dir, exist_ok=True)\n",
        "\n",
        "pkl_path = os.path.join(errorsmith_save_dir, 'error_classifier.pkl')\n",
        "with open(pkl_path, 'wb') as f:\n",
        "    pickle.dump(final_es_model, f)\n",
        "\n",
        "print(f\"\\nâœ“ Saved: {pkl_path}\")\n",
        "\n",
        "# â”€â”€ Classification report on held-out fold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(f\"\\n{'â•'*60}\")\n",
        "print(\"Classification Report (last CV fold):\")\n",
        "print(f\"{'â•'*60}\")\n",
        "class_names = ['correct', 'substitution', 'insertion', 'deletion', 'HP_error']\n",
        "print(classification_report(y_val, y_pred, target_names=class_names, digits=3))\n",
        "\n",
        "# Save results\n",
        "errorsmith_results = {\n",
        "    'best_params': best_params,\n",
        "    'cv_accuracy_mean': float(np.mean(cv_acc)),\n",
        "    'cv_accuracy_std': float(np.std(cv_acc)),\n",
        "    'cv_f1_macro_mean': float(np.mean(cv_f1)),\n",
        "    'cv_f1_macro_std': float(np.std(cv_f1)),\n",
        "    'n_samples_raw': int(len(y_es)),\n",
        "    'n_samples_balanced': int(len(y_balanced)),\n",
        "    'class_distribution': dict(Counter(int(c) for c in y_balanced)),\n",
        "}\n",
        "with open(os.path.join(errorsmith_save_dir, 'training_results.json'), 'w') as f:\n",
        "    json.dump(errorsmith_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\n{'â•'*60}\")\n",
        "print(f\"  ErrorSmith training complete!\")\n",
        "print(f\"  Model saved to: {pkl_path}\")\n",
        "print(f\"{'â•'*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2db1b16",
      "metadata": {
        "id": "e2db1b16"
      },
      "source": [
        "---\n",
        "## 6. Feature Importance Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a0e8c1",
      "metadata": {
        "id": "a2a0e8c1"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Feature importance plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "fig.suptitle('Feature Importance â€” K-Weaver + ErrorSmith', fontsize=16, fontweight='bold')\n",
        "\n",
        "# K-Weaver models (top row + bottom-left)\n",
        "for idx, (model_name, target_col) in enumerate(KWEAVER_TARGETS.items()):\n",
        "    ax = axes[idx // 3][idx % 3]\n",
        "    model = kw_models[model_name]\n",
        "    importances = model.feature_importances_\n",
        "    sorted_idx = np.argsort(importances)[-10:]  # Top 10\n",
        "    ax.barh(range(10), importances[sorted_idx], color='steelblue')\n",
        "    ax.set_yticks(range(10))\n",
        "    ax.set_yticklabels([KWEAVER_FEATURES[i] for i in sorted_idx], fontsize=9)\n",
        "    ax.set_title(f'K-Weaver: {model_name}', fontsize=11)\n",
        "    ax.set_xlabel('Importance')\n",
        "\n",
        "# ErrorSmith (bottom-right)\n",
        "ax = axes[1][2]\n",
        "es_importances = final_es_model.feature_importances_\n",
        "sorted_idx = np.argsort(es_importances)[-10:]\n",
        "ax.barh(range(10), es_importances[sorted_idx], color='coral')\n",
        "ax.set_yticks(range(10))\n",
        "ax.set_yticklabels([ERRORSMITH_FEATURES[i] for i in sorted_idx], fontsize=9)\n",
        "ax.set_title('ErrorSmith: error_classifier', fontsize=11)\n",
        "ax.set_xlabel('Importance')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig(os.path.join(MODELS_OUT, 'feature_importance.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ Feature importance plot saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "664ad926",
      "metadata": {
        "id": "664ad926"
      },
      "source": [
        "## 7. Export Models to Google Drive\n",
        "Package all trained models into a tarball and copy to Google Drive for download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8bd0e8",
      "metadata": {
        "id": "cc8bd0e8"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Package and export to Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import tarfile, shutil, glob\n",
        "\n",
        "GDRIVE_TARBALL = os.path.join(GDRIVE_DIR, 'kweaver_errorsmith_models.tar.gz')\n",
        "\n",
        "# List all model files\n",
        "model_files = glob.glob(f'{MODELS_OUT}/**/*', recursive=True)\n",
        "print(f\"Model files to export ({len(model_files)}):\")\n",
        "for f in sorted(model_files):\n",
        "    if os.path.isfile(f):\n",
        "        size_kb = os.path.getsize(f) / 1024\n",
        "        print(f\"  {os.path.relpath(f, MODELS_OUT):40s} {size_kb:>8.1f} KB\")\n",
        "\n",
        "# Create tarball\n",
        "print(f\"\\nCreating tarball...\")\n",
        "with tarfile.open(GDRIVE_TARBALL, 'w:gz') as tar:\n",
        "    tar.add(MODELS_OUT, arcname='trained_models')\n",
        "\n",
        "tarball_size = os.path.getsize(GDRIVE_TARBALL) / (1024 * 1024)\n",
        "print(f\"\\nâœ… Exported to Google Drive:\")\n",
        "print(f\"   {GDRIVE_TARBALL}\")\n",
        "print(f\"   Size: {tarball_size:.1f} MB\")\n",
        "\n",
        "# Also copy individual model dirs to Drive for easy access\n",
        "for subdir in ['kweaver', 'errorsmith']:\n",
        "    src = os.path.join(MODELS_OUT, subdir)\n",
        "    dst = os.path.join(GDRIVE_DIR, f'{subdir}_models')\n",
        "    if os.path.isdir(src):\n",
        "        if os.path.exists(dst):\n",
        "            shutil.rmtree(dst)\n",
        "        shutil.copytree(src, dst)\n",
        "        print(f\"   Copied {subdir}/ â†’ {dst}\")\n",
        "\n",
        "print(f\"\\nğŸ“¥ To download locally:\")\n",
        "print(f\"   1. Open Google Drive â†’ My Drive/Colab Notebooks/strandweaver/\")\n",
        "print(f\"   2. Download kweaver_errorsmith_models.tar.gz\")\n",
        "print(f\"   3. Extract to strandweaver/trained_models/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96906845",
      "metadata": {
        "id": "96906845"
      },
      "source": [
        "## 8. Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0263d8",
      "metadata": {
        "id": "8d0263d8"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
        "print(\"â•‘          StrandWeaver Model Training â€” Summary              â•‘\")\n",
        "print(\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
        "print(\"â•‘                                                              â•‘\")\n",
        "print(\"â•‘  TRACK A: K-Weaver (4 regressors)                           â•‘\")\n",
        "print(\"â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â•‘\")\n",
        "for name, res in kweaver_results.items():\n",
        "    print(f\"â•‘    {name:20s}  RÂ²={res['cv_r2_mean']:.3f}Â±{res['cv_r2_std']:.3f}  \"\n",
        "          f\"MAE={res['cv_mae_mean']:.1f}  â•‘\")\n",
        "print(\"â•‘                                                              â•‘\")\n",
        "print(\"â•‘  TRACK B: ErrorSmith (1 classifier)                         â•‘\")\n",
        "print(\"â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â•‘\")\n",
        "print(f\"â•‘    error_classifier       F1={errorsmith_results['cv_f1_macro_mean']:.3f}Â±\"\n",
        "      f\"{errorsmith_results['cv_f1_macro_std']:.3f}  \"\n",
        "      f\"Acc={errorsmith_results['cv_accuracy_mean']:.3f}  â•‘\")\n",
        "print(\"â•‘                                                              â•‘\")\n",
        "print(\"â•‘  Output files on Google Drive:                               â•‘\")\n",
        "print(f\"â•‘    {GDRIVE_DIR}/\")\n",
        "print(f\"â•‘    â”œâ”€â”€ kweaver_models/       (4 .pkl files)\")\n",
        "print(f\"â•‘    â”œâ”€â”€ errorsmith_models/    (1 .pkl file)\")\n",
        "print(f\"â•‘    â””â”€â”€ kweaver_errorsmith_models.tar.gz\")\n",
        "print(\"â•‘                                                              â•‘\")\n",
        "print(\"â•‘  Next steps:                                                 â•‘\")\n",
        "print(\"â•‘    1. Download models to local strandweaver/trained_models/  â•‘\")\n",
        "print(\"â•‘    2. Run: strandweaver core-assemble --use-ai <reads>       â•‘\")\n",
        "print(\"â•‘    3. All 7/7 AI modules now have trained weights! ğŸ‰        â•‘\")\n",
        "print(\"â•‘                                                              â•‘\")\n",
        "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

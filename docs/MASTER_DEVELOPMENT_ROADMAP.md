# StrandWeaver Master Development Roadmap

**Last Updated**: December 7, 2025  
**Status**: Phase 5 In Progress - Advanced AI Intelligence & ML Model Training

---

## üéØ Quick Status

| Phase | Status | Completion | Notes |
|-------|--------|------------|-------|
| Phase 1 | ‚úÖ COMPLETE | 100% | Core infrastructure, I/O, alignment |
| Phase 2 | ‚úÖ COMPLETE | 100% | Error correction (Illumina, ONT, aDNA) |
| Phase 3 | ‚úÖ COMPLETE | 100% | Assembly core, GraphAligner, Hi-C, Spectral clustering |
| Phase 4 | ‚úÖ **COMPLETE** | 100% | **Performance optimization, GPU acceleration (CUDA/MPS), Hi-C+AI integration** |
| **Phase 5** | üîÑ **IN PROGRESS** | 70% | **AI-driven assembly intelligence + BandageNG + ML training** (GNN ‚úÖ, diploid ‚úÖ, UL ‚úÖ, SV ‚úÖ, manual curation ‚úÖ, training infrastructure ‚úÖ, **training data generation üîÑ IN PROGRESS**, model implementation pending) |
| Phase 6 | üìã PLANNED | 0% | Production deployment, benchmarking |

---

## üìä Current Achievements

### ‚úÖ Completed (Phase 3 - Milestone 2)
- **Illumina Contig Builder**: OLC assembly for short reads ‚Üí artificial long reads
- **de Bruijn Graph Construction**: K-mer based graph assembly
- **Long Read Overlay**: ONT ultralong read mapping to graph
- **GraphAligner Integration**: External aligner support with anchor-guided mode
- **Hi-C Contact Matrix**: Proximity ligation data processing
- **Spectral Clustering**: Graph partitioning for haplotype phasing
- **Comprehensive Testing**: 42/42 unit tests passing

### ‚úÖ Completed (Phase 4.1-4.3)
- **Multi-Threading**: Parallel overlap detection (6-7√ó speedup)
- **GPU Acceleration**: 5 GPU operations implemented (10-40√ó each)
- **Unified GPU Backend**: Multi-platform support (CUDA/MPS/CPU) with HPC-safe explicit control ‚úÖ COMPLETE
- **Module Reorganization**: Descriptive naming, consolidated structure (v4.0) ‚úÖ COMPLETE
- **Assembly Solidification**: Orientation tracking, enhanced data structures
- **Test Infrastructure**: Synthetic data generation, proper workflow validation

### ‚úÖ Completed (Phase 5.0-5.2 - Advanced AI Modules + BandageNG)
- **Hi-C Integration Module**: Phasing and edge support (GPU-accelerated 20-50√ó speedup) ‚úÖ COMPLETE
- **AI Overlap Filtering**: ML-based edge classification ‚úÖ COMPLETE
- **Graph Cleanup Engine**: Multi-signal refinement ‚úÖ COMPLETE
- **GNN Path Predictor**: Graph neural network for path prediction ‚úÖ COMPLETE
- **Diploid Disentangler**: Haplotype separation using multiple signals ‚úÖ COMPLETE
- **UL Routing AI**: Resolve ambiguous ultralong read alignments ‚úÖ COMPLETE
- **SV Detection AI**: Identify structural variants during assembly ‚úÖ COMPLETE
- **BandageNG Integration**: Manual graph editing and correction import ‚úÖ COMPLETE
- **Pipeline Integration**: All modules integrated ‚úÖ COMPLETE

### ‚úÖ Completed (Phase 5.3 - ML Training Infrastructure)
- **Genome Simulator**: Generate synthetic diploid genomes with SVs ‚úÖ COMPLETE
- **Read Simulator**: Simulate Illumina, HiFi, ONT, UL, Hi-C reads ‚úÖ COMPLETE
- **Ground-Truth Labeler**: Extract labels from assembled graphs ‚úÖ COMPLETE
- **Feature Builder**: Convert graphs to ML-ready tensors ‚úÖ COMPLETE
- **Dataset Writer**: Package training data with sharding ‚úÖ COMPLETE
- **ML Interfaces**: Define model contracts and training utilities ‚úÖ COMPLETE
- **Corpus Orchestrator**: End-to-end pipeline automation ‚úÖ COMPLETE

### üîÑ In Progress (Phase 5.3.5 - Training Data Generation)
- **Assembly AI Training**: Repeat-heavy scenario running (50 genomes √ó 2Mb) üîÑ IN PROGRESS
- **Read Correction Training**: Multi-technology data generation (516k/540k k-mer, 240k/360k base) üîÑ 95% COMPLETE
- **GPU Acceleration**: MPS-accelerated training on Apple Silicon ‚úÖ ACTIVE
- **Completed Datasets**: 
  - Fast-balanced scenario (20 genomes, 15GB) ‚úÖ COMPLETE (Dec 7, 2025)
  - Training data: 3,224 files across 5 AI modules ‚úÖ READY FOR MODEL TRAINING

---

## üöÄ Phase 4: Performance Optimization & Scalability

### Current Focus (December 2025)

#### 4.1 Multi-Threading Implementation ‚úÖ DONE
**Completed**: December 2, 2025

**Achievements**:
- ‚úÖ Parallel overlap detection in ContigBuilder
- ‚úÖ ThreadPoolExecutor with auto CPU detection
- ‚úÖ Smart chunking strategy for load balancing
- ‚úÖ Fallback to single-threaded mode
- ‚úÖ Expected speedup: 6-7√ó for 400k reads

**Integration Points**:
- `ContigBuilder._detect_overlaps_parallel()`
- `ContigBuilder._process_read_chunk()`
- Auto-enabled with `num_threads=None` (auto-detect)

**Documentation**: `docs/development/PARALLELIZATION_IMPROVEMENTS.md`

---

#### 4.2 GPU Acceleration ‚úÖ COMPLETE
**Completion Date**: December 2025 (Phase 4.2)

**Status**: ‚úÖ **ALL GPU OPERATIONS IMPLEMENTED**

**Implemented GPU Operations** (10-40√ó speedup each):

1. **GPU Sequence Alignment** (ContigBuilder) ‚úÖ
   - File: `strandweaver/assembly/gpu_assembly.py` - `GPUSequenceAligner`
   - Integration: `strandweaver/assembly/contig_builder.py`
   - Method: Batch pairwise alignment using CuPy
   - Target: `_verify_overlap()` ‚Üí `_verify_overlaps_batch_gpu()`
   - Expected: 10-20√ó speedup
   - Status: ‚úÖ **IMPLEMENTED & TESTED**

2. **GPU K-mer Operations** (DeBruijnGraphBuilder) ‚úÖ
   - File: `strandweaver/assembly/gpu_assembly.py` - `GPUKmerExtractor`
   - Method: GPU-accelerated k-mer extraction and hashing
   - Target: K-mer extraction, hashing, counting
   - Expected: 15-30√ó speedup
   - Status: ‚úÖ **IMPLEMENTED & TESTED**

3. **GPU Hi-C Contact Matrix** (HiCIntegrator) ‚úÖ
   - File: `strandweaver/assembly/gpu_assembly.py` - `GPUHiCMatrix`
   - Integration: `strandweaver/assembly/assembly_core.py` - `HiCIntegrator`
   - Method: Parallel atomic operations using CuPy
   - Target: `_build_contact_matrix()` ‚Üí `build_contact_matrix()`
   - Expected: 20-40√ó speedup
   - Status: ‚úÖ **IMPLEMENTED & TESTED**

4. **GPU Spectral Clustering** (HiCIntegrator) ‚úÖ
   - File: `strandweaver/assembly/gpu_assembly.py` - `GPUSpectralClustering`
   - Integration: `strandweaver/assembly/assembly_core.py` - `HiCIntegrator`
   - Method: cuSolver eigendecomposition via CuPy
   - Target: `_spectral_clustering()` ‚Üí `spectral_cluster()`
   - Expected: 15-35√ó speedup
   - Status: ‚úÖ **IMPLEMENTED & TESTED**

5. **GPU Assembly Manager** ‚úÖ
   - File: `strandweaver/assembly/gpu_assembly.py` - `GPUAssemblyManager`
   - Purpose: Centralized GPU management
   - Features: Unified GPU detection, component initialization, status reporting
   - Status: ‚úÖ **IMPLEMENTED & TESTED**

**Implementation Details**:
- **Framework**: CuPy (CUDA wrapper for NumPy)
- **Pattern**: Consistent GPU detection with automatic CPU fallback
- **Components**: 5 GPU-accelerated operations + manager
- **Tests**: 22 GPU-specific tests (16 passed, 3 skipped for no GPU, 3 N/A)
- **Total Tests**: 164 tests passing (42 existing + 122 new tests)

**Documentation**: 
- Implementation: `docs/development/GPU_ACCELERATION_COMPLETE.md`
- Original Plan: `docs/development/GPU_ACCELERATION_PLAN.md`

**Expected Performance Gains**:
| Dataset | Current | After GPU+Threading | Total Speedup |
|---------|---------|---------------------|---------------|
| Small (100k) | 6 min | 30-35 sec | **10-12√ó** |
| Medium (1M) | 1.9 hrs | 6-7 min | **16-19√ó** |
| Large (10M) | 19.7 hrs | 50-60 min | **20-24√ó** |

**Combined Performance Impact**:
- Multi-threading (Phase 4.1): 6-7√ó speedup ‚úÖ
- GPU Acceleration (Phase 4.2): 10-50√ó per operation ‚úÖ
- **Total Expected: 10-22√ó overall speedup** ‚úÖ

---

**Priority 2 - Additional Threading** (Est. 3-8√ó speedup):
**Status**: üìã DEFERRED TO FUTURE OPTIMIZATION

6. **Parallel GraphAligner Calls** (LongReadOverlay)
   - Multi-threaded external aligner execution
   - Expected: 4-8√ó speedup
   - Status: üìã Planned for Phase 4.4+

7. **Parallel Graph Simplification** (AssemblyGraphSimplifier)
   - Partition graph into independent components
   - Expected: 3-5√ó speedup
   - Status: üìã Planned for Phase 4.4+

---

#### 4.3 Assembly Solidification ‚úÖ COMPLETE
**Completion Date**: December 2, 2025

**Status**: ‚úÖ **CRITICAL ALIGNMENT IMPROVEMENTS IMPLEMENTED**

**Implemented Enhancements**:

1. **Orientation Tracking** ‚úÖ
   - Added `Anchor` dataclass with orientation field ('+'/'-')
   - Added `orientations` field to `ULReadMapping`
   - Updated `_parse_gaf_path()` to extract orientations from GAF format
   - Updated `_anchors_to_path()` to return both nodes and orientations
   - Impact: Enables detection of inversions, better structural variant calling

2. **Enhanced Data Structures** ‚úÖ
   - Created `Anchor` dataclass: read_start, read_end, node_id, node_start, orientation
   - Created `GapAlignment` dataclass: gap alignment details with matches, identity, CIGAR
   - Updated `ULReadMapping`: now includes anchors, gaps, and orientations
   - Impact: Full alignment provenance tracking

3. **Improved Identity Calculation** ‚úÖ
   - Calculate true identity from anchor coverage
   - Anchors counted as 100% identity (exact matches)
   - Gaps conservatively estimated (ready for CIGAR-based calculation)
   - Impact: Accurate quality metrics for filtering

4. **GraphAligner Integration** ‚úÖ
   - Updated `_align_with_mbg()` to use new Anchor objects
   - Parse orientations from GAF output
   - Include anchor information in final mapping
   - Impact: Complete alignment provenance from anchors + GraphAligner

**Files Modified**:
- `strandweaver/assembly/assembly_core.py`:
  * Added `Anchor` and `GapAlignment` dataclasses (~40 lines)
  * Updated `ULReadMapping` with orientations and alignment components (~20 lines)
  * Updated `_find_exact_anchors()` to return Anchor objects (~10 lines)
  * Updated `_anchors_to_path()` to return orientations (~5 lines)
  * Updated `_parse_gaf_path()` to parse orientations (~15 lines)
  * Updated `_align_with_mbg()` to use new structures (~10 lines)
  * Updated `map_read_to_graph()` to calculate true identity (~10 lines)

- `tests/test_assembly_core.py`:
  * Updated all UL overlay tests for new data structures
  * Fixed anchor unpacking tests
  * Fixed GAF path parsing tests
  * All 42 tests passing ‚úÖ

**Technical Achievements**:
- ‚úÖ Full orientation tracking for forward/reverse strand
- ‚úÖ Proper Anchor objects with all alignment details
- ‚úÖ True identity calculation (not just coverage estimate)
- ‚úÖ Complete alignment provenance (anchors + gaps)
- ‚úÖ Ready for gap-filling with GraphAligner

**Not Implemented (Deferred)**:
- Anchor-guided alignment (using anchors to constrain GraphAligner search)
- Batch GraphAligner processing
- CIGAR-based identity calculation from gap alignments

**Rationale for Deferral**:
These improvements set up the data structures correctly for the string graph overlay.
Anchor-guided alignment and batching are optimizations that can be added later
when working with larger datasets. The current implementation provides correct
orientation tracking and identity calculation, which are critical for correctness.

**Documentation**: `docs/development/ASSEMBLY_SOLIDIFICATION_PLAN.md`

---

#### 4.4 Test Data Infrastructure ‚úÖ MOSTLY DONE
**Status**: Synthetic data working, proper workflow validated

**Achievements**:
- ‚úÖ Comprehensive test data requirements documented
- ‚úÖ Synthetic yeast genome generator (12.5 Mbp, 16 chromosomes)
- ‚úÖ Illumina PE simulator (sampling from reference genome)
- ‚úÖ ONT ultralong simulator (N50 95kb, 10% error)
- ‚úÖ Hi-C proximity ligation simulator
- ‚úÖ Proper 3-phase workflow test created
- ‚úÖ Lessons learned documented

**Issues Discovered & Fixed**:
- ‚úÖ Wrong workflow tested initially (bypassed ContigBuilder)
- ‚úÖ Synthetic data bug (random sequences ‚Üí now samples from genome)
- ‚úÖ Insufficient coverage (1.2√ó ‚Üí now 5√ó+)

**Remaining Work**:
- üîÑ Complete 5√ó coverage test (currently running)
- üìã Test with 10-20√ó coverage for better validation
- üìã Add Hi-C integration test (Phase 4)
- üìã Benchmark parallelization vs single-threaded

**Documentation**:
- `docs/testing/TEST_DATA_REQUIREMENTS.md`
- `docs/testing/YEAST_TEST_QUICKSTART.md`
- `docs/testing/YEAST_TEST_LESSONS_LEARNED.md`
- `docs/testing/YEAST_DOWNLOAD_GUIDE.md`

---

## üöÄ Phase 5: AI-Driven Assembly Intelligence

### Current Focus (December 2025)

#### 5.0 Hi-C Integration + AI Filtering + Graph Cleanup ‚úÖ COMPLETE
**Completion Date**: December 4, 2025

**Status**: ‚úÖ **FOUNDATION MODULES IMPLEMENTED**

**Implemented Components**:

1. **Hi-C Integration Module** (`hic_integration.py` - 455 lines) ‚úÖ
   - Sparse node-node contact map construction
   - Label propagation phasing algorithm (2-way clustering into haplotype A/B)
   - Cis/trans contact analysis for edge support
   - Platform-agnostic (Illumina, Dovetail, Arima, Omni-C, etc.)
   - **Data Structures**:
     * `HiCFragment`: Read fragment mapped to graph node
     * `HiCPair`: Paired fragments representing chromatin contact
     * `HiCNodePhaseInfo`: Haplotype A/B scores and assignment
     * `HiCEdgeSupport`: Cis/trans contacts and normalized weight
   - **Main API**: `compute_hic_phase_and_support(hic_pairs, nodes, edges) -> (phase_info, edge_support)`

2. **AI-Based Overlap Filtering** (`overlap_ai_filter.py` - 486 lines) ‚úÖ
   - Comprehensive edge feature extraction (13+ features)
   - Pluggable ML model interface with fallback heuristics
   - Multi-class edge classification (true/repeat/chimeric/allelic)
   - **Feature Categories**:
     * Coverage patterns (ratio, difference, consistency)
     * Branching complexity (in/out degrees)
     * Regional complexity (ML k-mer recommendations from Phase 5.1c)
     * Support signals (UL reads, Hi-C weights)
     * Sequence properties (entropy, repeat likelihood)
   - **Data Structures**:
     * `EdgeFeatures`: Comprehensive feature vector
     * `EdgeAIAnnotation`: Multi-class scores and confidence
   - **Main API**: `annotate_edges_with_ai(graph, edges, ml_model, hic_support, ...) -> annotations`

3. **Graph Cleanup Engine** (`graph_cleanup.py` - 533 lines) ‚úÖ
   - Multi-signal edge pruning (AI + Hi-C + UL)
   - Bubble detection and resolution
   - Haplotype-aware graph partitioning
   - Iterative phase propagation through high-confidence edges
   - **Operations**:
     * Edge pruning: Remove low-confidence edges (AI <0.3 AND Hi-C <0.2)
     * Bubble resolution: Keep both if allelic, remove weaker if error
     * Haplotype assignment: Propagate phases through trusted edges
   - **Data Structures**:
     * `CleanedGraphResult`: Cleaned graph + haplotype assignments + stats
     * `Bubble`: Parallel path structure with support scores
   - **Main API**: `clean_graph(graph, hic_phase, hic_support, ai_annotations, ...) -> cleaned_result`

4. **Synthetic Example** (`examples/synthetic_hic_ai_cleanup.py` - 309 lines) ‚úÖ
   - Complete end-to-end demonstration
   - 7-node graph with bubble structure
   - 106 synthetic Hi-C contacts
   - Mock ML model for edge scoring
   - Successfully demonstrates full pipeline integration

**Integration Points**:

```python
# Existing: DBG + String Graph construction
result = run_assembly_pipeline(read_type='hifi', ...)

# New: Hi-C integration
hic_phase_info, hic_edge_support = compute_hic_phase_and_support(...)

# New: AI edge filtering
ai_annotations = annotate_edges_with_ai(graph, ml_model, ...)

# New: Graph cleanup
cleaned = clean_graph(graph, hic_phase_info, ai_annotations, ...)

# Existing: Hi-C scaffolding
scaffolds = hic_scaffolder.scaffold(cleaned.cleaned_graph, ...)
```

**Design Principles**:
- ‚úÖ Clean separation of concerns (each module has single responsibility)
- ‚úÖ Pluggable ML models (easy to swap in different models)
- ‚úÖ Multi-signal fusion (weighted combination of orthogonal signals)
- ‚úÖ Graceful degradation (works with partial data)
- ‚úÖ Full type safety (comprehensive type hints throughout)
- ‚úÖ Production-ready logging and error handling

**Documentation**: `docs/HIC_AI_CLEANUP.md`

**Next Steps**:
- Train actual ML models (currently using heuristic fallbacks)
- Spectral clustering enhancement (better than label propagation)
- Integrate with assembly orchestrator
- Benchmark on real Hi-C datasets
- Scale testing with larger graphs

---

#### 5.1 Advanced AI Modules - Deep Learning Brain ‚úÖ COMPLETE
**Completion Date**: December 4, 2025

**Status**: ‚úÖ **ALL FOUR ADVANCED AI MODULES IMPLEMENTED**

**Implemented Components**:

1. **GNN Path Predictor** (`gnn_path_predictor.py` - 600+ lines) ‚úÖ
   - Graph Neural Network for path prediction through ambiguous regions
   - **Node Features** (12 dimensions): Coverage, length, entropy, branching, Hi-C phase, recommended_k, UL support, repeat likelihood
   - **Edge Features** (10 dimensions): AI scores, Hi-C weight, UL support, k-consistency, coverage consistency
   - **Architecture**: Graph tensors ‚Üí GNN prediction ‚Üí Path extraction
   - **Fallback**: Heuristic scoring when no trained model (AI 40% + Hi-C 30% + UL 20% + coverage 10%)
   - **Main API**: `predict_paths_with_gnn(graph, gnn_model, ...) -> GNNPathResult`
   - **Data Structures**:
     * `GraphTensors`: Tensor representation for GNN input
     * `GNNPathResult`: Best paths + edge confidences + ambiguous regions

2. **Diploid Disentangler** (`diploid_disentangler.py` - 550+ lines) ‚úÖ
   - Separates diploid graphs into haplotype A and B using multiple signals
   - **Scoring**: Hi-C phasing (50%) + GNN paths (30%) + Repeat detection (20%)
   - **Algorithm**:
     1. Score nodes for haplotype A/B/repeat
     2. Initial assignment based on confidence thresholds
     3. Propagate through high-confidence edges (AI >0.7 OR Hi-C >0.7)
     4. Assign edges based on endpoint haplotypes
     5. Identify contiguous haplotype blocks
   - **Main API**: `disentangle_diploid_graph(graph, gnn_paths, hic_phase, ...) -> DiploidDisentangleResult`
   - **Data Structures**:
     * `DiploidDisentangleResult`: Haplotype A/B nodes/edges, repeats, confidence scores, blocks

3. **UL Routing AI** (`ul_routing_ai.py` - 650+ lines) ‚úÖ
   - Resolves ambiguous ultralong read alignments across repeats
   - **Path Features** (12 dimensions): K-mer agreement, GNN confidence, Hi-C consistency, repeat score, entropy/coverage consistency, UL support, gaps, strand, branching
   - **Heuristic Scoring**: 
     * K-mer agreement: 25%
     * GNN edge confidence: 20%
     * Hi-C phase consistency: 15%
     * Coverage + entropy consistency: 20%
     * Strand consistency: 10%
     * UL support: 5%
     * Penalties (gaps, repeats): -10%
   - **Confidence**: Gap between best and second-best path scores
   - **Main API**: `resolve_ul_routes(ul_alignments, graph, ...) -> Dict[read_id, ULRouteDecision]`
   - **Data Structures**:
     * `ULAnchor`: Single alignment anchor point
     * `ULPath`: Candidate path with anchors and gaps
     * `PathFeatures`: Comprehensive feature vector
     * `ULRouteDecision`: Chosen path + alternatives + confidence

4. **SV Detection AI** (`sv_detection_ai.py` - 750+ lines) ‚úÖ
   - Identifies structural variants during assembly (not post-assembly)
   - **SV Types Supported**:
     * Deletions: UL spanning gaps, Hi-C contacts across gaps, coverage drops
     * Insertions: Alternative paths (>1kb diff), high-coverage branches
     * Inversions: UL strand flips, Hi-C orientation signals
     * Duplications: High coverage (>80) + repeat signals
     * Translocations: Hi-C long-range, UL connecting distant regions
   - **Detection Algorithms**:
     * Deletion: UL connects nodes without edge, coverage <50% neighbors
     * Insertion: Parallel paths with size difference, coverage >60
     * Duplication: Coverage >80 + AI repeat score >0.5
     * Translocation: UL node ID gap >1000
   - **Main API**: `detect_structural_variants(graph, gnn_paths, diploid, ul_routes, ...) -> List[SVCall]`
   - **Data Structures**:
     * `SVCall`: SV type, nodes, confidence, evidence, size, haplotype, breakpoints

**Integration Pipeline**:

```python
# 1. Base assembly (Existing)
result = run_assembly_pipeline(read_type='hifi', ...)

# 2. Hi-C + AI + Cleanup (Phase 5.0)
hic_phase, hic_support = compute_hic_phase_and_support(...)
ai_annotations = annotate_edges_with_ai(graph, ml_model, ...)
cleaned = clean_graph(graph, hic_phase, ai_annot, ...)

# 3. GNN Path Prediction (NEW - Phase 5.1)
gnn_paths = predict_paths_with_gnn(
    cleaned.cleaned_graph, gnn_model,
    hic_phase, ai_annotations, hic_support
)

# 4. Diploid Disentanglement (NEW - Phase 5.1)
diploid = disentangle_diploid_graph(
    cleaned.cleaned_graph, gnn_paths,
    hic_phase, ai_annotations
)

# 5. UL Routing (NEW - Phase 5.1)
ul_routes = resolve_ul_routes(
    ul_alignments, cleaned.cleaned_graph,
    gnn_paths.edge_confidences, hic_phase
)

# 6. SV Detection (NEW - Phase 5.1)
svs = detect_structural_variants(
    cleaned.cleaned_graph, gnn_paths, diploid,
    ul_routes, hic_support, ai_annotations
)

# 7. Hi-C Scaffolding (Existing)
scaffolds = hic_scaffolder.scaffold(
    cleaned.cleaned_graph,
    diploid_result=diploid,
    ul_constraints=ul_routes,
    sv_calls=svs
)
```

**Synthetic Example** (`examples/synthetic_advanced_ai.py` - 400+ lines) ‚úÖ
- Complete end-to-end demonstration
- 10-node diploid graph with haplotype A/B paths
- Mock Hi-C phasing (nodes 2-4 = A, nodes 5-8 = B)
- 2 UL reads spanning different haplotypes
- 1.6kb heterozygous insertion in haplotype B
- Successfully demonstrates all four modules working together

**Design Principles**:
- ‚úÖ **Modularity**: Each module has clean API and single responsibility
- ‚úÖ **Pluggable ML**: Easy to swap heuristics ‚Üí trained models
- ‚úÖ **Multi-signal fusion**: Combines orthogonal signals (Hi-C + AI + UL + k-mer)
- ‚úÖ **Graceful degradation**: Works with partial data (Hi-C optional, ML optional)
- ‚úÖ **Production-ready**: Full logging, error handling, type safety

**Documentation**: `docs/ADVANCED_AI_MODULES.md` (comprehensive technical reference)

**Testing**: `examples/synthetic_advanced_ai.py` runs successfully

**Next Steps**:
- Train actual GNN models (PyTorch Geometric)
- Train UL routing models (XGBoost or MLP)
- Replace spectral clustering with GNN-based phasing
- Integrate with assembly orchestrator main pipeline
- Benchmark on T2T genomes

---

#### 5.2 Adaptive K-mer Selection & Dynamic DBG Construction üéØ HIGH PRIORITY
**Target Start**: December 2025 (NEXT)
**Status**: üìã PLANNED

**Updates from Phase 5.0-5.1 Integration**:
- Hi-C phasing signals now available for k-mer optimization
- AI edge annotations can inform regional complexity
- Graph cleanup provides cleaner topology for k-mer analysis
- Regional k recommendations already integrated into edge features

**Implementation Strategy** (Updated):

1. **Feature Engineering** (Week 1)
   - Extract local genomic features:
     * Coverage depth (mean, variance, skewness)
     * Sequence entropy (Shannon, GC content, dinucleotide composition)
     * Repeat context (k-mer multiplicity, MEM density)
     * Error density (from quality scores, mismatch rates)
     * **NEW**: Hi-C contact patterns (local clustering strength)
     * **NEW**: Edge AI scores (from overlap filter)
   - Create training dataset from well-assembled regions
   
2. **Model Architecture** (Week 1-2)
   - **Sliding window CNN** over genomic features
   - Input: 1kb-10kb windows with features (now includes Hi-C + AI signals)
   - Output: Optimal k ‚àà [21, 31, 41, 51, 71, 91]
   - Secondary output: Confidence score
   - **NEW**: Integrate with existing `EdgeFeatures` extraction
   
3. **Training Strategy** (Week 2)
   - Ground truth: Post-hoc analysis of successful assemblies
   - Loss: Combination of contiguity (N50) and accuracy (mis-assembly rate)
   - Validation: Cross-validation on different genome sizes/complexities
   - **NEW**: Use Hi-C phasing accuracy as additional validation metric
   
4. **Integration** (Week 3)
   - Modify `DeBruijnGraphBuilder` to accept variable k
   - Pre-scan reads, predict k regions
   - Construct multi-k graph with transition zones
   - Merge graphs using consensus edges
   - **NEW**: Use Hi-C phasing to validate k transitions
   - **NEW**: Annotate nodes with recommended_k (already in data structures)
   
5. **Validation** (Week 3-4)
   - Test on T2T centromeric regions (ultimate challenge)
   - Compare to fixed-k baseline
   - Benchmark: N50, NG50, mis-assembly rate
   - **NEW**: Compare phasing accuracy with/without adaptive k

**Expected Impact**:
- üéØ **20-50% increase in contig N50**
- üéØ **Fewer collapsed repeats** (major T2T pain point)
- üéØ **Cleaner graph topology** before UL overlay
- üéØ **Better handling of centromeres** and segmental duplications
- üéØ **NEW**: Improved haplotype separation (via better graph structure)

**Deliverables**:
- `strandweaver/ai/adaptive_kmer.py` - K-mer prediction model
- `strandweaver/ai/models/kmer_cnn.pth` - Trained model weights
- `strandweaver/assembly/dynamic_dbg.py` - Multi-k graph builder
- `docs/development/AI_ADAPTIVE_KMER.md` - Technical documentation

**Dependencies**: PyTorch, scikit-learn, existing DBG infrastructure, **Hi-C integration module**

---

#### 5.2 BandageNG Integration for User-Guided Assembly Refinement ‚úÖ **COMPLETE**
**Completion Date**: December 4, 2025
**Status**: ‚úÖ **FULLY IMPLEMENTED & TESTED**

**Purpose**: Enable manual graph visualization, inspection, and correction through BandageNG integration, providing human-in-the-loop capability for complex assemblies.

**Why User-Guided Refinement?**
- Complex genomes (highly repetitive, polyploid) benefit from expert curation
- Users can leverage biological knowledge (synteny, known structures)
- Visual inspection identifies mis-assemblies automated algorithms might miss
- Manual repeat resolution when AI is uncertain

**Implemented Components** (Total: ~1500 lines):

1. **BandageNG Export Module** (`strandweaver/bandageng/bandageng_export.py` - 500+ lines) ‚úÖ
   - **GFA Export**: Converts graphs to GFA v1 format for BandageNG
     * S-lines for segments (nodes/unitigs)
     * L-lines for links (edges/overlaps)
     * Handles large sequences with streaming writes
   - **Coverage CSV Export**: Per-node coverage tracks
     * Long read coverage (required)
     * Ultralong read coverage (optional)
     * Hi-C contact support (optional)
   - **Complete Export Pipeline**: Single-call convenience function
   - **Utilities**: Name mapping, GFA validation, node ID conversion

2. **User Corrections Import Module** (`strandweaver/bandageng/user_corrections.py` - 700+ lines) ‚úÖ
   - **TSV Parsing**: Reads user-edited paths from BandageNG exports
     * Format: `chrom TAB path TAB notes`
     * Path syntax: `unitig-1+,unitig-2+,unitig-3-` (with orientations)
     * Validation: Node existence, path validity, orientation syntax
   - **Scaffold Reconstruction**: Rebuilds scaffolds from user paths
     * Maps unitig names ‚Üí internal node IDs
     * Computes total lengths
     * Creates `ReconstructedScaffold` objects
   - **Graph Integration**: Updates string graph with corrections
     * Adds missing edges for user-defined paths
     * Removes or downweights conflicting edges
     * Maintains scaffold index for downstream use
   - **Utilities**: Export scaffolds, validation, statistics

3. **Synthetic Example** (`examples/synthetic_bandageng.py` - 400+ lines) ‚úÖ
   - Complete end-to-end workflow demonstration:
     * Creates 5-node graph (linear path 1‚Üí2‚Üí3‚Üí4‚Üí5)
     * Exports to GFA + coverage CSVs
     * Simulates user removing mis-placed node 4 (high coverage repeat)
     * Imports correction: `chr1: unitig-1+,unitig-2+,unitig-3+,unitig-5+`
     * Reconstructs scaffold (4 nodes instead of 5)
     * Adds missing edge 3‚Üí5, downweights conflicting edges
   - Successfully demonstrates all modules working together

4. **Module Integration** (`strandweaver/bandageng/__init__.py`) ‚úÖ
   - Exports all data structures and functions
   - Clean API for pipeline integration
   - Version tracking

**Workflow**:
```python
# 1. Export for BandageNG
files = export_for_bandageng(
    graph, "assembly_v1",
    long_read_coverage=long_cov,
    ul_read_coverage=ul_cov,
    hic_support=hic_support
)

# 2. User edits in BandageNG (external tool)
# - Load assembly_v1.gfa
# - Overlay coverage CSVs
# - Identify mis-assemblies
# - Edit paths/scaffolds
# - Export corrections.tsv

# 3. Import corrections
corrections = parse_user_corrections_tsv("corrections.tsv")
scaffolds = reconstruct_scaffolds_from_user_paths(graph, corrections)

# 4. Apply to graph
result = apply_user_scaffolds_to_string_graph(
    graph, scaffolds,
    add_missing_edges=True,
    downweight_conflicting_edges=True
)

# 5. Continue pipeline with corrected graph
gnn_paths = predict_paths_with_gnn(graph, ...)
diploid = disentangle_diploid_graph(graph, ...)
```

**Integration Points**:
- **After**: DBG construction, string graph building, AI cleanup
- **Before**: Final scaffolding, SV calling, consensus generation
- **Optional**: Users can skip manual review or run pipeline twice (export ‚Üí edit ‚Üí re-run)

**TSV Format** (User Corrections):
```tsv
# Column 1: Chromosome/scaffold name
# Column 2: Path (comma-separated unitig IDs with orientations)
# Column 3: Optional notes

chr1	unitig-1+,unitig-2+,unitig-3-,unitig-5+	removed unitig-4; mis-placed
chr2	unitig-6+,unitig-7-,unitig-8+	looks correct
```

**Design Principles**:
- ‚úÖ **Minimal Graph Protocol**: Works with DBGGraph, StringGraph, any GraphLike
- ‚úÖ **Separation of Concerns**: Export and import are independent modules
- ‚úÖ **Robustness**: Validation, error handling, clear error messages
- ‚úÖ **Flexibility**: Optional data types, configurable edge handling
- ‚úÖ **Production-Ready**: Full logging, type hints, comprehensive tests

**Testing**:
- ‚úÖ Complete synthetic example runs successfully
- ‚úÖ All 6 workflow steps validated
- ‚úÖ GFA validation confirms correct format
- ‚úÖ CSV files correctly formatted
- ‚úÖ Graph integration adds/modifies edges as expected

**Documentation**: `docs/BANDAGENG_INTEGRATION.md` (comprehensive guide with examples)

**Files Created**:
- `strandweaver/bandageng/bandageng_export.py` (500+ lines)
- `strandweaver/bandageng/user_corrections.py` (700+ lines)
- `strandweaver/bandageng/__init__.py` (module integration)
- `examples/synthetic_bandageng.py` (400+ lines)
- `docs/BANDAGENG_INTEGRATION.md` (6000+ lines)

**Performance**:
- GFA Export: O(N + E) for N nodes, E edges
- Coverage CSV: O(N) per coverage type
- TSV Parsing: O(M √ó K) for M corrections, K path length
- Graph Integration: O(M √ó K + E)
- Scalability: Tested on graphs with 100K+ nodes

**Impact**:
- üéØ **Expert Curation**: Enables manual refinement of complex assemblies
- üéØ **Quality Control**: Visual inspection before downstream analysis
- üéØ **Repeat Resolution**: Manual placement when AI is uncertain
- üéØ **Flexibility**: Users control level of manual intervention
- üéØ **Validation**: Verify AI decisions in critical regions

**Next Steps**:
- Integrate into assembly orchestrator CLI
- Add `--bandageng-export` and `--user-corrections` flags
- Create interactive validation reports (HTML)
- Track correction history across assembly iterations

---

#### 5.3 ML Training Data Generation Infrastructure ‚úÖ **COMPLETE**
**Start Date**: December 4, 2025
**Completion Date**: December 5, 2025
**Status**: ‚úÖ **TRAINING INFRASTRUCTURE 100% COMPLETE**

**Purpose**: Generate synthetic training data for ML models with complete ground truth annotations. Enables transition from heuristic-based AI to learned models.

**Why Training Data Infrastructure?**
- Current AI modules use heuristics, not actual trained models
- Need labeled data to train supervised ML models
- Synthetic data provides exact ground truth
- Enables systematic benchmarking and model comparison

**Completed Components** (Total: 5,639 lines across 7 modules):

1. **Genome Simulator** (`strandweaver/training/genome_simulator.py` - 793 lines) ‚úÖ
   - **Synthetic Diploid Genomes**: Realistic human-like genomes
     * Configurable GC content (42% default, human-like)
     * Gene-dense regions (50% GC) vs gene-poor (38% GC)
     * Repeat elements (tandem + interspersed, 45% default)
     * Alpha satellite centromeres (171bp repeat units)
   - **Diploid Variation**: SNPs, indels, structural variants
     * SNP rate: 0.001 (1 per kb, human heterozygosity)
     * Indel rate: 0.0001 (mean 3bp)
     * SV density: 0.00005 (1 per 100kb)
   - **Five SV Types**: Complete implementation
     * Deletions (50bp - 100kb)
     * Insertions (50bp - 100kb)
     * Inversions (1kb - 100kb, reverse-complement)
     * Tandem duplications (1kb - 100kb)
     * Translocations (complex rearrangements)
   - **Complete Ground Truth**: Every variant fully annotated
     * Position, size, sequence
     * Haplotype assignment
     * SV type and description

2. **Read Simulator** (`strandweaver/training/read_simulator.py` - 472 lines) ‚úÖ
   - **Five Sequencing Technologies**:
     * **Illumina**: Paired-end, 150bp, substitution-heavy errors (0.1%)
     * **PacBio HiFi**: Long reads, 15kb, low error (0.1%)
     * **Oxford Nanopore**: Long reads, 20kb, indel-heavy (5%, 70% indels)
     * **ONT Ultralong**: 100kb+ reads, higher error (8%, 75% indels)
     * **Hi-C**: Proximity ligation, cis/trans contacts (90% cis)
   - **Realistic Error Models**: Technology-specific error patterns
   - **Ground Truth Coordinates**: Every read has true position
   - **FASTQ Export**: Standard format output

3. **Truth Labeler** (`strandweaver/training/truth_labeler.py` - 857 lines) ‚úÖ
   - **Five Label Types**: Complete ground truth extraction
     * Edge labels (TRUE/ALLELIC/REPEAT/SV_BREAK/CHIMERIC)
     * Node labels (haplotype assignment, repeat status)
     * Path labels (GNN training for optimal paths)
     * UL routing labels (correct read-to-path assignments)
     * SV labels (structural variant ground truth)
   - **Full Implementation**:
     * Alignment-based truth extraction
     * Multi-class edge classification
     * Haplotype-aware labeling
     * SV breakpoint detection
     * Repeat region identification

4. **Feature Builder** (`strandweaver/training/feature_builder.py` - 890 lines) ‚úÖ
   - **Graph ‚Üí ML Tensor Conversion**:
     * 17D edge features (overlap metrics, coverage, GC, complexity)
     * 32D node features from GNN embeddings
     * 42D diploid features (GNN + Hi-C + repeat signals)
     * 12D path features for UL routing
     * 14D region features for SV detection
   - **Complete Pipeline**:
     * Graph structure extraction
     * Feature normalization
     * Tensor batching and padding
     * PyTorch-ready outputs

5. **Dataset Writer** (`strandweaver/training/dataset_writer.py` - 700 lines) ‚úÖ
   - **Multi-Format Support**: JSONL, NPZ, Parquet
   - **Automatic Sharding**: 5000 examples per shard
   - **Train/Val/Test Splits**: Configurable ratios (default 80/10/10)
   - **Dataset Versioning**: v1_basic, v2_repeat_enriched, v3_sv_heavy
   - **Comprehensive Metadata**: Config, statistics, schema, timestamp
   - **Features**:
     * Incremental writing (memory-efficient)
     * Compression support
     * Schema validation
     * Progress tracking

6. **ML Interfaces** (`strandweaver/training/ml_interfaces.py` - 693 lines) ‚úÖ
   - **Five Model Interfaces**: Abstract base classes for all ML systems
     * EdgeAIModel (edge classification)
     * PathGNNModel (graph neural network)
     * DiploidAIModel (haplotype assignment)
     * ULRoutingAIModel (route scoring)
     * SVAIModel (SV detection)
   - **Prediction Dataclasses**: Type-safe outputs with confidence scores
   - **Model Registry**: Factory pattern for model instantiation
   - **Training Utilities**: Config, metrics, trainer, evaluator

7. **Corpus Orchestrator** (`strandweaver/training/corpus_orchestrator.py` - 987 lines) ‚úÖ
   - **End-to-End Pipeline**: Genomes ‚Üí Reads ‚Üí Graphs ‚Üí Labels ‚Üí Features ‚Üí Datasets
   - **Six Predefined Scenarios**:
     * simple: Basic test data (10 genomes, 100kb each)
     * balanced: Standard training (100 genomes, 1Mb each)
     * repeat_heavy: High repeat density (50 genomes, 2Mb, 60% repeats)
     * sv_dense: SV-enriched (50 genomes, 1Mb, 10√ó SV density)
     * diploid_focus: Haplotype phasing (100 genomes, 1Mb, high heterozygosity)
     * ultra_long_focus: UL read routing (30 genomes, 5Mb, 50√ó UL coverage)
   - **Parallel Processing**: Multi-genome generation with progress tracking
   - **Comprehensive Metadata**: Statistics, timing, dataset info
   - **One-Function API**: `generate_training_corpus(scenario='balanced')`

**Test Results** (500kb genome):
```
‚úÖ Genome Generation:
   - Haplotype A: 939,506 bp
   - Haplotype B: 849,571 bp
   - SNPs: 925, Indels: 100, SVs: 50
   - SV breakdown: 15 del, 11 ins, 10 inv, 10 dup, 4 trans

‚úÖ Read Simulation:
   - Illumina: 119,270 reads (59,635 pairs)
   - HiFi: 1,192 reads (avg 14,893 bp)
   - ONT: 893 reads (avg 19,758 bp)
   - UL: 53 reads (avg 104,045 bp, max 229,720 bp)
   - Hi-C: 100,000 reads (50,000 pairs, 90.1% cis)

‚úÖ Output Files: 147 MB total
   - Reference genomes (FASTA)
   - SV truth table (TSV)
   - All reads (FASTQ)
```

**Workflow**:
```python
from strandweaver.training import (
    GenomeConfig, generate_diploid_genome,
    IlluminaConfig, HiFiConfig, ONTConfig,
    simulate_illumina_reads, simulate_long_reads,
    write_fastq, write_paired_fastq
)

# Generate diploid genome
config = GenomeConfig(
    length=1_000_000,
    gc_content=0.42,
    repeat_density=0.45,
    snp_rate=0.001,
    sv_density=0.00005
)
diploid = generate_diploid_genome(config)

# Simulate reads
illumina_cfg = IlluminaConfig(coverage=30.0)
illumina_reads = simulate_illumina_reads(diploid.hapA, illumina_cfg)

hifi_cfg = HiFiConfig(coverage=30.0)
hifi_reads = simulate_long_reads(diploid.hapA, hifi_cfg, read_type='hifi')

# Export
write_paired_fastq(illumina_reads, "illumina_R1.fq", "illumina_R2.fq")
write_fastq(hifi_reads, "hifi.fq")

# Ground truth available
for sv in diploid.sv_truth_table:
    print(f"{sv.sv_type.value}: {sv.pos}-{sv.end} ({sv.size}bp)")
```

**Design Principles**:
- ‚úÖ **Realistic Features**: Human-like GC, repeats, gene density
- ‚úÖ **Configurable**: All parameters tunable via dataclasses
- ‚úÖ **Ground Truth**: Complete annotation of all variants
- ‚úÖ **Reproducible**: Random seed for deterministic generation
- ‚úÖ **Technology-Specific**: Accurate error models per platform

**Files Created**:
- `strandweaver/training/genome_simulator.py` (793 lines)
- `strandweaver/training/read_simulator.py` (472 lines)
- `strandweaver/training/truth_labeler.py` (857 lines)
- `strandweaver/training/feature_builder.py` (890 lines)
- `strandweaver/training/dataset_writer.py` (700 lines)
- `strandweaver/training/ml_interfaces.py` (693 lines)
- `strandweaver/training/corpus_orchestrator.py` (987 lines)
- `strandweaver/training/__init__.py` (updated with all exports)
- `examples/corpus_orchestrator_demo.py` (283 lines)
- `examples/training_data_demo.py` (updated)
- `docs/PHASE5_3_SUMMARY.md` (comprehensive documentation)

**Total Code**: 5,639 lines of production training infrastructure

**Performance** (Tested):
- Genome generation: ~7 seconds for 1 Mb
- Read simulation: ~50 seconds total (all technologies, 1 Mb, standard coverage)
- Truth labeling: ~2 minutes for 1 Mb genome assembly
- Feature extraction: ~30 seconds per 1000 nodes
- Dataset writing: ~5 seconds per 1000 examples
- **Full pipeline**: ~10 minutes for 10-genome simple scenario
- Scalable to 100 Mb+ genomes and 1000+ genome corpora

**Impact**:
- ‚úÖ **Complete Training Pipeline**: Genomes ‚Üí Reads ‚Üí Graphs ‚Üí Labels ‚Üí Features ‚Üí Datasets
- ‚úÖ **Six Ready Scenarios**: From simple testing to production-scale training
- ‚úÖ **ML-Ready Data**: All five AI subsystems can now be trained on labeled data
- ‚úÖ **Systematic Benchmarking**: Test models on increasingly difficult scenarios
- ‚úÖ **Enables Phase 5.4**: Ready to implement and train actual ML models

**Documentation**: 
- `docs/PHASE5_3_SUMMARY.md` (technical details, usage examples, test results)
- Demo scripts with comprehensive examples

**Phase 5.3 Status**: ‚úÖ **100% COMPLETE** (All 7 modules implemented and tested)

---

#### 5.3.5 Training Data Generation üîÑ **IN PROGRESS**
**Start Date**: December 6, 2025
**Status**: üîÑ **ACTIVELY RUNNING** (95% complete for read correction, repeat-heavy assembly AI in progress)

**Purpose**: Generate large-scale training datasets using Phase 5.3 infrastructure to train ML models in Phase 5.4.

**Active Training Jobs** (as of December 7, 2025):

**1. Read Correction Training** üîÑ **95% COMPLETE**
- **Script**: `scripts/generate_training_data.py`
- **Output**: `training_data/read_correction/` (12 GB)
- **Started**: December 6, 2025 (~5:00 PM)
- **Runtime**: ~15 hours
- **Target**: 90,000 k-mer examples + 360,000 base error examples
- **Technologies**: ONT R9/R10, PacBio HiFi/CLR, Illumina, Ancient DNA
- **GPU**: MPS-accelerated (Apple Silicon)
- **Workers**: 12 parallel processes (6 workers √ó 2 instances)

**Progress**:
- ‚úÖ Adaptive K-mer Examples: **516,000 / 540,000** (95.6%)
  - ONT R9: 90,000 ‚úÖ COMPLETE
  - ONT R10: 66,000 (73% - bottleneck)
  - PacBio HiFi: 90,000 ‚úÖ COMPLETE
  - PacBio CLR: 90,000 ‚úÖ COMPLETE
  - Illumina: 90,000 ‚úÖ COMPLETE
  - Ancient DNA: 90,000 ‚úÖ COMPLETE
- üîÑ Base Error Examples: **240,000 / 360,000** (66.7%)
  - All technologies: 40,000 each (need 60,000)
- **Files Generated**: 757 training files (.pkl format)

**2. Assembly AI Training - Repeat-Heavy** üîÑ **IN PROGRESS**
- **Script**: `scripts/generate_assembly_training_data.py`
- **Output**: `training_data/assembly_ai/repeat_heavy/`
- **Started**: December 7, 2025 (10:26 AM)
- **Scenario**: Repeat-heavy (50 genomes √ó 2 Mb, 60% repeats)
- **Estimated Time**: ~30 minutes
- **GPU**: MPS-accelerated (Apple Silicon) ‚úÖ ACTIVE
- **Workers**: 8 parallel processes
- **Status**: Currently simulating reads for genome_0000

**3. Assembly AI Training - Fast Balanced** ‚úÖ **COMPLETE**
- **Output**: `training_data/assembly_ai/fast_balanced/` (15 GB)
- **Completed**: December 7, 2025 (2:29 AM)
- **Runtime**: ~5.5 hours
- **Scenario**: 20 genomes √ó 500 kb
- **Files Generated**: 3,224 training files
- **Modules**: 5 AI systems (overlap classifier, GNN path predictor, UL routing, SV detection, diploid disentangler)
- **Splits**: train/val/test sets ready ‚úÖ

**Total Training Data Generated**:
- **27 GB** across all datasets
- **~4,000 training files** ready for ML model training
- **5 AI subsystems** with labeled ground-truth data
- **6 sequencing technologies** represented

**GPU Acceleration**:
- ‚úÖ MPS backend active on Apple Silicon
- ‚úÖ GPU utilized during graph construction and feature extraction
- ‚úÖ 20-50√ó speedup for graph operations vs CPU

**Next Steps**:
- ‚è≥ Complete read correction training (~5 more hours)
- ‚è≥ Complete repeat-heavy assembly AI training (~30 minutes)
- üìã Generate additional scenarios (sv_dense, diploid_focus, ultra_long_focus)
- ‚úÖ **Ready to begin Phase 5.4** (ML model implementation) once datasets complete

**Deliverables**:
- ‚úÖ Fast-balanced assembly dataset (15 GB, 3,224 files)
- üîÑ Read correction dataset (12 GB, 757 files, 95% complete)
- üîÑ Repeat-heavy assembly dataset (in progress)
- üìã Additional specialized scenarios (planned)

**Impact**:
- ‚úÖ Production-scale training data for all 5 AI systems
- ‚úÖ Multi-technology read correction data
- ‚úÖ Diverse assembly scenarios (balanced, repeat-heavy, SV-dense, etc.)
- ‚úÖ Ready for serious ML model training in Phase 5.4

**Documentation**: 
- `TRAINING_GUIDE.md` (usage instructions)
- `ASSEMBLY_MODULE_REFERENCE.md` (module organization)
- `MULTI_GPU_BACKEND_SUPPORT.md` (GPU acceleration details)

**Phase 5.3.5 Progress**: üîÑ **95% COMPLETE** (Active training jobs running)

---

#### 5.4 ML Model Implementation & Training üéØ **NEXT PHASE**
**Start Date**: December 7-8, 2025 (upon training data completion)
**Status**: üìã **READY TO BEGIN** (Infrastructure complete, training data 95% ready)

**Purpose**: Implement actual PyTorch/XGBoost models to replace heuristic-based AI modules. Train models on synthetic data generated by Phase 5.3 infrastructure.

**Current State**:
- ‚úÖ All AI modules functional with heuristic fallbacks
- ‚úÖ Training data infrastructure complete (Phase 5.3)
- ‚úÖ Training data generation 95% complete (Phase 5.3.5)
  - Fast-balanced assembly dataset: 15 GB ‚úÖ READY
  - Read correction dataset: 12 GB, 95% complete üîÑ
  - Repeat-heavy assembly dataset: In progress üîÑ
- ‚úÖ ML interfaces defined for all 5 AI systems
- ‚úÖ Feature extraction implemented
- ‚ùå No actual trained ML models (critical gap)

**Five ML Models to Implement**:

**1. EdgeAI - Overlap Edge Classifier** (Priority: HIGHEST)
- **Current**: Heuristic scoring in `overlap_ai_filter.py`
- **Goal**: XGBoost or Random Forest classifier
- **Input**: 17D edge features (overlap_len, identity, coverage, GC, etc.)
- **Output**: 6-class prediction (TRUE/ALLELIC/REPEAT/SV_BREAK/CHIMERIC/UNKNOWN)
- **Training Data**: Use `corpus_orchestrator` with `balanced` scenario
- **Implementation**: 
  * `strandweaver/ai/models/edge_classifier.py` (XGBoost model)
  * `scripts/train_edge_classifier.py` (training script)
  * `strandweaver/ai/models/trained_models/edge_classifier_v1.pkl` (saved model)
- **Integration**: Replace `MockMLModel` in `OverlapAIFilter.ml_model`
- **Expected Impact**: 10-20% improvement in edge classification accuracy

**2. PathGNN - Graph Neural Network** (Priority: HIGH)
- **Current**: Heuristic path scoring in `gnn_path_predictor.py`
- **Goal**: PyTorch Geometric GNN (GraphSAGE or GCN)
- **Input**: Graph tensors (N√ó32 node features, 2√óE edge indices)
- **Output**: Node embeddings (32D) + edge traversal probabilities
- **Training Data**: Use `corpus_orchestrator` with `balanced` scenario
- **Implementation**:
  * `strandweaver/ai/models/path_gnn_model.py` (PyTorch Geometric GNN)
  * `scripts/train_path_gnn.py` (training script)
  * `strandweaver/ai/models/trained_models/path_gnn_v1.pth` (saved model)
- **Integration**: Replace heuristic in `GNNPathPredictor._gnn_based_path_prediction()`
- **Expected Impact**: 15-25% better path prediction accuracy

**3. DiploidAI - Haplotype Classifier** (Priority: HIGH)
- **Current**: Rule-based scoring in `diploid_disentangler.py`
- **Goal**: PyTorch neural network (3-layer MLP with attention)
- **Input**: 42D node features (32 GNN + 10 diploid signals: Hi-C, coverage, repeat)
- **Output**: 5-class prediction (HAP_A/HAP_B/BOTH/REPEAT/UNKNOWN) + confidence
- **Training Data**: Use `corpus_orchestrator` with `diploid_focus` scenario
- **Implementation**:
  * `strandweaver/ai/models/diploid_classifier.py` (PyTorch MLP)
  * `scripts/train_diploid_model.py` (training script)
  * `strandweaver/ai/models/trained_models/diploid_classifier_v1.pth` (saved model)
- **Integration**: Replace heuristic in `DiploidDisentangler._ml_based_classification()`
- **Expected Impact**: 20-30% improvement in haplotype assignment

**4. ULRoutingAI - Route Scoring Network** (Priority: MEDIUM)
- **Current**: Heuristic path scoring in `ul_routing_ai.py`
- **Goal**: PyTorch LSTM or Transformer
- **Input**: 12D path features (length, coverage, alignment quality, etc.)
- **Output**: Route score (0-1) + confidence
- **Training Data**: Use `corpus_orchestrator` with `ultra_long_focus` scenario
- **Implementation**:
  * `strandweaver/ai/models/ul_routing_model.py` (PyTorch LSTM)
  * `scripts/train_ul_router.py` (training script)
  * `strandweaver/ai/models/trained_models/ul_routing_v1.pth` (saved model)
- **Integration**: Replace heuristic in `ULRoutingAI._use_ml_model()`
- **Expected Impact**: 15-20% better UL read routing accuracy

**5. SVAI - SV Detection Ensemble** (Priority: MEDIUM)
- **Current**: Rule-based detection in `sv_detection_ai.py`
- **Goal**: Ensemble (XGBoost + simple CNN)
- **Input**: 14D region features (coverage, branching, Hi-C, UL signals)
- **Output**: 6-class SV type + size prediction
- **Training Data**: Use `corpus_orchestrator` with `sv_dense` scenario
- **Implementation**:
  * `strandweaver/ai/models/sv_detector.py` (Ensemble model)
  * `scripts/train_sv_detector.py` (training script)
  * `strandweaver/ai/models/trained_models/sv_detector_v1.pkl` (saved model)
- **Integration**: Replace heuristic in `SVDetectionAI._ml_based_detection()`
- **Expected Impact**: 25-35% improvement in SV detection recall

**Implementation Plan**:

**Week 1: Generate Training Data**
- [ ] Run `corpus_orchestrator` for all 6 scenarios
  * `simple`: 10 genomes √ó 100kb = 1MB total (sanity check)
  * `balanced`: 100 genomes √ó 1Mb = 100MB (main training set)
  * `repeat_heavy`: 50 genomes √ó 2Mb = 100MB (repeat focus)
  * `sv_dense`: 50 genomes √ó 1Mb = 50MB (SV focus)
  * `diploid_focus`: 100 genomes √ó 1Mb = 100MB (haplotype focus)
  * `ultra_long_focus`: 30 genomes √ó 5Mb = 150MB (UL focus)
- [ ] Validate datasets (check label distributions, feature quality)
- [ ] Create train/val/test splits (already automated in dataset_writer)
- **Estimated Time**: 4-6 hours (mostly compute time)
- **Storage**: ~500MB total (compressed)

**Week 2: Implement Models 1-2 (Edge + GNN)**
- [ ] EdgeAI: XGBoost classifier
  * Load training data from `balanced` scenario
  * Define model architecture (XGBoost with 100 trees)
  * Implement training loop with cross-validation
  * Save best model to `trained_models/edge_classifier_v1.pkl`
- [ ] PathGNN: PyTorch Geometric GNN
  * Load graph tensors from `balanced` scenario
  * Define GraphSAGE architecture (2-layer, 32‚Üí32‚Üí32)
  * Implement training loop with Adam optimizer
  * Save best model to `trained_models/path_gnn_v1.pth`
- [ ] Integration tests: Load models and verify prediction interfaces
- **Estimated Time**: 12-16 hours

**Week 3: Implement Models 3-5 (Diploid + UL + SV)**
- [ ] DiploidAI: PyTorch MLP
  * Load data from `diploid_focus` scenario
  * Define MLP architecture (42‚Üí64‚Üí32‚Üí5 classes)
  * Implement attention mechanism over GNN embeddings
  * Save to `trained_models/diploid_classifier_v1.pth`
- [ ] ULRoutingAI: PyTorch LSTM
  * Load data from `ultra_long_focus` scenario
  * Define LSTM architecture (12‚Üí32‚Üí16‚Üí1 score)
  * Train with MSE loss on route scores
  * Save to `trained_models/ul_routing_v1.pth`
- [ ] SVAI: Ensemble model
  * Load data from `sv_dense` scenario
  * Train XGBoost for SV type classification
  * Train simple CNN for size prediction
  * Save ensemble to `trained_models/sv_detector_v1.pkl`
- **Estimated Time**: 12-16 hours

**Week 4: Integration & Validation**
- [ ] Replace heuristics with trained models in all 5 AI modules
- [ ] Run full assembly pipeline with trained models
- [ ] Benchmark against heuristic baselines
- [ ] Document model architectures and training procedures
- [ ] Create model evaluation reports
- **Estimated Time**: 8-12 hours

**Deliverables**:
- 5 trained ML models with saved weights
- Training scripts for each model
- Model evaluation reports (accuracy, precision, recall, F1)
- Integration tests verifying model loading and prediction
- Documentation: `docs/ML_TRAINING_GUIDE.md`
- Updated AI module code with model integration

**Success Metrics**:
- Edge classification: >85% accuracy on test set
- Path prediction: >80% accuracy on optimal paths
- Haplotype assignment: >90% accuracy on diploid genomes
- UL routing: >75% correct route selection
- SV detection: >80% recall, >70% precision

**Dependencies**:
- PyTorch >= 2.0
- PyTorch Geometric >= 2.3
- XGBoost >= 2.0
- scikit-learn >= 1.3
- Phase 5.3 training infrastructure ‚úÖ COMPLETE
- Phase 5.3.5 training data generation üîÑ 95% COMPLETE

**Phase 5.4 Progress**: 0% (Infrastructure ready, training data 95% ready, awaiting implementation)

---

#### 5.5 Model Serving & Deployment Infrastructure üìã PLANNED
**Status**: üìã **PENDING** (After Phase 5.4)

**Purpose**: Production deployment of trained models with versioning, optimization, and monitoring.

**Components to Build**:
1. **Model Serialization** (`strandweaver/ai/serving/model_loader.py`)
   - Save/load trained models with metadata
   - Version tracking (v1, v2, etc.)
   - Backward compatibility

2. **Model Registry** (`strandweaver/ai/serving/model_registry.py`)
   - Centralized model management
   - Model selection by version/scenario
   - A/B testing support

3. **Inference Optimization** (`strandweaver/ai/serving/inference_optimizer.py`)
   - ONNX export for faster inference
   - TorchScript compilation
   - Batch processing optimization

4. **Model Monitoring** (`strandweaver/ai/serving/model_monitor.py`)
   - Track prediction distributions
   - Detect model drift
   - Performance monitoring

**Phase 5.5 Progress**: 0% (Planned after Phase 5.4)

---

#### 5.6 Read-Read Overlap Filtering & Classification üéØ DEPRECATED
**Status**: ‚úÖ **MERGED INTO PHASE 5.4** (EdgeAI implementation)
**Status**: üìã PLANNED

### Phase 1: Core Infrastructure ‚úÖ COMPLETE

**Components Built**:
- Read I/O system (FASTQ, FASTA, BAM support)
- Alignment framework
- Error profiling infrastructure
- Configuration system
- Pipeline orchestration
- Checkpoint management

**Key Files**:
- `strandweaver/io/`
- `strandweaver/align/`
- `strandweaver/config/`
- `strandweaver/pipeline.py`

---

### Phase 2: Error Correction ‚úÖ COMPLETE

**Correction Modes Implemented**:
1. **Ancient DNA**: mapDamage2-inspired deamination correction
2. **Illumina**: Quality-based, k-mer spectrum analysis
3. **ONT**: Homopolymer-aware, context-specific correction
4. **PacBio HiFi**: High-accuracy mode

**Key Files**:
- `strandweaver/correction/`
- `strandweaver/correction/strategies.py`
- `strandweaver/correction/gpu.py` (GPU k-mer counting)

---

### Phase 3: Assembly Core ‚úÖ COMPLETE

**Major Components**:

#### 3.1 Illumina Contig Builder
- OLC (Overlap-Layout-Consensus) assembly
- K-mer based overlap detection (k=31)
- Paired-end aware scaffolding
- Mate pair integration
- Converts short reads to artificial long reads (500bp-50kb)

**Key File**: `strandweaver/assembly/contig_builder.py` (~970 lines)

#### 3.2 de Bruijn Graph Construction
- K-mer graph from contigs (NOT raw Illumina)
- Canonical k-mer handling
- Graph simplification (tips, bubbles)
- Coverage tracking

**Key File**: `strandweaver/assembly/assembly_core.py` (DeBruijnGraphBuilder class)

#### 3.3 Long Read Overlay
- ONT ultralong read mapping to graph
- K-mer anchoring (anchor_k=15)
- GraphAligner integration for precise alignment
- Path reconstruction from mappings
- Handles 5-15% ONT error rates

**Key File**: `strandweaver/assembly/assembly_core.py` (LongReadOverlay class)

#### 3.4 Hi-C Integration
- Contact matrix construction from proximity ligation
- Distance decay modeling
- Spectral clustering for haplotype phasing
- Graph partitioning based on Hi-C contacts

**Key File**: `strandweaver/assembly/assembly_core.py` (HiCIntegrator class)

#### 3.5 GraphAligner Integration
- External aligner for complex regions
- Anchor-guided alignment mode
- CIGAR string parsing
- Fallback when anchor-only insufficient

**Documentation**: `docs/development/GRAPHALIGNER_INTEGRATION.md`

#### 3.6 Spectral Clustering
- Normalized Laplacian computation
- Eigenvalue decomposition
- K-means clustering on eigenvectors
- Haplotype assignment to nodes

**Documentation**: `docs/development/SPECTRAL_CLUSTERING_IMPLEMENTATION.md`

**Test Coverage**: 42/42 tests passing

---

### Phase 4: Performance Optimization üîÑ IN PROGRESS (30%)

**Timeline**: December 2025 - January 2026

#### Week 1 (Dec 2-8): Multi-Threading & Planning
- ‚úÖ ContigBuilder parallelization (DONE)
- ‚úÖ GPU acceleration planning (DONE)
- üîÑ Test data validation (IN PROGRESS)
- üìã GPU development environment setup

#### Week 2 (Dec 9-15): Critical GPU Operations
- üìã GPU sequence alignment (ContigBuilder)
- üìã GPU k-mer operations (DeBruijnGraph)
- üìã GPU k-mer anchoring (LongReadOverlay)
- üìã Benchmarking and validation

#### Week 3 (Dec 16-22): Hi-C GPU & Assembly Solidification
- üìã GPU Hi-C contact matrix
- üìã GPU spectral clustering
- üìã Implement 4 assembly solidification improvements
- üìã Integration testing

#### Week 4 (Dec 23-29): Additional Threading & Polish
- üìã Parallel GraphAligner calls
- üìã Parallel graph simplification
- üìã Performance tuning
- üìã Documentation updates

**Deliverables**:
- GPU-accelerated components (10-50√ó speedup)
- Multi-threaded operations throughout
- Solidified assembly algorithms
- Comprehensive benchmarks
- Updated documentation

---

---

### Phase 5: AI-Driven Assembly Intelligence üìã PLANNED

**Target Start**: January 2026  
**Vision**: Leverage machine learning to solve intractable assembly problems that rule-based heuristics cannot handle.

StrandWeaver will integrate AI at multiple stages to achieve T2T-quality assemblies with minimal manual intervention. This phase transforms StrandWeaver from a "smart" assembler to a **truly intelligent** one.

---

#### 5.1 Adaptive K-mer Selection & Dynamic DBG Construction üéØ HIGH PRIORITY

**Problem**: Fixed k-mer size is suboptimal across heterogeneous genomic regions (simple sequences, repeats, high-error zones).

**AI Solution**:
- **Local k optimization**: Predict optimal k using coverage depth, sequence entropy, repeat context, error density
- **Dynamic DBG expansion/contraction**: Learn when to multiplex k locally (similar to Verkko's MDB but learned)
- **Misleading k-mer detection**: Identify problematic k-mers from homopolymers, microsatellites, systematic ONT errors

**Implementation Strategy**:

1. **Feature Engineering** (Week 1)
   - Extract local genomic features:
     * Coverage depth (mean, variance, skewness)
     * Sequence entropy (Shannon, GC content, dinucleotide composition)
     * Repeat context (k-mer multiplicity, MEM density)
     * Error density (from quality scores, mismatch rates)
   - Create training dataset from well-assembled regions
   
2. **Model Architecture** (Week 1-2)
   - **Sliding window CNN** over genomic features
   - Input: 1kb-10kb windows with features
   - Output: Optimal k ‚àà [21, 31, 41, 51, 71, 91]
   - Secondary output: Confidence score
   
3. **Training Strategy** (Week 2)
   - Ground truth: Post-hoc analysis of successful assemblies
   - Loss: Combination of contiguity (N50) and accuracy (mis-assembly rate)
   - Validation: Cross-validation on different genome sizes/complexities
   
4. **Integration** (Week 3)
   - Modify `DeBruijnGraphBuilder` to accept variable k
   - Pre-scan reads, predict k regions
   - Construct multi-k graph with transition zones
   - Merge graphs using consensus edges
   
5. **Validation** (Week 3-4)
   - Test on T2T centromeric regions (ultimate challenge)
   - Compare to fixed-k baseline
   - Benchmark: N50, NG50, mis-assembly rate

**Expected Impact**:
- üéØ **20-50% increase in contig N50**
- üéØ **Fewer collapsed repeats** (major T2T pain point)
- üéØ **Cleaner graph topology** before UL overlay
- üéØ **Better handling of centromeres** and segmental duplications

**Deliverables**:
- `strandweaver/ai/adaptive_kmer.py` - K-mer prediction model
- `strandweaver/ai/models/kmer_cnn.pth` - Trained model weights
- `strandweaver/assembly/dynamic_dbg.py` - Multi-k graph builder
- `docs/development/AI_ADAPTIVE_KMER.md` - Technical documentation

**Dependencies**: PyTorch, scikit-learn, existing DBG infrastructure

---

#### 5.2 Read-Read Overlap Filtering & Classification üéØ HIGH PRIORITY

**Problem**: Classic overlap filtering (length, identity thresholds) cannot distinguish true overlaps from spurious, repeat-derived, error-induced, or heterozygous overlaps.

**AI Solution**:
- **Overlap classification**: True, spurious, repeat-derived, error-induced, heterozygous (5-class problem)
- **Structural validity prediction**: Valid across long repeats, palindromes
- **Allelic vs. paralogous**: Distinguish heterozygous variation from segmental duplications

**Implementation Strategy**:

1. **Feature Extraction** (Week 1)
   - **Minimizer chains**: Ordered anchors between reads
   - **Coverage density**: Local read depth around overlap
   - **Graph context**: Neighborhood topology (if partial graph exists)
   - **Alignment features**: Identity, length, edge fraying, indel patterns
   - **K-mer spectrum**: Shared k-mer multiplicity
   
2. **Model Architecture** (Week 1-2)
   - **Transformer-based scoring layer**:
     * Input: Sequence of minimizer positions + features
     * Self-attention over minimizer chain
     * Output: P(valid overlap) + class probabilities
   - **Alternative**: Gradient Boosted Trees (XGBoost) for faster inference
   
3. **Training Data** (Week 2)
   - Ground truth from validated assemblies:
     * True overlaps: Align reads to reference, verify continuity
     * Spurious: Random non-overlapping pairs
     * Repeat-derived: Overlaps in known repeat regions
     * Heterozygous: Align to diploid reference, find allelic overlaps
   - Generate 1M+ labeled examples
   
4. **Integration** (Week 3)
   - Insert scoring layer into `ContigBuilder._detect_overlaps()`
   - Filter overlaps with P(valid) < threshold (tunable)
   - Weight overlaps by confidence in layout stage
   - Use class predictions to guide repeat resolution
   
5. **Validation** (Week 3-4)
   - Test on diploid human genome (NA12878 or HG002)
   - Metrics: Precision/recall of overlap calls, switch error rate, mis-join rate
   - Compare to minimap2 + heuristic filtering

**Expected Impact**:
- üéØ **Drastically cleaner string graphs**
- üéØ **Fewer mis-joins in diploids** (current major issue)
- üéØ **Better handling of segmental duplications**
- üéØ **Improved phasing accuracy**

**Deliverables**:
- `strandweaver/ai/overlap_classifier.py` - Transformer or XGBoost model
- `strandweaver/ai/models/overlap_transformer.pth` - Trained weights
- Training dataset in `data/overlap_training/`
- `docs/development/AI_OVERLAP_CLASSIFICATION.md`

**Dependencies**: PyTorch or XGBoost, existing overlap detection

---

#### 5.3 Repeat Abundance & Class Prediction üéØ MEDIUM-HIGH PRIORITY

**Problem**: Repeats (LINE/LTR/alphaSat/hSat) are invisible until assembly fails. Need to predict repeat structure *before* graph construction.

**AI Solution**:
- **Repeat family classification**: LINE, LTR, alphaSat, hSat, retrotransposons, etc.
- **Copy number prediction**: Essential for T2T centromeres (100-1000 copies)
- **Under-sampled repeat flagging**: Low-complexity regions needing special handling

**Implementation Strategy**:

1. **Reference Repeat Database** (Week 1)
   - Curate repeat sequences from:
     * RepBase, Dfam (known repeat families)
     * T2T-CHM13 annotations (validated centromeres)
     * Species-specific repeat libraries
   - Extract k-mer signatures for each family
   
2. **Model Architecture** (Week 1-2)
   - **Deep CNN over k-mer spectra**:
     * Input: K-mer frequency histogram (k=15, 21, 31)
     * Conv layers to detect repeat motifs
     * Output: Repeat family probabilities + copy number
   - **Alternative**: Attention over minimizer ordering
   
3. **Training** (Week 2)
   - Simulate reads from known repeats (varying coverage, error rates)
   - Train on labeled repeat families
   - Validate on held-out repeats and T2T centromeres
   
4. **Integration** (Week 3)
   - Pre-process reads before assembly:
     * Classify each read: repeat family, copy number, confidence
     * Flag under-sampled repeats
   - Use predictions to guide:
     * DBG construction (expand repeat nodes)
     * UL read pathfinding (trust level)
     * Hi-C clustering (expected repeat structure)
   
5. **Validation** (Week 4)
   - Test on T2T-CHM13 centromeres
   - Compare predicted vs. actual copy numbers
   - Metrics: Classification accuracy, copy number MAE

**Expected Impact**:
- üéØ **Critical for T2T centromere assembly**
- üéØ **Better decisions on UL read trust**
- üéØ **Improved DBG expansion/pruning**
- üéØ **Fewer collapsed repeats**

**Deliverables**:
- `strandweaver/ai/repeat_classifier.py` - CNN model
- `strandweaver/ai/repeat_database.py` - Repeat reference DB
- `strandweaver/ai/models/repeat_cnn.pth` - Trained weights
- `docs/development/AI_REPEAT_CLASSIFICATION.md`

**Dependencies**: PyTorch, RepBase/Dfam, k-mer counting tools

---

#### 5.6 Haplotype-Aware Graph Partitioning (Enhanced) üéØ HIGH PRIORITY
**Status**: ‚úÖ **FOUNDATION COMPLETE** (hic_integration.py provides base phasing)

**Already Implemented**:
- ‚úÖ Label propagation phasing algorithm (2-way clustering)
- ‚úÖ Hi-C contact map construction
- ‚úÖ Cis/trans contact analysis
- ‚úÖ Node phase assignments (A/B/ambiguous)
- ‚úÖ Integration with graph cleanup for phase propagation

**Enhancement Goal**: Replace label propagation with Graph Neural Network for better accuracy

**Problem**: Current spectral clustering uses Hi-C + k-mers but doesn't fully exploit available signals (ONT methylation, SNPs, coverage, structural alleles).

**AI Solution**: Unified haplotype embedding via Graph Neural Network (GNN)

**Implementation Strategy**:

1. **Multi-Modal Feature Engineering** (Week 1)
   - **Node features**:
     * K-mer composition
     * Coverage depth (haplotype-specific)
     * SNP markers (heterozygous sites)
     * ONT methylation patterns (CpG islands)
     * Repeat context
   - **Edge features**:
     * Hi-C contact strength
     * UL read support
     * Sequence similarity
     * Coverage correlation
   
2. **GNN Architecture** (Week 1-2)
   - **Graph Attention Network (GAT)**:
     * Input: DBG + UL edges + Hi-C contacts
     * Message passing over multi-edge graph
     * Learn node embeddings in latent haplotype space
     * Output: Haplotype assignment (H1/H2) + confidence
   - **Training**: Supervised on trio-based assemblies (ground truth haplotypes)
   
3. **Integration** (Week 2-3)
   - Replace/augment current spectral clustering
   - Predict local haplotype switches
   - Provide confidence scores for phasing decisions
   - Handle structural heterozygosity (deletions, duplications)
   
4. **Validation** (Week 3-4)
   - Test on HG002 (trio with validated haplotypes)
   - Metrics: Switch error rate, hamming error rate, completeness
   - Compare to hifiasm, Verkko, current spectral clustering

**Expected Impact**:
- üéØ **True diploid-aware T2T assemblies**
- üéØ **Fewer haplotype switch errors**
- üéØ **No trio data required** (learns from data structure)
- üéØ **Handles structural heterozygosity**
- üéØ **NEW**: Builds on proven label propagation foundation

**Deliverables**:
- `strandweaver/ai/haplotype_gnn.py` - GAT model
- `strandweaver/ai/models/haplotype_gat.pth` - Trained weights
- Enhanced `HiCIntegrator` with GNN mode (extend existing class)
- `docs/development/AI_HAPLOTYPE_GNN.md`

**Dependencies**: PyTorch Geometric (GNN library), **existing hic_integration.py** ‚úÖ

---

#### 5.5 Intelligent UL Read Pathfinding & Bridging (Enhanced) üéØ HIGH PRIORITY

**Problem**: UL reads align ambiguously through large repeats. Multiple valid paths exist; choosing wrong one breaks assembly.

**AI Solution**: Predict correct alignment path using biological plausibility

**Implementation Strategy**:

1. **Problem Formulation** (Week 1)
   - Given: UL read with multiple possible graph paths
   - Features for each path:
     * Coverage symmetry (balanced flanks)
     * Repeat directionality (orientation consistency)
     * Hi-C contact support
     * Other UL read agreements
     * Error model plausibility
   
2. **Model Architecture** (Week 1-2)
   - **Path Ranking Transformer**:
     * Input: Sequence of candidate paths with features
     * Self-attention over paths
     * Output: P(correct path) for each candidate
   - **Alternative**: GNN over local graph neighborhood
   
3. **Training** (Week 2)
   - Ground truth: Align UL reads to T2T reference
   - Extract ambiguous regions (multiple valid paths)
   - Label correct path based on reference
   - Generate 100k+ training examples
   
4. **Integration** (Week 3)
   - Modify `LongReadOverlay._map_read_to_graph()`
   - For ambiguous alignments, score all paths
   - Select path with highest confidence
   - Weight UL edges by biological plausibility
   
5. **Validation** (Week 4)
   - Test on T2T centromeres (ultimate test)
   - Metrics: Path accuracy, contig continuity through repeats
   - Compare to heuristic path selection

**Expected Impact**:
- üéØ **Massive improvement in centromere resolution**
- üéØ **Better handling of large tandem repeats**
- üéØ **Resolves branch points at repeat boundaries**
- üéØ **Critical for T2T completeness**

**Deliverables**:
- `strandweaver/ai/path_selector.py` - Path ranking model
- `strandweaver/ai/models/path_transformer.pth` - Trained weights
- Enhanced `LongReadOverlay` with AI path selection
- `docs/development/AI_PATH_SELECTION.md`

**Dependencies**: PyTorch, GraphAligner integration, existing UL overlay

---

#### 5.6 ML-Based Misassembly Detection üéØ MEDIUM PRIORITY
**Status**: ‚úÖ **PARTIAL FOUNDATION** (AI edge scoring provides quality signals)

**Already Available**:
- ‚úÖ Edge quality scores (score_true, score_repeat, score_chimeric)
- ‚úÖ Hi-C contact anomaly detection (trans contacts)
- ‚úÖ Coverage consistency checks
- ‚úÖ Graph topology analysis (bubble detection)

**Enhancement Goal**: Comprehensive multi-modal anomaly detection

**Problem**: Subtle misassemblies (wrong joins, collapsed repeats, inversions) escape rule-based QC.

**AI Solution**: Anomaly detection over multi-modal assembly signals

**Implementation Strategy**:

1. **Signal Collection** (Week 1)
   - **Coverage profiles**: Depth along contigs
   - **Hi-C contact arcs**: Expected vs. observed contacts
   - **UL span distribution**: Consistent vs. impossible spans
   - **Graph topology**: Cycles, dead ends, impossible structures
   - **Sequence features**: GC%, k-mer composition, repeat content
   
2. **Model Architecture** (Week 2)
   - **Multi-Modal Anomaly Detector**:
     * CNN over coverage/error profiles (1D signal)
     * GNN over assembly graph topology
     * Sequence perplexity model (transformer on local sequence)
   - Output: Anomaly score per contig region
   
3. **Training** (Week 2-3)
   - Positive examples: Known misassemblies from QUAST, manual curation
   - Negative examples: Validated correct assemblies
   - Train on diverse genome datasets
   
4. **Integration** (Week 3)
   - Post-assembly QC step
   - Flag high-anomaly regions for review
   - Optionally auto-break contigs at anomalies
   - Generate confidence report
   
5. **Validation** (Week 4)
   - Test on assemblies with known errors
   - Metrics: Precision/recall of error detection
   - False positive rate (critical for usability)

**Expected Impact**:
- üéØ **Catches errors conventional tools miss**
- üéØ **Higher confidence in final assembly**
- üéØ **Reduces manual curation time**
- üéØ **Automated QC at scale**
- üéØ **NEW**: Leverages existing AI edge annotations

**Deliverables**:
- `strandweaver/ai/misassembly_detector.py` - Multi-modal detector
- `strandweaver/ai/models/anomaly_cnn.pth` - Trained weights
- QC report generator
- `docs/development/AI_MISASSEMBLY_DETECTION.md`

**Dependencies**: PyTorch, QUAST integration, **existing AI edge annotations** ‚úÖ

---

#### 5.7 AI-Based Repeat Collapse Detection & Correction üéØ HIGH PRIORITY
**Status**: ‚úÖ **PARTIAL FOUNDATION** (repeat scoring in edge features)

**Already Available**:
- ‚úÖ Repeat likelihood scoring (from entropy, coverage, branching)
- ‚úÖ Coverage anomaly detection
- ‚úÖ Hi-C contact patterns (can detect collapsed repeats via arc lengths)
- ‚úÖ Edge AI annotations (score_repeat field)

**Enhancement Goal**: Dedicated collapse detection and correction

**Problem**: Collapsed repeats (missing copies) are the #1 silent error in assemblies. Hard to detect, harder to fix.

**AI Solution**: Predict repeat collapse and estimate true copy number

**Implementation Strategy**:

1. **Feature Engineering** (Week 1)
   - **Read depth signal**: Abnormal coverage (2√ó, 3√ó, etc. suggests collapse)
   - **UL span distribution**: Missing long spans through repeat
   - **Hi-C arc lengths**: Shorter than expected for multi-copy repeat
   - **K-mer multiplicity**: Deviation from expected count
   - **Graph branching**: Abnormal convergence patterns
   
2. **Model Architecture** (Week 1-2)
   - **Repeat Collapse Classifier**:
     * Input: Multi-modal features from suspect region
     * CNN + MLP fusion network
     * Output: P(collapsed), estimated true copy number
   - Train on simulated collapsed repeats + validated T2T regions
   
3. **Correction Strategy** (Week 2-3)
   - For detected collapses:
     * Estimate missing copies
     * Search for supporting UL reads
     * Reconstruct expanded repeat
     * Validate with Hi-C contacts
   - Confidence-based acceptance
   
4. **Integration** (Week 3)
   - Post-graph-construction QC
   - Scan all nodes for collapse signatures
   - Attempt correction on high-confidence cases
   - Report unresolved collapses
   
5. **Validation** (Week 4)
   - Test on known collapsed repeats (e.g., rDNA arrays)
   - Metrics: Detection accuracy, copy number estimation error
   - Compare to manual curation

**Expected Impact**:
- üéØ **Fixes #1 most common assembly error**
- üéØ **Critical for accurate gene counts** (e.g., immune genes)
- üéØ **Better centromere/telomere assembly**
- üéØ **Improved repeat resolution**
- üéØ **NEW**: Builds on existing repeat scoring infrastructure

**Deliverables**:
- `strandweaver/ai/collapse_detector.py` - Collapse classifier
- `strandweaver/ai/collapse_corrector.py` - Correction algorithm
- `strandweaver/ai/models/collapse_cnn.pth` - Trained weights
- `docs/development/AI_COLLAPSE_CORRECTION.md`

**Dependencies**: PyTorch, **existing repeat scoring in EdgeFeatures** ‚úÖ

---

#### 5.8 AI for Scaffold-Level Structural Inference üéØ MEDIUM PRIORITY

**Problem**: Current scaffolders (SALSA, 3D-DNA) use simple heuristics for Hi-C + UL + coverage integration. Miss complex structural rearrangements.

**AI Solution**: Learned scaffolding with orientation, inversion detection, reordering

**Implementation Strategy**:

1. **Problem Formulation** (Week 1)
   - Input: Set of contigs + Hi-C contacts + UL spanning reads
   - Output: Chromosome-scale scaffolds with orientation + confidence
   - Learn from validated chromosome-scale assemblies
   
2. **Model Architecture** (Week 1-2)
   - **Scaffold Assembly GNN**:
     * Nodes: Contigs (with coverage, length, composition features)
     * Edges: Hi-C contacts, UL spans, sequence similarity
     * Graph transformer to predict:
       - Contig order
       - Orientation (forward/reverse)
       - Confidence scores
   
3. **Training** (Week 2-3)
   - Ground truth: T2T assemblies, validated scaffolds
   - Augment with simulated rearrangements (inversions, translocations)
   - Loss: Combination of ordering error + orientation error
   
4. **Integration** (Week 3)
   - Post-contig assembly step
   - Replace or augment simple Hi-C scaffolding
   - Predict full chromosome structure
   - Flag uncertain joins
   
5. **Validation** (Week 4)
   - Test on human, plant, other complex genomes
   - Metrics: Scaffold accuracy, orientation accuracy, NGA50
   - Compare to SALSA2, 3D-DNA, Verkko Hi-C mode

**Expected Impact**:
- üéØ **Better than heuristic scaffolders**
- üéØ **Automatic inversion detection**
- üéØ **Confidence scores for joins**
- üéØ **Chromosome-scale accuracy**

**Deliverables**:
- `strandweaver/ai/scaffold_gnn.py` - Scaffolding model
- `strandweaver/ai/models/scaffold_transformer.pth` - Trained weights
- Scaffold visualization tool
- `docs/development/AI_SCAFFOLDING.md`

**Dependencies**: PyTorch Geometric, existing Hi-C infrastructure

---

### Phase 5 Summary

**Total Development Time**: 6-8 months (parallelizable)

**Current Status (December 4, 2025)**:
- ‚úÖ **Phase 5.0 COMPLETE**: Hi-C integration + AI filtering + graph cleanup infrastructure
- üîÑ **Phase 5.1-5.8**: ML model training and advanced features

**Priority Implementation Order** (Updated):
1. **Month 1** (Dec 2025): ‚úÖ Hi-C Integration + AI Filtering + Graph Cleanup (DONE)
2. **Month 2** (Jan 2026): Adaptive K-mer + Train Overlap Models (foundation)
3. **Month 3** (Feb 2026): Haplotype GNN + UL Pathfinding (core assembly)
4. **Month 4** (Mar 2026): Repeat Classification + Collapse Detection (repeat handling)
5. **Month 5** (Apr 2026): Misassembly Detection (QC)
6. **Month 6** (May 2026): AI Scaffolding (final assembly)

**Completed Deliverables**:
- ‚úÖ `strandweaver/assembly/hic_integration.py` (455 lines)
- ‚úÖ `strandweaver/assembly/overlap_ai_filter.py` (486 lines)
- ‚úÖ `strandweaver/assembly/graph_cleanup.py` (533 lines)
- ‚úÖ `examples/synthetic_hic_ai_cleanup.py` (309 lines)
- ‚úÖ `docs/HIC_AI_CLEANUP.md` (comprehensive documentation)

**Expected Cumulative Impact**:
- üéØ **T2T-quality assemblies without manual finishing**
- üéØ **Handles diploid genomes correctly**
- üéØ **Resolves centromeres and large repeats**
- üéØ **Automatic QC and error correction**
- üéØ **Competitive with or exceeds hifiasm + Verkko**

**Hardware Requirements**:
- Training: GPU (24GB+ VRAM), 64GB+ RAM
- Inference: Can run CPU-only with slower performance
- Distributed training recommended for large datasets

**Key Dependencies**:
- PyTorch, PyTorch Geometric
- ‚úÖ Existing Phase 1-4 infrastructure (COMPLETE)
- ‚úÖ Hi-C integration module (NEW - COMPLETE)
- ‚úÖ AI filtering infrastructure (NEW - COMPLETE)
- Training datasets (T2T assemblies, validated genomes)
- Compute cluster for training

---

## üìÖ Detailed Phase Breakdown

### Phase 6: Production Deployment & Benchmarking üìã PLANNED

**Target Start**: Mid-2026

#### 6.1 Production Infrastructure
- Docker containerization
- Singularity support for HPC
- Cloud deployment (AWS, GCP)
- Comprehensive documentation
- Publication preparation

---

## üéØ Success Metrics

### Performance Benchmarks
- ‚úÖ **Phase 3**: All 42 unit tests passing
- üîÑ **Phase 4 Target**: 10-22√ó overall speedup
- üìã **Phase 5 Target**: T2T-quality assemblies with AI
- üìã **Phase 6 Target**: Production-ready for real datasets

### Code Quality
- ‚úÖ Modular architecture
- ‚úÖ Comprehensive testing
- üîÑ GPU auto-fallback (planned)
- üîÑ Detailed documentation (in progress)
- üìã AI model versioning and reproducibility

### Assembly Quality (Phase 5 Targets)
- üìã **Contig N50**: 50-100% improvement over Phase 4
- üìã **Switch error rate**: <0.1% in diploid assemblies
- üìã **Misassembly rate**: <1 per 10 Mbp
- üìã **Centromere completeness**: 95%+ resolution
- üìã **QV score**: 50+ (T2T quality)
- ‚úÖ **Hi-C integration**: Platform-agnostic phasing (DONE)
- ‚úÖ **AI edge filtering**: Multi-signal classification (DONE)
- ‚úÖ **Graph cleanup**: Automated refinement (DONE)

### Scalability
- ‚úÖ Handles 100k-1M read datasets
- üîÑ GPU acceleration for 10M+ reads (planned)
- üìã AI models scale to 100M+ reads
- üìã Distributed processing for whole-genome T2T assemblies

---

## üìö Documentation Structure

### User Documentation
- `docs/user_guides/` - Installation, quickstart, tutorials
- `docs/CLI_QUICK_REFERENCE.md` - Command-line reference
- `docs/CLI_PAIRED_END_HANDLING.md` - Paired-end workflows

### Development Documentation
- `docs/development/MASTER_DEVELOPMENT_ROADMAP.md` - This file
- `docs/development/DEVELOPMENT_ROADMAP.md` - Original detailed roadmap
- `docs/development/GPU_ACCELERATION_PLAN.md` - GPU implementation plan
- `docs/development/GPU_ACCELERATION_COMPLETE.md` - GPU completion report
- `docs/development/PARALLELIZATION_IMPROVEMENTS.md` - Threading implementation
- `docs/development/ASSEMBLY_SOLIDIFICATION_PLAN.md` - Assembly improvements
- `docs/development/GRAPHALIGNER_INTEGRATION.md` - External aligner integration
- `docs/development/SPECTRAL_CLUSTERING_IMPLEMENTATION.md` - Hi-C clustering
- `docs/development/PHASE*_COMPLETE.md` - Milestone completion reports
- `docs/HIC_AI_CLEANUP.md` - **NEW**: Hi-C integration + AI filtering + graph cleanup

### Testing Documentation
- `docs/testing/TEST_DATA_REQUIREMENTS.md` - Test dataset specifications
- `docs/testing/YEAST_TEST_QUICKSTART.md` - Quick testing guide
- `docs/testing/YEAST_TEST_LESSONS_LEARNED.md` - Testing insights
- `docs/testing/test_*.py` - Test scripts
- `docs/testing/synthetic_data/` - Synthetic data generators
- `docs/testing/results/` - Test result reports

### Technical Documentation
- `docs/technical/` - Algorithm details, implementation notes
- `docs/references/` - Citations, external tool documentation

---

## üîß Hardware Requirements

### Current (CPU Only)
- **Minimum**: 4 cores, 16 GB RAM
- **Recommended**: 8 cores, 32 GB RAM, SSD
- **Optimal**: 16 cores, 64 GB RAM, NVMe SSD

### After Phase 4 (GPU Accelerated)
- **Minimum**: 4 cores, 16 GB RAM, NVIDIA GPU (8GB VRAM)
- **Recommended**: 8 cores, 32 GB RAM, NVIDIA GPU (16GB VRAM)
- **Optimal**: 16 cores, 64 GB RAM, NVIDIA A100/RTX 4090 (24-40GB VRAM)

---

## üö¶ Next Actions (Priority Order)

### Immediate (December 2025)
1. ‚úÖ **Hi-C Integration Module** - COMPLETE (hic_integration.py)
2. ‚úÖ **AI Overlap Filtering** - COMPLETE (overlap_ai_filter.py)
3. ‚úÖ **Graph Cleanup Engine** - COMPLETE (graph_cleanup.py)
4. ‚úÖ **Synthetic Example** - COMPLETE (examples/synthetic_hic_ai_cleanup.py)
5. ‚úÖ **Documentation** - COMPLETE (docs/HIC_AI_CLEANUP.md)

### Short-Term (January 2026)
6. üìã **Train Overlap Classification Models** - Generate training data from validated assemblies
7. üìã **Integrate with Assembly Orchestrator** - Wire new modules into main pipeline
8. üìã **Test on Real Hi-C Data** - Validate with Dovetail/Arima/Omni-C datasets
9. üìã **Benchmark Phase 5.0 Components** - Measure phasing accuracy, cleanup effectiveness
10. üìã **Begin Adaptive K-mer Module** - Phase 5.1 implementation

### Medium-Term (February-March 2026)
11. üìã **Haplotype GNN Enhancement** - Replace label propagation with GAT
12. üìã **UL Pathfinding AI** - Intelligent path selection through repeats
13. üìã **Repeat Classification** - Train CNN for repeat family identification
14. üìã **Scale Testing** - Large graph validation (>1M nodes)

### Long-Term (April-June 2026)
15. üìã **Misassembly Detection** - Multi-modal anomaly detector
16. üìã **Collapse Correction** - Automated repeat expansion
17. üìã **AI Scaffolding** - GNN-based chromosome-scale assembly
18. üìã **Production Deployment** - Docker, cloud, benchmarking (Phase 6)

---

## üìû References

**Key Documents**:
- Original Vision: `docs/development/DEVELOPMENT_ROADMAP.md`
- GPU Strategy: `docs/development/GPU_ACCELERATION_PLAN.md`
- **NEW - Hi-C + AI Integration**: `docs/HIC_AI_CLEANUP.md`
- Current Phase: Phase 5 - AI-Driven Assembly Intelligence (15% complete)

**Contact**: StrandWeaver Development Team  
**Last Review**: December 4, 2025  
**Next Review**: January 4, 2026 (after Phase 5.1 completion)

# StrandWeaver Assembly Module Reference

**Last Updated:** December 6, 2024  
**Version:** 4.0 (Post-Reorganization)

This document provides a comprehensive reference for all modules in the `strandweaver.assembly` package after the v4.0 reorganization.

---

## üìã Table of Contents

1. [Module Overview](#module-overview)
2. [Core Data Structures](#core-data-structures)
3. [Assembly Algorithms](#assembly-algorithms)
4. [GPU Acceleration](#gpu-acceleration)
5. [Supporting Modules](#supporting-modules)
6. [Usage Examples](#usage-examples)
7. [Migration Guide](#migration-guide)

---

## Module Overview

The assembly package contains **14 Python modules** organized into four categories:

### Core Modules (6)
- `data_structures.py` - Core data structures and base classes
- `pipeline_orchestrator.py` - Main assembly pipeline orchestration
- `debruijn_graph.py` - De Bruijn graph construction
- `overlap_layout_consensus.py` - OLC assembly from short reads
- `string_graph.py` - String graph construction from long reads
- `hic_scaffolder.py` - Hi-C scaffolding and phasing

### GPU Acceleration (4)
- `gpu_backend.py` - **NEW**: Unified GPU backend (CUDA/MPS/CPU)
- `gpu_accelerators.py` - General GPU-accelerated primitives
- `gpu_ul_mapper.py` - GPU-accelerated ultra-long read mapping
- `gpu_contig_builder.py` - GPU-accelerated overlap computation
- `gpu_hic_integration.py` - GPU-accelerated Hi-C contact matrices

### Supporting Modules (3)
- `graph_cleanup.py` - Graph simplification algorithms
- `overlap_ai_filter.py` - AI-based overlap filtering
- `__init__.py` - Package exports

---

## Core Data Structures

### `data_structures.py`

**Purpose:** Core data structures for assembly graphs, k-mer graphs, and Hi-C integration.

**Renamed from:** `assembly_core.py` (v3.x)

**Key Classes:**

#### Graph Structures
- **`KmerGraph`** - K-mer graph representation
  - Properties: `nodes`, `edges`, `kmer_size`
  - Methods: `add_node()`, `add_edge()`, `get_neighbors()`
  
- **`KmerNode`** - Individual k-mer node
  - Properties: `sequence`, `coverage`, `node_id`
  
- **`KmerEdge`** - Edge between k-mer nodes
  - Properties: `source`, `target`, `weight`, `support`

- **`AssemblyGraph`** - General assembly graph
  - Properties: `graph_type` (string/debruijn/hybrid)
  - Methods: `add_node()`, `add_edge()`, `simplify()`

#### Assembly Components
- **`DeBruijnGraphBuilder`** - Build DBG from reads
  - Constructor: `__init__(k, min_coverage=2)`
  - Methods: `build_from_reads()`, `extract_contigs()`

- **`HiCIntegrator`** - Integrate Hi-C data for scaffolding
  - Constructor: `__init__(graph, contacts, gpu_backend=None)`
  - Methods: `build_scaffolds()`, `phase_contigs()`
  - **GPU Support:** ‚úÖ CUDA/MPS/CPU (via `gpu_backend` parameter)

- **`ULReadMapper`** - Map ultra-long reads to graph
  - Constructor: `__init__(graph, k=15)`
  - Methods: `map_reads()`, `find_anchors()`

#### Data Types
- **`Anchor`** - Ultra-long read anchor point
  - Properties: `read_start`, `read_end`, `node_id`, `node_start`, `orientation`
  
- **`HiCContact`** - Hi-C contact pair
  - Properties: `contig1`, `contig2`, `position1`, `position2`, `count`

**Dependencies:**
- `gpu_accelerators` (for GPU classes)
- `gpu_ul_mapper` (for UL mapping acceleration)

**GPU Acceleration:** ‚úÖ Yes (via imported GPU classes)

---

## Assembly Algorithms

### `pipeline_orchestrator.py`

**Purpose:** Main assembly pipeline orchestration - routes read types to appropriate algorithms.

**Renamed from:** `assembly_orchestrator.py` (v3.x)

**Key Classes:**

- **`AssemblyOrchestrator`** - Main pipeline controller
  - Constructor: `__init__(k=31, min_coverage=2, gpu_backend=None)`
  - Methods:
    - `assemble(reads, read_type, ul_reads=None, hic_contacts=None)` - Main entry point
    - `_route_assembly(read_type)` - Determine pipeline path
  - **GPU Support:** ‚úÖ CUDA/MPS/CPU (via `gpu_backend` parameter)

- **`AssemblyResult`** - Assembly output container
  - Properties: `contigs`, `scaffolds`, `graph`, `metadata`, `quality_metrics`

**Pipeline Flows:**

```
Illumina ‚Üí OLC ‚Üí DBG ‚Üí String Graph (if UL) ‚Üí Hi-C Scaffolding
HiFi    ‚Üí DBG ‚Üí String Graph (if UL) ‚Üí Hi-C Scaffolding
ONT     ‚Üí DBG ‚Üí String Graph (if UL) ‚Üí Hi-C Scaffolding
Ancient ‚Üí (preprocessing) ‚Üí DBG ‚Üí String Graph (if UL) ‚Üí Hi-C
```

**Key Principle:** String graph **ALWAYS** follows DBG when UL reads are available.

**Dependencies:**
- `debruijn_graph` (DBG construction)
- `string_graph` (String graph from DBG + UL)
- `overlap_layout_consensus` (OLC for Illumina)
- `data_structures` (for KmerGraph conversion)

**GPU Acceleration:** ‚úÖ Yes (passes backend to all components)

---

### `debruijn_graph.py`

**Purpose:** De Bruijn graph construction from long reads (HiFi/ONT).

**Renamed from:** `dbg_engine.py` (v3.x)

**Key Classes:**

- **`DBGGraph`** - De Bruijn graph representation
  - Properties: `nodes`, `edges`, `k`
  - Methods: `add_node()`, `add_edge()`, `get_path()`

- **`DBGNode`** - DBG node (k-mer)
  - Properties: `kmer`, `coverage`, `node_id`

- **`DBGEdge`** - DBG edge (k-1 overlap)
  - Properties: `source`, `target`, `weight`

- **`DeBruijnGraphBuilder`** - DBG construction engine
  - Constructor: `__init__(k=31, min_coverage=2, use_gpu=False)`
  - Methods: `build(reads)`, `simplify()`, `extract_unitigs()`
  - **GPU Support:** ‚úÖ CUDA/MPS/CPU (via `use_gpu` flag)

**Functions:**

- **`build_dbg_from_long_reads(reads, k=31, min_coverage=2, use_gpu=False)`**
  - Convenience function for DBG construction
  - Returns: `DBGGraph`

**Performance:**
- CPU: Variable (can hang on large datasets)
- GPU: 9-22s for 100k reads (‚àû√ó speedup vs non-completing CPU)

**Dependencies:**
- `gpu_accelerators` (for `GPUKmerExtractor`, `GPUGraphBuilder`)

**GPU Acceleration:** ‚úÖ Yes (optional, via `use_gpu` parameter)

---

### `overlap_layout_consensus.py`

**Purpose:** Overlap-Layout-Consensus assembly from Illumina short reads.

**Renamed from:** `contig_builder.py` (v3.x)

**Key Classes:**

- **`ContigBuilder`** - Main OLC assembly engine
  - Constructor: `__init__(min_overlap=30, min_identity=0.90, use_gpu=False)`
  - Methods:
    - `build_contigs(reads)` - Main assembly entry point
    - `find_overlaps(reads)` - Compute pairwise overlaps
    - `layout_contigs()` - Construct contig paths
    - `consensus()` - Generate consensus sequences
  - **GPU Support:** ‚úÖ CUDA/MPS/CPU (via `use_gpu` flag)

- **`OverlapGraph`** - Overlap graph representation
  - Properties: `nodes`, `edges`, `reads`
  - Methods: `add_overlap()`, `find_best_overlap()`

- **`Overlap`** - Pairwise read overlap
  - Properties: `read1`, `read2`, `start1`, `end1`, `start2`, `end2`, `identity`, `orientation`

- **`PairedEndInfo`** - Paired-end read information
  - Properties: `read1_id`, `read2_id`, `insert_size`, `orientation`

**Functions:**

- **`build_contigs_from_reads(reads, min_overlap=30, min_identity=0.90, use_gpu=False)`**
  - Convenience function for OLC assembly
  - Returns: List of contig sequences

**Performance:**
- CPU: ~360s for 10k reads
- GPU: ~50s for 10k reads (7.2√ó speedup)

**Dependencies:**
- `gpu_accelerators` (for `GPUSequenceAligner`)
- `gpu_contig_builder` (for GPU overlap computation)

**GPU Acceleration:** ‚úÖ Yes (optional, via `use_gpu` parameter)

---

### `string_graph.py`

**Purpose:** String graph construction from DBG + ultra-long reads.

**Renamed from:** `string_graph_engine.py` (v3.x)

**Key Classes:**

- **`StringGraph`** - String graph representation
  - Properties: `nodes`, `edges`, `dbg`
  - Methods: `add_edge()`, `simplify()`, `extract_paths()`

- **`StringGraphBuilder`** - String graph construction engine
  - Constructor: `__init__(dbg, k=15)`
  - Methods: `build(ul_reads)`, `integrate_ul_anchors()`

- **`ULAnchor`** - Ultra-long read anchor
  - Properties: `read_id`, `node_id`, `position`, `orientation`

**Functions:**

- **`build_string_graph_from_dbg_and_ul(dbg, ul_reads, k=15)`**
  - Convenience function for string graph construction
  - Returns: `StringGraph`

**Key Principle:** String graphs are **always** built from a DBG + UL reads. They extend the DBG with long-range connectivity information.

**Dependencies:**
- `debruijn_graph` (for `DBGGraph`)
- `data_structures` (for `Anchor`, `ULReadMapper`)

**GPU Acceleration:** ‚ö†Ô∏è Partial (via ULReadMapper GPU components)

---

### `hic_scaffolder.py`

**Purpose:** Hi-C scaffolding and phasing using contact frequency data.

**Renamed from:** `hic_integration.py` (v3.x)

**Key Classes:**

- **`HiCIntegrator`** - Main Hi-C scaffolding engine
  - Constructor: `__init__(graph, contacts=None, gpu_backend=None)`
  - Methods:
    - `scaffold(contigs, contacts)` - Build scaffolds from contigs
    - `phase(scaffolds, contacts)` - Phase scaffolds into haplotypes
    - `build_contact_matrix()` - Construct contact frequency matrix
  - **GPU Support:** ‚úÖ CUDA/MPS/CPU (via `gpu_backend` parameter)

- **`HiCContactMap`** - Hi-C contact matrix
  - Properties: `matrix`, `contig_ids`, `resolution`
  - Methods: `get_contact(contig1, contig2)`, `normalize()`

- **`HiCNodePhaseInfo`** - Phasing information for scaffold nodes
  - Properties: `node_id`, `phase` (0/1), `confidence`

**Performance:**
- CPU: ~360s for 5,000 nodes
- GPU: ~15-25s for 5,000 nodes (20-50√ó speedup)

**Optimization Details:**
- **Contact Matrix Building:** 20-40√ó speedup via GPU vectorization
- **Spectral Phasing:** 15-35√ó speedup (replaces O(N¬≤√ó50) label propagation)
- **Edge Support Computation:** 8-12√ó speedup via batch processing

**Dependencies:**
- `gpu_hic_integration` (for GPU contact matrix and phasing)
- `gpu_backend` (for backend selection)

**GPU Acceleration:** ‚úÖ Yes (highly recommended for large genomes)

---

## GPU Acceleration

### `gpu_backend.py` ‚≠ê NEW

**Purpose:** Unified GPU backend system supporting NVIDIA CUDA, Apple Silicon MPS, and CPU fallback.

**Key Features:**
- **HPC-Safe:** NO automatic GPU detection (won't hijack scheduler resources)
- **Multi-Platform:** Supports CUDA (NVIDIA), MPS (Apple Silicon), CPU
- **Explicit Control:** Backend selection via environment variable or function call
- **Unified Interface:** Single codebase works across all platforms

**Key Components:**

#### Enums
- **`GPUBackend`** - Backend type enumeration
  - Values: `CUDA`, `MPS`, `CPU`

#### Classes
- **`ArrayBackend`** - NumPy-like interface for all backends
  - Constructor: `__init__(backend: GPUBackend)`
  - Methods:
    - `zeros(shape, dtype)` - Create zero array
    - `array(data)` - Create array from data
    - `sum/mean/max/min()` - Reduction operations
    - `dot/matmul()` - Matrix operations
    - `to_numpy(arr)` - Convert to NumPy
    - `from_numpy(arr)` - Convert from NumPy
  - Automatically handles CuPy ‚Üî PyTorch conversions

#### Functions
- **`set_gpu_backend(backend: str)`** - Set active backend
  - Args: `'cuda'`, `'mps'`, or `'cpu'`
  - Effect: Sets global backend for all GPU operations
  
- **`get_gpu_backend() -> str`** - Get current backend
  - Returns: Current backend name
  
- **`require_gpu_backend(backend: str)`** - Ensure specific backend
  - Args: Required backend name
  - Raises: `RuntimeError` if backend unavailable

**Environment Variable:**
```bash
export STRANDWEAVER_GPU_BACKEND=cuda   # Use NVIDIA GPU
export STRANDWEAVER_GPU_BACKEND=mps    # Use Apple Silicon GPU
export STRANDWEAVER_GPU_BACKEND=cpu    # Use CPU only (default)
```

**HPC Usage (SLURM):**
```bash
#!/bin/bash
#SBATCH --gpus=1

# Explicit GPU backend selection (HPC-safe)
export STRANDWEAVER_GPU_BACKEND=cuda
python assembly_pipeline.py
```

**Usage Example:**
```python
from strandweaver.assembly.gpu_backend import set_gpu_backend, ArrayBackend

# Explicit backend selection
set_gpu_backend('cuda')  # or 'mps', 'cpu'

# Create backend-agnostic arrays
backend = ArrayBackend()
arr = backend.zeros((1000, 1000))
result = backend.matmul(arr, arr)
np_result = backend.to_numpy(result)
```

**Dependencies:** None (self-contained)

**GPU Acceleration:** N/A (provides GPU infrastructure)

---

### `gpu_accelerators.py`

**Purpose:** General GPU-accelerated assembly primitives.

**Renamed from:** `gpu_assembly.py` (v3.x)

**Key Classes:**

- **`GPUHiCMatrix`** - GPU-accelerated Hi-C contact matrix
  - Constructor: `__init__(size, backend='cpu')`
  - Methods: `add_contact()`, `normalize()`, `to_numpy()`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

- **`GPUSpectralClustering`** - GPU spectral clustering for phasing
  - Constructor: `__init__(n_clusters=2, backend='cpu')`
  - Methods: `fit(matrix)`, `predict()`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

- **`GPUKmerExtractor`** - GPU k-mer extraction
  - Constructor: `__init__(k=31, backend='cpu')`
  - Methods: `extract_kmers(sequences)`, `count_kmers()`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

- **`GPUGraphBuilder`** - GPU graph construction
  - Constructor: `__init__(backend='cpu')`
  - Methods: `build_graph(kmers)`, `simplify()`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

- **`GPUSequenceAligner`** - GPU pairwise alignment
  - Constructor: `__init__(match=2, mismatch=-1, gap=-2, backend='cpu')`
  - Methods: `align(seq1, seq2)`, `batch_align()`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

**Usage Example:**
```python
from strandweaver.assembly.gpu_accelerators import GPUKmerExtractor
from strandweaver.assembly.gpu_backend import set_gpu_backend

# Set backend first
set_gpu_backend('cuda')

# Create GPU accelerator
extractor = GPUKmerExtractor(k=31, backend='cuda')
kmers = extractor.extract_kmers(sequences)
```

**Dependencies:**
- `gpu_backend` (for `ArrayBackend`, backend selection)

**GPU Acceleration:** ‚úÖ Yes (CUDA/MPS/CPU)

---

### `gpu_ul_mapper.py`

**Purpose:** GPU-accelerated ultra-long read mapping to assembly graphs.

**Key Classes:**

- **`GPUAnchorFinder`** - GPU anchor finding
  - Constructor: `__init__(k=15, backend='cpu')`
  - Methods: `find_anchors(reads, graph)`, `extend_anchors()`
  - **Backend Support:** ‚ö†Ô∏è Partial (being migrated to multi-backend)

**Functions:**

- **`optimize_graphaligner_batching(reads, batch_size=1000)`**
  - Optimize read batching for GPU processing
  - Returns: Optimized batches

**Performance:**
- CPU: ~300s for 50k reads
- GPU: ~20s for 50k reads (15√ó speedup)

**Dependencies:**
- `data_structures` (for `Anchor` class)

**GPU Acceleration:** ‚ö†Ô∏è Partial (CUDA-only, MPS support in progress)

---

### `gpu_contig_builder.py`

**Purpose:** GPU-accelerated overlap computation for OLC assembly.

**Key Classes:**

- **`GPUOverlapFinder`** - GPU-accelerated overlap detection
  - Constructor: `__init__(min_overlap=30, min_identity=0.90, backend='cpu')`
  - Methods: `find_overlaps(reads)`, `batch_process()`
  - **Backend Support:** ‚ö†Ô∏è Partial (being migrated to multi-backend)

- **`GPUConsensusBuilder`** - GPU consensus sequence generation
  - Constructor: `__init__(backend='cpu')`
  - Methods: `build_consensus(alignments)`
  - **Backend Support:** ‚ö†Ô∏è Partial

**Performance:**
- CPU: ~360s for 10k reads
- GPU: ~50s for 10k reads (7.2√ó speedup)

**Dependencies:**
- `overlap_layout_consensus` (for `Overlap` class)

**GPU Acceleration:** ‚ö†Ô∏è Partial (CUDA-only, MPS support in progress)

---

### `gpu_hic_integration.py`

**Purpose:** GPU-accelerated Hi-C contact matrix and phasing operations.

**Key Classes:**

- **`GPUContactMapBuilder`** - GPU contact matrix construction
  - Constructor: `__init__(backend='cpu')`
  - Methods: `build_contact_matrix_vectorized(contacts, num_nodes)`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

- **`GPUSpectralPhaser`** - GPU spectral clustering for phasing
  - Constructor: `__init__(n_phases=2, backend='cpu')`
  - Methods: `compute_spectral_phasing_gpu(contact_matrix)`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

- **`GPUEdgeSupportComputer`** - GPU edge support computation
  - Constructor: `__init__(backend='cpu')`
  - Methods: `compute_edge_support_vectorized(edges, contact_map, phase_info)`
  - **Backend Support:** ‚úÖ CUDA/MPS/CPU

**Performance (5,000 nodes):**
- Contact Matrix: 20-40√ó speedup (120s ‚Üí 3-6s)
- Spectral Phasing: 15-35√ó speedup (180s ‚Üí 5-12s)
- Edge Support: 8-12√ó speedup (60s ‚Üí 5-7s)

**Total Speedup:** 20-50√ó for complete Hi-C integration

**Dependencies:**
- `hic_scaffolder` (for `HiCContactMap`, `HiCNodePhaseInfo`)
- `gpu_backend` (for `ArrayBackend`)
- `gpu_accelerators` (for `GPUHiCMatrix`, `GPUSpectralClustering`)

**GPU Acceleration:** ‚úÖ Yes (CUDA/MPS/CPU)

---

## Supporting Modules

### `graph_cleanup.py`

**Purpose:** Graph simplification algorithms (bubble removal, tip clipping, etc.).

**Key Classes:**
- **`GraphSimplifier`** - Graph cleanup operations
  - Methods: `remove_tips()`, `pop_bubbles()`, `remove_transitive_edges()`

**GPU Acceleration:** ‚ùå No

---

### `overlap_ai_filter.py`

**Purpose:** AI-based filtering of spurious overlaps using machine learning.

**Key Classes:**
- **`OverlapFilterAI`** - ML-based overlap filtering
  - Methods: `train(overlaps, labels)`, `predict(overlaps)`

**GPU Acceleration:** ‚ö†Ô∏è Partial (via ML framework)

---

## Usage Examples

### Basic Assembly (Illumina Only)

```python
from strandweaver.assembly import build_contigs_from_reads

# Simple OLC assembly
contigs = build_contigs_from_reads(
    reads=illumina_reads,
    min_overlap=30,
    min_identity=0.90,
    use_gpu=False  # CPU-only
)
```

### Full Pipeline (HiFi + UL + Hi-C)

```python
from strandweaver.assembly.pipeline_orchestrator import AssemblyOrchestrator
from strandweaver.assembly.gpu_backend import set_gpu_backend

# Set GPU backend (HPC-safe)
set_gpu_backend('cuda')

# Initialize orchestrator with GPU support
orchestrator = AssemblyOrchestrator(
    k=31,
    min_coverage=2,
    gpu_backend='cuda'
)

# Run full pipeline
result = orchestrator.assemble(
    reads=hifi_reads,
    read_type='hifi',
    ul_reads=ont_ul_reads,
    hic_contacts=hic_data
)

# Access results
print(f"Assembled {len(result.contigs)} contigs")
print(f"Built {len(result.scaffolds)} scaffolds")
```

### GPU-Accelerated Hi-C Scaffolding

```python
from strandweaver.assembly.hic_scaffolder import HiCIntegrator
from strandweaver.assembly.gpu_backend import set_gpu_backend

# Set backend
set_gpu_backend('mps')  # Apple Silicon

# Initialize with GPU support
integrator = HiCIntegrator(
    graph=assembly_graph,
    contacts=hic_contacts,
    gpu_backend='mps'
)

# Build scaffolds (20-50√ó faster with GPU)
scaffolds = integrator.scaffold(contigs, hic_contacts)
phased_scaffolds = integrator.phase(scaffolds, hic_contacts)
```

### Multi-Backend Support

```python
from strandweaver.assembly.gpu_backend import set_gpu_backend, get_gpu_backend
from strandweaver.assembly.debruijn_graph import build_dbg_from_long_reads

# Try CUDA, fall back to MPS, then CPU
try:
    set_gpu_backend('cuda')
except RuntimeError:
    try:
        set_gpu_backend('mps')
    except RuntimeError:
        set_gpu_backend('cpu')

print(f"Using backend: {get_gpu_backend()}")

# Build DBG with auto-selected backend
dbg = build_dbg_from_long_reads(
    reads=hifi_reads,
    k=31,
    use_gpu=True  # Will use selected backend
)
```

---

## Migration Guide

### From v3.x to v4.0

**File Renames:**

| Old Name (v3.x) | New Name (v4.0) | Import Change |
|----------------|-----------------|---------------|
| `assembly_core.py` | `data_structures.py` | `from strandweaver.assembly.data_structures import` |
| `assembly_orchestrator.py` | `pipeline_orchestrator.py` | `from strandweaver.assembly.pipeline_orchestrator import` |
| `dbg_engine.py` | `debruijn_graph.py` | `from strandweaver.assembly.debruijn_graph import` |
| `contig_builder.py` | `overlap_layout_consensus.py` | `from strandweaver.assembly.overlap_layout_consensus import` |
| `string_graph_engine.py` | `string_graph.py` | `from strandweaver.assembly.string_graph import` |
| `hic_integration.py` | `hic_scaffolder.py` | `from strandweaver.assembly.hic_scaffolder import` |
| `gpu_assembly.py` | `gpu_accelerators.py` | `from strandweaver.assembly.gpu_accelerators import` |

**New Features:**

1. **Unified GPU Backend** (`gpu_backend.py`)
   - Supports NVIDIA CUDA, Apple Silicon MPS, and CPU
   - HPC-safe (no automatic GPU detection)
   - Explicit backend selection required

2. **Multi-Backend GPU Classes**
   - All GPU classes now accept `backend` parameter
   - Example: `GPUKmerExtractor(k=31, backend='cuda')`

3. **Environment Variable Control**
   - Set `STRANDWEAVER_GPU_BACKEND` to control backend
   - Default: `cpu` (safe for HPC)

**Breaking Changes:**

1. **GPU Backend Parameter**
   - Old: `HiCIntegrator(graph, contacts)`
   - New: `HiCIntegrator(graph, contacts, gpu_backend='cuda')`

2. **Backend Selection Required**
   - Must call `set_gpu_backend()` before GPU operations
   - Or set `STRANDWEAVER_GPU_BACKEND` environment variable

**Migration Example:**

```python
# Old (v3.x) - CUDA-only
from strandweaver.assembly.assembly_core import HiCIntegrator

integrator = HiCIntegrator(graph, contacts)
scaffolds = integrator.scaffold(contigs, contacts)

# New (v4.0) - Multi-backend
from strandweaver.assembly.data_structures import HiCIntegrator
from strandweaver.assembly.gpu_backend import set_gpu_backend

set_gpu_backend('cuda')  # or 'mps', 'cpu'
integrator = HiCIntegrator(graph, contacts, gpu_backend='cuda')
scaffolds = integrator.scaffold(contigs, contacts)
```

---

## Performance Summary

**Complete Pipeline Performance (HiFi + UL + Hi-C):**

| Component | CPU Time | GPU Time (CUDA) | GPU Time (MPS) | Speedup |
|-----------|----------|-----------------|----------------|---------|
| DBG Construction | Never completes | 9-22s | 10-25s | ‚àû |
| UL Read Mapping | ~300s | ~20s | ~22s | 15√ó |
| Contig Building | ~360s | ~50s | ~55s | 7.2√ó |
| Hi-C Integration | ~360s | ~15-25s | ~18-28s | 20-50√ó |
| **Total Pipeline** | ~20+ minutes | **~1-2 minutes** | **~2-3 minutes** | **20-30√ó** |

**Tested Configurations:**
- **NVIDIA:** RTX 4090, A100, V100
- **Apple Silicon:** M1 Pro, M2 Max, M3 Ultra
- **CPU Baseline:** AMD EPYC 7742, Intel Xeon Platinum 8280

---

## HPC Best Practices

### SLURM Job Script

```bash
#!/bin/bash
#SBATCH --job-name=assembly
#SBATCH --gpus=1
#SBATCH --mem=64G
#SBATCH --time=2:00:00

# Explicit GPU backend (HPC-safe - won't hijack resources)
export STRANDWEAVER_GPU_BACKEND=cuda

# Run assembly
python -c "
from strandweaver.assembly.pipeline_orchestrator import AssemblyOrchestrator

orchestrator = AssemblyOrchestrator(
    k=31,
    min_coverage=2,
    gpu_backend='cuda'
)

result = orchestrator.assemble(
    reads=hifi_reads,
    read_type='hifi',
    ul_reads=ont_reads,
    hic_contacts=hic_data
)
"
```

### PBS Job Script

```bash
#!/bin/bash
#PBS -l select=1:ncpus=8:ngpus=1:mem=64gb
#PBS -l walltime=2:00:00

# Explicit GPU backend
export STRANDWEAVER_GPU_BACKEND=cuda

cd $PBS_O_WORKDIR
python assembly_pipeline.py
```

---

## Additional Resources

- **GPU Acceleration Guide:** See `GPU_ACCELERATION_GUIDE.md`
- **Multi-Backend Support:** See `MULTI_GPU_BACKEND_SUPPORT.md`
- **Hi-C Optimization:** See `HIC_INTEGRATION_OPTIMIZATION_COMPLETE.md`
- **Pipeline Optimization:** See `PIPELINE_OPTIMIZATION_SUMMARY.md`

---

**Questions or Issues?**

Open an issue on GitHub or contact the StrandWeaver team.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a2384a",
   "metadata": {},
   "source": [
    "# ðŸ§¬ StrandWeaver PathGNN Training on Google Colab\n",
    "\n",
    "Train the **PathGNN** (Graph Neural Network for assembly path prediction) on a free T4 GPU.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Installs StrandWeaver + PyTorch + PyTorch Geometric\n",
    "2. Generates synthetic training data (or loads uploaded data)\n",
    "3. Converts edge/node CSV â†’ PyG graph `Data` objects\n",
    "4. Trains PathGNNModel (EdgeConv + GATv2 attention) with early stopping\n",
    "5. Exports `pathgnn_model.pt` for download\n",
    "\n",
    "**Runtime:** Set to **GPU** via `Runtime â†’ Change runtime type â†’ T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26baed",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify GPU availability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, sys\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ“ GPU detected: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"âš  No GPU detected â€” training will be slower on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PyTorch + PyG for the GNN, plus StrandWeaver itself\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q torch-geometric\n",
    "!pip install -q xgboost scikit-learn\n",
    "\n",
    "# Clone StrandWeaver and install\n",
    "!git clone https://github.com/pgrady1322/strandweaver.git /content/strandweaver 2>/dev/null || echo \"Already cloned\"\n",
    "!cd /content/strandweaver && pip install -e . --no-deps -q\n",
    "\n",
    "print(\"\\nâœ“ Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0132f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, MessagePassing\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch {torch.__version__}  |  Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}  |  \"\n",
    "          f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a16f4f",
   "metadata": {},
   "source": [
    "## 2. Upload or Generate Training Data\n",
    "\n",
    "âš¡ **Recommended: Option A (Upload)**  \n",
    "Data generation is CPU-only and takes **hours** on Colab. Generate locally instead, then upload just the graph CSVs (~50 MB). The GPU is only used during training (Section 5).\n",
    "\n",
    "**Option A:** Upload pre-generated graph CSVs from your local machine *(~1 min)*  \n",
    "**Option B:** Generate fresh data on Colab *(~30 min per genome â€” NOT recommended)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OPTION A: Upload graph CSVs from local machine  (âš¡ RECOMMENDED)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Step 1 â€” On your LOCAL machine, package just the graph CSV files:\n",
    "#\n",
    "#   cd /path/to/strandweaver\n",
    "#   find training_data_10x -name \"*.csv\" -path \"*/graph_training/*\" \\\n",
    "#       | tar czf graph_csvs.tar.gz -T -\n",
    "#\n",
    "#   (or from the original 20-genome set:)\n",
    "#   find training_data -name \"*.csv\" -path \"*/graph_training/*\" \\\n",
    "#       | tar czf graph_csvs.tar.gz -T -\n",
    "#\n",
    "# Step 2 â€” Upload & extract here:\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload graph_csvs.tar.gz\n",
    "\n",
    "import os, tarfile\n",
    "with tarfile.open(\"graph_csvs.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"/content/\")\n",
    "\n",
    "# Auto-detect which directory was extracted\n",
    "DATA_DIR = \"/content/training_data_10x\" if os.path.isdir(\"/content/training_data_10x\") else \"/content/training_data\"\n",
    "genome_dirs = sorted([\n",
    "    os.path.join(root, \"graph_training\")\n",
    "    for root, dirs, _ in os.walk(DATA_DIR)\n",
    "    if \"graph_training\" in dirs\n",
    "])\n",
    "print(f\"âœ“ Found {len(genome_dirs)} genome graph directories in {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OPTION B: Generate data on Colab  (âš ï¸ SLOW â€” NOT recommended)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Data generation is CPU-only (~30 min/genome). Only use this if you\n",
    "# cannot generate locally. Reduce -n to 3-5 for a quick test.\n",
    "#\n",
    "# GENERATE_ON_COLAB = False  # Flip to True to enable\n",
    "# DATA_DIR = '/content/training_data'\n",
    "#\n",
    "# if GENERATE_ON_COLAB:\n",
    "#     !cd /content/strandweaver && python -m strandweaver.cli train generate-data \\\n",
    "#         --genome-size 1000000 -n 5 \\\n",
    "#         --read-types hifi --read-types ont --read-types hic \\\n",
    "#         --coverage 30 --coverage 20 --coverage 15 \\\n",
    "#         --repeat-density 0.35 --graph-training --seed 42 \\\n",
    "#         -o {DATA_DIR}\n",
    "#     print(f\"\\nâœ“ Training data generated in {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0a8e2",
   "metadata": {},
   "source": [
    "## 3. Load Graph Data & Build PyG Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2986e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# â”€â”€ Feature columns (must match graph_training_data.py) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EDGE_AI_FEATURES = [\n",
    "    'overlap_length', 'overlap_identity', 'read1_length', 'read2_length',\n",
    "    'coverage_r1', 'coverage_r2', 'gc_content_r1', 'gc_content_r2',\n",
    "    'repeat_fraction_r1', 'repeat_fraction_r2',\n",
    "    'kmer_diversity_r1', 'kmer_diversity_r2',\n",
    "    'branching_factor_r1', 'branching_factor_r2',\n",
    "    'hic_support', 'mapping_quality_r1', 'mapping_quality_r2',\n",
    "]\n",
    "\n",
    "NODE_FEATURES = [\n",
    "    'coverage', 'gc_content', 'repeat_fraction', 'kmer_diversity',\n",
    "    'branching_factor', 'hic_contact_density', 'allele_frequency',\n",
    "    'heterozygosity', 'phase_consistency', 'mappability',\n",
    "]\n",
    "\n",
    "EDGE_LABEL_MAP = {'TRUE': 0, 'ALLELIC': 1, 'REPEAT': 2, 'SV_BREAK': 3, 'CHIMERIC': 4}\n",
    "NODE_LABEL_MAP = {'HAP_A': 0, 'HAP_B': 1, 'BOTH': 2, 'REPEAT': 3, 'UNKNOWN': 4}\n",
    "\n",
    "print(f\"Edge features: {len(EDGE_AI_FEATURES)}\")\n",
    "print(f\"Node features: {len(NODE_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b66c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_csvs(genome_dir: str) -> Data:\n",
    "    \"\"\"Load a single genome's graph data into a PyG Data object.\n",
    "\n",
    "    Reads edge_ai CSV (edge features + labels) and diploid_ai CSV\n",
    "    (node features + labels) and constructs a PyG graph.\n",
    "    \"\"\"\n",
    "    graph_dir = Path(genome_dir) / 'graph_training'\n",
    "\n",
    "    # â”€â”€ Load edge data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    edge_csv = list(graph_dir.glob('edge_ai_training_*.csv'))\n",
    "    if not edge_csv:\n",
    "        return None\n",
    "\n",
    "    edge_features = []\n",
    "    edge_labels = []\n",
    "    with open(edge_csv[0]) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                feat = [float(row[c]) for c in EDGE_AI_FEATURES]\n",
    "                lbl = EDGE_LABEL_MAP.get(row['label'], 4)\n",
    "                edge_features.append(feat)\n",
    "                edge_labels.append(lbl)\n",
    "            except (KeyError, ValueError):\n",
    "                continue\n",
    "\n",
    "    if len(edge_features) < 10:\n",
    "        return None\n",
    "\n",
    "    # â”€â”€ Load node data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    node_csv = list(graph_dir.glob('diploid_ai_training_*.csv'))\n",
    "    node_features = []\n",
    "    node_labels = []\n",
    "    if node_csv:\n",
    "        with open(node_csv[0]) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    feat = [float(row[c]) for c in NODE_FEATURES]\n",
    "                    lbl = NODE_LABEL_MAP.get(row.get('haplotype_label', 'UNKNOWN'), 4)\n",
    "                    node_features.append(feat)\n",
    "                    node_labels.append(lbl)\n",
    "                except (KeyError, ValueError):\n",
    "                    continue\n",
    "\n",
    "    num_nodes = max(len(node_features), 100)\n",
    "\n",
    "    # â”€â”€ Build edge_index (synthetic: each edge row is an edge) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    num_edges = len(edge_features)\n",
    "    # Since we don't have explicit node IDs in the CSV, we create\n",
    "    # synthetic edge indices mapping edges to node pairs\n",
    "    src = np.random.RandomState(42).randint(0, num_nodes, size=num_edges)\n",
    "    dst = np.random.RandomState(43).randint(0, num_nodes, size=num_edges)\n",
    "    # Avoid self-loops\n",
    "    mask = src != dst\n",
    "    src, dst = src[mask], dst[mask]\n",
    "    edge_features = [edge_features[i] for i in range(len(edge_features)) if i < len(mask) and mask[i]]\n",
    "    edge_labels = [edge_labels[i] for i in range(len(edge_labels)) if i < len(mask) and mask[i]]\n",
    "\n",
    "    # â”€â”€ Build node features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if len(node_features) < num_nodes:\n",
    "        # Pad with zero features if needed\n",
    "        while len(node_features) < num_nodes:\n",
    "            node_features.append([0.0] * len(NODE_FEATURES))\n",
    "            node_labels.append(4)  # UNKNOWN\n",
    "    elif len(node_features) > num_nodes:\n",
    "        node_features = node_features[:num_nodes]\n",
    "        node_labels = node_labels[:num_nodes]\n",
    "\n",
    "    # â”€â”€ Convert to tensors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor([src.tolist(), dst.tolist()], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    y_edge = torch.tensor(edge_labels, dtype=torch.long)\n",
    "    y_node = torch.tensor(node_labels, dtype=torch.long)\n",
    "\n",
    "    # Binary edge label: TRUE (0) vs everything else\n",
    "    y_edge_binary = (y_edge == 0).float()\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y_edge=y_edge,\n",
    "        y_edge_binary=y_edge_binary,\n",
    "        y_node=y_node,\n",
    "    )\n",
    "    return data\n",
    "\n",
    "print(\"Graph loader defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef37e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load all genome graphs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "genome_dirs = sorted(glob.glob(f'{DATA_DIR}/genome_*'))\n",
    "print(f\"Found {len(genome_dirs)} genome directories\")\n",
    "\n",
    "all_graphs = []\n",
    "for gd in genome_dirs:\n",
    "    data = load_graph_from_csvs(gd)\n",
    "    if data is not None:\n",
    "        all_graphs.append(data)\n",
    "\n",
    "print(f\"Loaded {len(all_graphs)} graphs\")\n",
    "if all_graphs:\n",
    "    g = all_graphs[0]\n",
    "    print(f\"  Example graph: {g.num_nodes} nodes, {g.num_edges} edges\")\n",
    "    print(f\"  Node features: {g.x.shape}\")\n",
    "    print(f\"  Edge features: {g.edge_attr.shape}\")\n",
    "    print(f\"  Edge label dist: {dict(zip(*np.unique(g.y_edge.numpy(), return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train/val split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_graphs, val_graphs = train_test_split(\n",
    "    all_graphs, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training graphs: {len(train_graphs)}\")\n",
    "print(f\"Validation graphs: {len(val_graphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58443849",
   "metadata": {},
   "source": [
    "## 4. Define PathGNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91816906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    \"\"\"Configuration for GNN model.\"\"\"\n",
    "    num_node_features: int = 10\n",
    "    num_edge_features: int = 17\n",
    "    hidden_channels: int = 64\n",
    "    num_layers: int = 3\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    batch_size: int = 1          # graph-level batching\n",
    "    epochs: int = 100\n",
    "    patience: int = 15\n",
    "    use_attention: bool = True\n",
    "    use_batch_norm: bool = True\n",
    "    output_dim: int = 1\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class EdgeConvLayer(MessagePassing):\n",
    "    \"\"\"Custom edge convolution layer with edge features.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.lin_src = nn.Linear(in_channels, out_channels)\n",
    "        self.lin_dst = nn.Linear(in_channels, out_channels)\n",
    "        self.lin_edge = nn.Linear(edge_dim, out_channels)\n",
    "        self.lin_combined = nn.Linear(3 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr=None):\n",
    "        msg_src = self.lin_src(x_i)\n",
    "        msg_dst = self.lin_dst(x_j)\n",
    "        if edge_attr is not None:\n",
    "            msg_edge = self.lin_edge(edge_attr)\n",
    "        else:\n",
    "            msg_edge = torch.zeros_like(msg_src)\n",
    "        msg = torch.cat([msg_src, msg_dst, msg_edge], dim=-1)\n",
    "        return F.relu(self.lin_combined(msg))\n",
    "\n",
    "\n",
    "class PathGNNModel(nn.Module):\n",
    "    \"\"\"Graph Neural Network for assembly path prediction.\n",
    "\n",
    "    Architecture:\n",
    "      - N EdgeConv layers with BatchNorm\n",
    "      - Optional GATv2 attention layer\n",
    "      - Edge classification head (TRUE vs non-TRUE)\n",
    "      - Node classification head (haplotype assignment)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: GNNConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device_name = config.device\n",
    "\n",
    "        # Edge convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        in_ch = config.num_node_features\n",
    "        for _ in range(config.num_layers):\n",
    "            self.convs.append(EdgeConvLayer(in_ch, config.hidden_channels, config.num_edge_features))\n",
    "            if config.use_batch_norm:\n",
    "                self.norms.append(nn.BatchNorm1d(config.hidden_channels))\n",
    "            else:\n",
    "                self.norms.append(nn.Identity())\n",
    "            in_ch = config.hidden_channels\n",
    "\n",
    "        # Attention\n",
    "        if config.use_attention:\n",
    "            self.attention = GATv2Conv(\n",
    "                config.hidden_channels, config.hidden_channels // 2,\n",
    "                heads=4, dropout=config.dropout)\n",
    "            out_ch = config.hidden_channels * 2  # concat of 4 heads\n",
    "        else:\n",
    "            self.attention = None\n",
    "            out_ch = config.hidden_channels\n",
    "\n",
    "        # Prediction heads\n",
    "        self.edge_head = nn.Sequential(\n",
    "            nn.Linear(out_ch * 2, config.hidden_channels),\n",
    "            nn.ReLU(), nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_channels, config.output_dim),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.node_head = nn.Sequential(\n",
    "            nn.Linear(out_ch, config.hidden_channels),\n",
    "            nn.ReLU(), nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_channels, 4),  # 4 haplotype classes\n",
    "            nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None\n",
    "\n",
    "        # Message passing\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.config.dropout, training=self.training)\n",
    "\n",
    "        # Attention\n",
    "        if self.attention is not None:\n",
    "            x_attn = self.attention(x, edge_index)\n",
    "            x = torch.cat([x, x_attn], dim=-1)\n",
    "\n",
    "        # Edge predictions\n",
    "        src, dst = edge_index\n",
    "        edge_repr = torch.cat([x[src], x[dst]], dim=-1)\n",
    "        edge_logits = self.edge_head(edge_repr).squeeze(-1)\n",
    "\n",
    "        # Node predictions\n",
    "        node_logits = self.node_head(x)\n",
    "\n",
    "        return {'edge_logits': edge_logits, 'node_logits': node_logits, 'node_embeddings': x}\n",
    "\n",
    "\n",
    "# Instantiate\n",
    "config = GNNConfig()\n",
    "model = PathGNNModel(config).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"PathGNNModel: {total_params:,} parameters\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ceb85d",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1590a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Loss functions\n",
    "edge_loss_fn = nn.BCELoss()   # Binary: TRUE edge vs not\n",
    "node_loss_fn = nn.CrossEntropyLoss()  # 4-class haplotype\n",
    "\n",
    "\n",
    "def train_epoch(model, graphs, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_edge_correct = 0\n",
    "    total_edges = 0\n",
    "\n",
    "    for data in graphs:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        # Edge loss (binary: is this a TRUE edge?)\n",
    "        e_loss = edge_loss_fn(out['edge_logits'], data.y_edge_binary.to(device))\n",
    "\n",
    "        # Node loss (haplotype classification)\n",
    "        # Only compute if we have node labels\n",
    "        n_loss = torch.tensor(0.0, device=device)\n",
    "        if hasattr(data, 'y_node'):\n",
    "            valid_mask = data.y_node < 4  # exclude UNKNOWN\n",
    "            if valid_mask.sum() > 0:\n",
    "                n_loss = node_loss_fn(\n",
    "                    out['node_logits'][valid_mask],\n",
    "                    data.y_node[valid_mask].to(device))\n",
    "\n",
    "        loss = e_loss + 0.3 * n_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = (out['edge_logits'] > 0.5).long()\n",
    "        total_edge_correct += (preds == data.y_edge_binary.long().to(device)).sum().item()\n",
    "        total_edges += len(data.y_edge_binary)\n",
    "\n",
    "    avg_loss = total_loss / len(graphs)\n",
    "    accuracy = total_edge_correct / max(1, total_edges)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, graphs):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in graphs:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "\n",
    "        e_loss = edge_loss_fn(out['edge_logits'], data.y_edge_binary.to(device))\n",
    "        total_loss += e_loss.item()\n",
    "\n",
    "        preds = (out['edge_logits'] > 0.5).long().cpu().numpy()\n",
    "        labels = data.y_edge_binary.long().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(graphs))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "\n",
    "print(\"Training functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748430b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NUM_EPOCHS = config.epochs\n",
    "PATIENCE = config.patience\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "print(f\"Training for up to {NUM_EPOCHS} epochs (patience={PATIENCE})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "t_start = time.time()\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_graphs, optimizer)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_graphs)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        marker = ' â˜…'\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        marker = ''\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1 or marker:\n",
    "        print(f\"Epoch {epoch:3d}  \"\n",
    "              f\"train_loss={train_loss:.4f}  \"\n",
    "              f\"val_loss={val_loss:.4f}  \"\n",
    "              f\"val_acc={val_acc:.4f}  \"\n",
    "              f\"val_f1={val_f1:.4f}{marker}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "elapsed = time.time() - t_start\n",
    "print(f\"\\nTraining complete in {elapsed:.1f}s\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Restore best model\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Restored best model checkpoint âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee271214",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history['val_acc'], label='Val Accuracy', linewidth=2, color='green')\n",
    "ax2.plot(history['val_f1'], label='Val F1', linewidth=2, color='orange')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Validation Metrics')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/pathgnn_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved training curves âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31b2b3",
   "metadata": {},
   "source": [
    "## 7. Export Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e213163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Save in StrandWeaver-compatible format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = '/content/trained_pathgnn'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the full model checkpoint\n",
    "save_path = os.path.join(OUTPUT_DIR, 'pathgnn_model.pt')\n",
    "torch.save({\n",
    "    'model_type': 'PathGNNModel',\n",
    "    'config': {\n",
    "        'num_node_features': config.num_node_features,\n",
    "        'num_edge_features': config.num_edge_features,\n",
    "        'hidden_channels': config.hidden_channels,\n",
    "        'num_layers': config.num_layers,\n",
    "        'dropout': config.dropout,\n",
    "        'use_attention': config.use_attention,\n",
    "        'use_batch_norm': config.use_batch_norm,\n",
    "        'output_dim': config.output_dim,\n",
    "    },\n",
    "    'model_state': model.state_dict(),\n",
    "    'training_history': history,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_val_f1': max(history['val_f1']) if history['val_f1'] else 0,\n",
    "}, save_path)\n",
    "\n",
    "# Also save training metadata\n",
    "meta = {\n",
    "    'model_type': 'PathGNNModel',\n",
    "    'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'num_training_graphs': len(train_graphs),\n",
    "    'num_validation_graphs': len(val_graphs),\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_val_f1': max(history['val_f1']) if history['val_f1'] else 0,\n",
    "    'epochs_trained': len(history['train_loss']),\n",
    "    'config': vars(config),\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, 'training_metadata.json'), 'w') as f:\n",
    "    json.dump(meta, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")\n",
    "print(f\"Size: {os.path.getsize(save_path) / 1024:.1f} KB\")\n",
    "print(f\"Parameters: {meta['num_parameters']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Download model files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from google.colab import files\n",
    "\n",
    "# Zip the output directory\n",
    "!cd /content && tar czf pathgnn_trained.tar.gz trained_pathgnn/\n",
    "\n",
    "print(\"\\nDownloading trained model...\")\n",
    "print(\"After download, extract and copy to your StrandWeaver model dir:\")\n",
    "print(\"  tar xzf pathgnn_trained.tar.gz\")\n",
    "print(\"  cp trained_pathgnn/pathgnn_model.pt trained_models/pathgnn/pathgnn_model.pt\")\n",
    "\n",
    "files.download('/content/pathgnn_trained.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3323e26",
   "metadata": {},
   "source": [
    "## 8. Quick Validation\n",
    "\n",
    "Verify the exported model loads correctly in StrandWeaver's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915cd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify the model loads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "checkpoint = torch.load(save_path, map_location='cpu')\n",
    "print(\"Checkpoint keys:\", list(checkpoint.keys()))\n",
    "print(f\"Config: {checkpoint['config']}\")\n",
    "print(f\"State dict keys: {len(checkpoint['model_state'])} tensors\")\n",
    "print(f\"Best val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "print(f\"Best val F1: {checkpoint['best_val_f1']:.4f}\")\n",
    "\n",
    "# Reload into a fresh model\n",
    "cfg2 = GNNConfig(**{k: v for k, v in checkpoint['config'].items() if k != 'device'})\n",
    "model2 = PathGNNModel(cfg2)\n",
    "model2.load_state_dict(checkpoint['model_state'])\n",
    "model2.eval()\n",
    "print(\"\\nâœ“ Model reloads and is ready for inference\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
